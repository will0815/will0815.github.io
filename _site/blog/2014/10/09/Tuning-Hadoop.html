<!DOCTYPE HTML>
<html>
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0">
  <meta name="description" content="Will go, Just do it.">
  <meta name="author" content="Will.Quan">
  <meta name="keywords" content="Tuning Hadoop, willgo, Will.Quan">
  <title>Tuning Hadoop - willgo</title>
  <link rel="canonical" href="index.html/blog/2014/10/09/Tuning-Hadoop.html">
  <link rel="icon" href="/res/img/favicon.ico" type="image/x-icon">
  <link rel="shortcut icon" href="/res/img/favicon.ico" type="image/x-icon">
  <link rel="stylesheet" href="/res/css/public.css">
  <link rel="stylesheet" href="/res/css/light.css">
  <script src="/res/js/light.js"></script>
  <script type="text/javascript">
var _bdhmProtocol = (("https:" == document.location.protocol) ? " https://" : " http://");
document.write(unescape("%3Cscript src='" + _bdhmProtocol + "hm.baidu.com/h.js%3Fda48b6233123178f300913a0e707883e' type='text/javascript'%3E%3C/script%3E"));
</script>

</head>
<body>
  <div id="blog">
    <div class="sidebar">
  <div class="profilepic">
    <a href="/"><img src="/res/img/icon.png" alt="logo"></img></a>
  </div>
  <h1 class="title"><a href="/">willgo</a></h1>
  <h2 class="sub-title">最好的从未错过</h2>
  <nav id="nav">
    <ul>
    
      <li><a href="/page/timing.html"><i class="fa fa-clock-o"></i>&nbsp;资料时间</a></li>
    
      <li><a href="/page/category.html"><i class="fa fa-tags"></i>&nbsp;文章分类</a></li>
    
      <li><a href="/page/read.html"><i class="fa fa-book"></i>&nbsp;逗绊读书</a></li>
    
      <li><a href="/page/life.html"><i class="fa fa-eyedropper"></i>&nbsp;生活记录</a></li>
    
      <li><a href="/page/about.html"><i class="fa fa-paper-plane-o"></i>&nbsp;假装关于</a></li>
    
    </ul>
  </nav>  
  <nav id="sub-nav">
    <a class="weibo " href="http://weibo.com/u/2369015654" title="新浪微博" target="_blank"><i class="fa fa-weibo"></i></a>
    <a class="github" href="https://github.com/will0815/" title="GitHub" target="_blank"><i class="fa fa-github fa-2x"></i></a>
    <a class="rss" href="/page/feed.xml" title="RSS订阅" target="_blank"><i class="fa fa-rss"></i></a>
  </nav>
  <div id="license">
    <a href="http://creativecommons.org/licenses/by-nc-sa/3.0/cn/" target="_blank" title="本站所有作品采用：&#10;知识共享《署名 非商业性使用 相同方式共享 3.0》&#10;进行许可" >
    <img alt="License" height="31" width="88" src="/res/img/license.png" /></a>
  </div>
</div>

    <div class="main">
        <div style="font-family: segoepr;font-size: 40px;
              margin-top:-5px; color:#fff;">
            <script type="text/javascript" 
            src="http://open.iciba.com/ds_open.php?id=11519&name=willgo&auth=BE45CDE6481CC46336529A1B407855B1" charset="utf-8">
            </script>
        </div>
    <header class="post-header">
  <h1 class="post-title"><a href="/blog/2014/10/09/Tuning-Hadoop.html">Tuning Hadoop</a></h2>
  <p class="post-meta">
    <i class="fa fa-calendar"></i>
    2014年10月09日
    <i class="space"></i>
    <i class="fa fa-tags"></i>
    <a class="post-category" href="/page/category.html#hadoop">hadoop</a>	
  </p>
</header>
<div class="post-main">
<h2 id="hadoop">Hadoop集群性能调优</h2>

<p><strong>mapred-site.xml</strong></p>

<ol>
  <li>
    <p>mapred.child.java.opts</p>

    <p>配置每个map或reduce使用的内存数量。默认的是200M。网友推荐：如果内存是8G，CPU有8个核，那么就设置成1G就可以了。实际上，在map和reduce的过程中对内存的消耗并不大，但是如果配置的太小，则有可能出现”无可分配内存”的错误。对于Hadoop环境独占的机器可以考虑：总内存 / (Map/Reduce 并发数) * 80%</p>
  </li>
  <li>
    <p>dfs.block.size </p>

    <p>生的map来综合考量的。一般来说，文件大，集群数量少，还是建议将block size设置大一些的好。</p>
  </li>
  <li>
    <p>mapred.compress.map.output </p>

    <p>几乎所有会产生大量map output的hadoop作业，都能够通过将中间文件用Lzo进行压缩来提升性能。尽管lzo会使得cpu的load有所增高，但是reduce在shuffle阶段的disk IO通常都能节省不少时间，对job的性能有好处。
 当采用map中间结果压缩的情况下，用户还可以选择压缩时采用另外几种压缩格式进行压缩，现在hadoop支持的压缩格式 有：GzipCodec，LzoCodec，BZip2Codec，LzmaCodec等压缩格式。通常来说，想要达到比较平衡的cpu和磁盘压缩 比，LzoCodec比较适合。但也要取决于job的具体情况。</p>

    <pre><code> &lt;property&gt;
     &lt;name&gt;mapred.compress.map.output&lt;/name&gt;
     &lt;value&gt;true&lt;/value&gt;
   &lt;/property&gt;
 &lt;property&gt; 
     &lt;name&gt;mapred.map.output.compression.codec&lt;/name&gt; 
     &lt;value&gt;com.hadoop.compression.lzo.LzoCodec&lt;/value&gt; 
 &lt;/property&gt;  
</code></pre>
  </li>
  <li>
    <p>mapred.tasktracker.map.tasks.maximum</p>

    <p>The maximum number of map tasks that will be run simultaneously by a task tracker.</p>

    <p>一个tasktracker最多可以同时运行的map任务数量
 默认值：2
 mapred.tasktracker.map.tasks.maximum = cpu数量
 Sanders: 应该设置成为等于或略小于CPU总核数
 cpu数量 = 服务器CPU总核数 / 每个CPU的核数
 服务器CPU总核数 = more  /proc/cpuinfo | grep ‘processor’ | wc -l
 每个CPU的核数 = more  /proc/cpuinfo | grep ‘cpu cores’</p>
  </li>
  <li>
    <p>mapred.map.tasks</p>

    <p>The default number of map tasks per job
 一个Job会使用task tracker的map任务槽数量，这个值 ≤ mapred.tasktracker.map.tasks.maximum
 默认值：2
 优化值：
 CPU数量 （我们目前的实践值）
 (CPU数量 &gt; 2) ? (CPU数量 * 0.75) : 1  （mapr的官方建议）</p>
  </li>
  <li>
    <p>io.sort.mb    </p>

    <p>每一个map都会对应存在一个内存 buffer， map会将已经产生的部分结果先写入到该buffer中，这个buffer默认是100MB大小. 当map的产生数据非常大时，并且把io.sort.mb调 大，那么map在整个计算过程中spill的次数就势必会降低，map task对磁盘的操作就会变少，如果map tasks的瓶颈在磁盘上，这样调整就会大大提高map的计算性能。</p>
  </li>
  <li>io.sort.spill.percent  <br />
 这个值就是上述buffer的阈值，默认是0.8，既80%，当buffer中的数据达到这个阈值，后台线程会起来对buffer中已有的数据进行排序，然后写入磁盘，此时map输出的数据继续往剩余的20% buffer写数据，如果buffer的剩余20%写满，排序还没结束，map task被block等待。</li>
  <li>
    <p>io.sort.factor   <br />
 当一个map task执行完之后，本地磁盘上(mapred.local.dir)有若干个spill文件，map task最后做的一件事就是执行merge sort，把这些spill文件合成一个文件（partition），有时候我们会自定义partition函数，就是在这个时候被调用。</p>

    <p>当map的中间结果非常大，调大io.sort.factor，有利于减少merge次数，进而减少 map对磁盘的读写频率，有可能达到优化作业的目的。</p>

    <p>默认是10</p>
  </li>
</ol>

<p><strong>Reducer</strong></p>

<ol>
  <li>
    <p>mapred.tasktracker.reduce.tasks.maximum</p>

    <p>The maximum number of reduce tasks that will be run simultaneously by a task tracker.
 一个task tracker最多可以同时运行的reduce任务数量
 默认值：2
 优化值： (CPU数量 &gt; 2) ? (CPU数量 * 0.50): 1 （mapr的官方建议）</p>

    <p>如果使用fair的调度模式，设置成相同，应该是可以的，但是如果是FIFO模式，我个人认为在map或是reduce阶段，CPU的核数没有得到充分的利用，有些可惜，所以，FIFO模式下，还是尽量配置的map并发数量多于redcue并发数量</p>
  </li>
  <li>
    <p>mapred.reduce.tasks
 The default number of reduce tasks per job. Typically set to 99% of the cluster’s reduce capacity, so that if a node fails the reduces can  still be executed in a single wave.</p>

    <pre><code> 一个Job会使用task tracker的reduce任务槽数量
 默认值：1
 优化值：
 1.0.95 * mapred.tasktracker.tasks.maximum
 理由：启用95%的reduce任务槽运行task, recude task运行一轮就可以完成。剩余5%的任务槽永远失败任务，重新执行
 2.1.75 * mapred.tasktracker.tasks.maximum
 理由：因为reduce task数量超过reduce槽数，所以需要两轮才能完成所有reduce task。具体快的原理还没有完全理解.  io.sort.mb/io.sort.factor/ io.sort.spill.percent  和Map类似
</code></pre>
  </li>
  <li>
    <p>mapred.reduce.parallel.copies    </p>

    <p>Reduce copy数据的线程数量，默认值是5</p>

    <p>Reduce task在做shuffle时，实际上就是从不同的已经完成的map上去下载属于自己这个reduce的部分数据，由于map通常有许多个，所以对一个reduce来说，下载也可以是并行的从多个map下载
 Reduce到每个完成的Map Task copy数据（通过RPC调用），默认同时启动5个线程到map节点取数据。这个配置还是很关键的，如果你的map输出数据很大，有时候会发现map早就100%了，reduce一直在1% 2%。。。。。。缓慢的变化，那就是copy数据太慢了，5个线程copy 10G的数据，确实会很慢，这时就要调整这个参数了，但是调整的太大，又会事倍功半，容易造成集群拥堵，所以 Job tuning的同时，也是个权衡的过程，你要熟悉你的数据</p>
  </li>
  <li>
    <p>mapred.job.shuffle.input.buffer.percent</p>

    <p>Reduce在shuffle阶段对下载来的map数据，并不是立刻就写入磁盘的，而是会先缓存在内存中，然后当使用内存达到一定量的时候才刷入磁盘。当指定了JVM的堆内存最大值以后，上面这个配置项就是Reduce用来存放从Map节点取过来的数据所用的内存占堆内存的比例，默认是70%</p>
  </li>
  <li>
    <p>mapred.job.shuffle.merge.percent </p>

    <p>同Map的io.sort.spill.percent 一样，当shuffle时不是等到buffer满后再在flush到磁盘，而是达到一定阈值后写入磁盘，其默认值是0.66</p>
  </li>
  <li>
    <p>mapred.job.reduce.input.buffer.percent </p>

    <p>当reduce task真正进入reduce函数的计算阶段的时候，在读取reduce需要的数据时需要多 少的内存百分比来作为reduce读已经sort好的数据的buffer百分比。默认情况下为0，也就是说，默认情况下，reduce是全部从磁盘开始读 处理数据。如果这个参数大于0，那么就会有一定量的数据被缓存在内存并输送给reduce，当reduce计算逻辑消耗内存很小时，可以分一部分内存用来 缓存数据</p>
  </li>
  <li>
    <p>mapred.inmem.merge.threshold</p>

    <p>从map节点取过来的文件个数，当达到这个个数之后，也进行merger sort，然后写到reduce节点的本地磁盘；mapred.job.shuffle.merge.percent优先判断.默认值1000，完全取决于map输出数据的大小
 默认值1000</p>
  </li>
</ol>

</div>
<div class="share">
  <div class="bdsharebuttonbox">
    <a href="#" class="bds_more" data-cmd="more"></a>
    <a href="#" class="bds_qzone" data-cmd="qzone" title="分享到QQ空间"></a>
    <a href="#" class="bds_tsina" data-cmd="tsina" title="分享到新浪微博"></a>
  </div>
  <script>window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"1","bdMiniList":["qzone","tsina","weixin","sqq"],"bdPic":"","bdStyle":"0","bdSize":"16"},"share":{}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='/res/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];</script>
</div>
<div class="pagination">

<a class="pagination-item newer" href="/blog/2014/10/09/Holographic-user-portrait.html"><i class="fa fa-arrow-left"></i>&nbsp;&nbsp;Holographic user portrait</a>


<a class="pagination-item older" href="/blog/2014/10/09/zookeeper-install.html">zookeeper install&nbsp;&nbsp;<i class="fa fa-arrow-right"></i></a>

</div>
<div class="comments">
  <div class="ds-thread" data-title="Tuning Hadoop" data-thread-key="/blog/2014/10/09/Tuning-Hadoop" data-url="index.html/blog/2014/10/09/Tuning-Hadoop.html"></div>
  <script type="text/javascript">var duoshuoQuery = {short_name:"willgo0815"};(function(){var ds = document.createElement('script');ds.type = 'text/javascript';ds.async = true;ds.src = '/res/js/embed.js';ds.charset = 'UTF-8';(document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(ds);})();</script>
</div>
    <footer>Copyright&nbsp;&copy;&nbsp;2015 <a href="index.html">willgo</a><br/><i class="fa fa-cogs" style="color:blueviolet;">&nbsp;</i>Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a>
</br> <a href="http://m.kuaidi100.com" target="_blank">快递查询</a> 
</footer>

    </div>
  </div>    
  <div id="top"><a id="rocket" href="javascript:;" title="返回顶部"><i></i></a></div>
  
</body>
</html>
