<!DOCTYPE HTML>
<html>
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0">
  <meta name="description" content="Will go, Just do it.">
  <meta name="author" content="Will.Quan">
  <meta name="keywords" content="Will go, Just do it., willgo, 最好的从未错过, Will.Quan">
  <title>Will go, Just do it.</title>
  <link rel="canonical" href="index.html">
  <link rel="icon" href="/res/img/favicon.ico" type="image/x-icon">
  <link rel="shortcut icon" href="/res/img/favicon.ico" type="image/x-icon">
  <link rel="stylesheet" href="/res/css/public.css">
  <link rel="stylesheet" href="/res/css/light.css">
  <script src="/res/js/light.js"></script>
  <script type="text/javascript">
var _bdhmProtocol = (("https:" == document.location.protocol) ? " https://" : " http://");
document.write(unescape("%3Cscript src='" + _bdhmProtocol + "hm.baidu.com/h.js%3Fda48b6233123178f300913a0e707883e' type='text/javascript'%3E%3C/script%3E"));
</script>

</head>
<body>
  <div id="blog">
    <div class="sidebar">
  <div class="profilepic">
    <a href="/"><img src="/res/img/icon.png" alt="logo"></img></a>
  </div>
  <h1 class="title"><a href="/">willgo</a></h1>
  <h2 class="sub-title">最好的从未错过</h2>
  <nav id="nav">
    <ul>
    
      <li><a href="/page/timing.html"><i class="fa fa-clock-o"></i>&nbsp;资料时间</a></li>
    
      <li><a href="/page/category.html"><i class="fa fa-tags"></i>&nbsp;文章分类</a></li>
    
      <li><a href="/page/read.html"><i class="fa fa-book"></i>&nbsp;逗绊读书</a></li>
    
      <li><a href="/page/life.html"><i class="fa fa-eyedropper"></i>&nbsp;生活记录</a></li>
    
      <li><a href="/page/about.html"><i class="fa fa-paper-plane-o"></i>&nbsp;假装关于</a></li>
    
    </ul>
  </nav>  
  <nav id="sub-nav">
    <a class="weibo " href="http://weibo.com/u/2369015654" title="新浪微博" target="_blank"><i class="fa fa-weibo"></i></a>
    <a class="github" href="https://github.com/will0815/" title="GitHub" target="_blank"><i class="fa fa-github fa-2x"></i></a>
    <a class="rss" href="/page/feed.xml" title="RSS订阅" target="_blank"><i class="fa fa-rss"></i></a>
  </nav>
  <div id="license">
    <a href="http://creativecommons.org/licenses/by-nc-sa/3.0/cn/" target="_blank" title="本站所有作品采用：&#10;知识共享《署名 非商业性使用 相同方式共享 3.0》&#10;进行许可" >
    <img alt="License" height="31" width="88" src="/res/img/license.png" /></a>
  </div>
</div>

    <div class="main">
        <div style="font-family: segoepr;font-size: 40px;
              margin-top:-5px; color:#fff;">
            <script type="text/javascript" 
            src="http://open.iciba.com/ds_open.php?id=11519&name=willgo&auth=BE45CDE6481CC46336529A1B407855B1" charset="utf-8">
            </script>
        </div>
    
<div class="posts">
  <section class="post">
    <header class="post-header">
      <h1 class="post-title"><a href="/blog/2014/10/09/zookeeper-install.html">zookeeper install</a></h1>
      <p class="post-meta">
        <i class="fa fa-calendar"></i>
        2014年10月09日
        <i class="space"></i>
        <i class="fa fa-tags"></i>
        <a class="post-category" href="/page/category.html#hadoop">hadoop</a>
      </p>
    </header>
    <div class="post-main">
      <p><strong>Zookeeper集群安装配置</strong></p>

<ol>
  <li>
    <p>修改zookeeper配置文件zoo.cfg</p>

    <p>在Linux系统下解压zookeeper安装包zookeeper-3.4.3.tar.gz ，进入到conf目录，将zoo_sample.cfg拷贝一份命名为zoo.cfg（Zookeeper 在启动时会找这个文件作为默认配置文件），打开该文件进行修改为以下格式</p>

    <pre><code> dataDir=/home/hadoop/temp/zookeeper/data
 server.23=192.168.6.23:2888:3888
 server.24=192.168.6.24:2888:3888
 server.25=192.168.6.25:2888:3888
</code></pre>
  </li>
  <li>
    <p>新建目录、新建并编辑myid文件
（本次配置myid文件放在/home/hadoop/temp/zookeeper/data目录下）</p>

    <pre><code> mkdir /home/hadoop/will/zookeeper/data	//dataDir目录
 vi /home/hadoop/temp/zookeeper/data/myid  注意myid文件中的内容为：Master中为0，Slave1中为1，Slave2中为2，分别与zoo.cfg中对应起来。
</code></pre>
  </li>
  <li>
    <p>同步安装包</p>

    <p>将解压修改后的zookeeper-3.4.3文件夹分别拷贝到Master、Slave1、Slave2的相同zookeeper安装路径下。注意：myid文件的内容不是一样的，各服务器中分别是对应zoo.cfg中的设置。</p>
  </li>
  <li>
    <p>启动zookeeper
 Zookeeper的启动与hadoop不一样，需要每个节点都执行，分别进入3个节点的zookeeper-3.4.3目录，启动zookeeper：</p>

    <pre><code> bin/zkServer.sh start  注意：此时如果报错先不理会，需要继续在另两台服务器中执行相同操作。 
</code></pre>
  </li>
  <li>
    <p>检查zookeeper是否配置成功
 待3台服务器均启动后，如果过程正确的话zookeeper应该已经自动选好leader，进入每台服务器的zookeeper-3.4.3目录，执行以下操作查看zookeeper启动状态：</p>

    <pre><code> bin/zkServer.sh status  如果出现以下代码表示安装成功了。

 [java] view plaincopy
 JMX enabled by default  
 Using config: /home/hadoop/zookeeper-3.4.3/bin/../conf/zoo.cfg  
 Mode: follower	//或者有且只有一个leader
</code></pre>
  </li>
</ol>

<h2 id="zookeeper">2 - 解读zookeeper的配置项</h2>
<p>zookeeper的默认配置文件为zookeeper/conf/zoo_sample.cfg，需要将其修改为zoo.cfg。其中各配置项的含义，解释如下：</p>

<ol>
  <li>
    <p>tickTime：CS通信心跳数</p>

    <p>Zookeeper 服务器之间或客户端与服务器之间维持心跳的时间间隔，也就是每个 tickTime 时间就会发送一个心跳。tickTime以毫秒为单位。
 1.tickTime=2000  </p>
  </li>
  <li>
    <p>initLimit：LF初始通信时限</p>

    <p>集群中的follower服务器(F)与leader服务器(L)之间初始连接时能容忍的最多心跳数（tickTime的数量）。
 1.initLimit=5  </p>
  </li>
  <li>
    <p>syncLimit：LF同步通信时限</p>

    <p>集群中的follower服务器与leader服务器之间请求和应答之间能容忍的最多心跳数（tickTime的数量）。
 1.syncLimit=2  
 </p>
  </li>
  <li>
    <p>dataDir：数据文件目录</p>

    <p>Zookeeper保存数据的目录，默认情况下，Zookeeper将写数据的日志文件也保存在这个目录里。
 1.dataDir=/home/michael/opt/zookeeper/data  </p>
  </li>
  <li>
    <p>dataLogDir：日志文件目录</p>

    <p>Zookeeper保存日志文件的目录。
 1.dataLogDir=/home/michael/opt/zookeeper/log  </p>
  </li>
  <li>
    <p>clientPort：客户端连接端口</p>

    <p>客户端连接 Zookeeper 服务器的端口，Zookeeper 会监听这个端口，接受客户端的访问请求。
 1.clientPort=2333  </p>
  </li>
  <li>
    <p>服务器名称与地址：集群信息（服务器编号，服务器地址，LF通信端口，选举端口）</p>

    <p>这个配置项的书写格式比较特殊，规则如下：
 1.server.N=YYY:A:B  </p>

    <p>其中N表示服务器编号，YYY表示服务器的IP地址，A为LF通信端口，表示该服务器与集群中的leader交换的信息的端口。B为选举端口，表示选举新leader时服务器间相互通信的端口（当leader挂掉时，其余服务器会相互通信，选择出新的leader）。一般来说，集群中每个服务器的A端口都是一样，每个服务器的B端口也是一样。但是当所采用的为伪集群时，IP地址都一样，只能时A端口和B端口不一样。
 如：</p>

    <pre><code> 1.server.0=192.168.6.23:2008:6008  
 2.server.1=192.168.6.24:2008:6008  
 3.server.2=192.168.6.25:2008:6008  
 4.server.3=192.168.6.26:2008:6008  
</code></pre>
  </li>
</ol>

      <div class="readall"><a href="/blog/2014/10/09/zookeeper-install.html" id="post-readall">阅读全文&nbsp;<i class="fa fa-chevron-right"></i></a></div>
    </div>
  </section>
</div>

<div class="posts">
  <section class="post">
    <header class="post-header">
      <h1 class="post-title"><a href="/blog/2014/10/09/Tuning-Hadoop.html">Tuning Hadoop</a></h1>
      <p class="post-meta">
        <i class="fa fa-calendar"></i>
        2014年10月09日
        <i class="space"></i>
        <i class="fa fa-tags"></i>
        <a class="post-category" href="/page/category.html#hadoop">hadoop</a>
      </p>
    </header>
    <div class="post-main">
      <h2 id="hadoop">Hadoop集群性能调优</h2>

<p><strong>mapred-site.xml</strong></p>

<ol>
  <li>
    <p>mapred.child.java.opts</p>

    <p>配置每个map或reduce使用的内存数量。默认的是200M。网友推荐：如果内存是8G，CPU有8个核，那么就设置成1G就可以了。实际上，在map和reduce的过程中对内存的消耗并不大，但是如果配置的太小，则有可能出现”无可分配内存”的错误。对于Hadoop环境独占的机器可以考虑：总内存 / (Map/Reduce 并发数) * 80%</p>
  </li>
  <li>
    <p>dfs.block.size </p>

    <p>生的map来综合考量的。一般来说，文件大，集群数量少，还是建议将block size设置大一些的好。</p>
  </li>
  <li>
    <p>mapred.compress.map.output </p>

    <p>几乎所有会产生大量map output的hadoop作业，都能够通过将中间文件用Lzo进行压缩来提升性能。尽管lzo会使得cpu的load有所增高，但是reduce在shuffle阶段的disk IO通常都能节省不少时间，对job的性能有好处。
 当采用map中间结果压缩的情况下，用户还可以选择压缩时采用另外几种压缩格式进行压缩，现在hadoop支持的压缩格式 有：GzipCodec，LzoCodec，BZip2Codec，LzmaCodec等压缩格式。通常来说，想要达到比较平衡的cpu和磁盘压缩 比，LzoCodec比较适合。但也要取决于job的具体情况。</p>

    <pre><code> &lt;property&gt;
     &lt;name&gt;mapred.compress.map.output&lt;/name&gt;
     &lt;value&gt;true&lt;/value&gt;
   &lt;/property&gt;
 &lt;property&gt; 
     &lt;name&gt;mapred.map.output.compression.codec&lt;/name&gt; 
     &lt;value&gt;com.hadoop.compression.lzo.LzoCodec&lt;/value&gt; 
 &lt;/property&gt;  
</code></pre>
  </li>
  <li>
    <p>mapred.tasktracker.map.tasks.maximum</p>

    <p>The maximum number of map tasks that will be run simultaneously by a task tracker.</p>

    <p>一个tasktracker最多可以同时运行的map任务数量
 默认值：2
 mapred.tasktracker.map.tasks.maximum = cpu数量
 Sanders: 应该设置成为等于或略小于CPU总核数
 cpu数量 = 服务器CPU总核数 / 每个CPU的核数
 服务器CPU总核数 = more  /proc/cpuinfo | grep ‘processor’ | wc -l
 每个CPU的核数 = more  /proc/cpuinfo | grep ‘cpu cores’</p>
  </li>
  <li>
    <p>mapred.map.tasks</p>

    <p>The default number of map tasks per job
 一个Job会使用task tracker的map任务槽数量，这个值 ≤ mapred.tasktracker.map.tasks.maximum
 默认值：2
 优化值：
 CPU数量 （我们目前的实践值）
 (CPU数量 &gt; 2) ? (CPU数量 * 0.75) : 1  （mapr的官方建议）</p>
  </li>
  <li>
    <p>io.sort.mb    </p>

    <p>每一个map都会对应存在一个内存 buffer， map会将已经产生的部分结果先写入到该buffer中，这个buffer默认是100MB大小. 当map的产生数据非常大时，并且把io.sort.mb调 大，那么map在整个计算过程中spill的次数就势必会降低，map task对磁盘的操作就会变少，如果map tasks的瓶颈在磁盘上，这样调整就会大大提高map的计算性能。</p>
  </li>
  <li>io.sort.spill.percent  <br />
 这个值就是上述buffer的阈值，默认是0.8，既80%，当buffer中的数据达到这个阈值，后台线程会起来对buffer中已有的数据进行排序，然后写入磁盘，此时map输出的数据继续往剩余的20% buffer写数据，如果buffer的剩余20%写满，排序还没结束，map task被block等待。</li>
  <li>
    <p>io.sort.factor   <br />
 当一个map task执行完之后，本地磁盘上(mapred.local.dir)有若干个spill文件，map task最后做的一件事就是执行merge sort，把这些spill文件合成一个文件（partition），有时候我们会自定义partition函数，就是在这个时候被调用。</p>

    <p>当map的中间结果非常大，调大io.sort.factor，有利于减少merge次数，进而减少 map对磁盘的读写频率，有可能达到优化作业的目的。</p>

    <p>默认是10</p>
  </li>
</ol>

<p><strong>Reducer</strong></p>

<ol>
  <li>
    <p>mapred.tasktracker.reduce.tasks.maximum</p>

    <p>The maximum number of reduce tasks that will be run simultaneously by a task tracker.
 一个task tracker最多可以同时运行的reduce任务数量
 默认值：2
 优化值： (CPU数量 &gt; 2) ? (CPU数量 * 0.50): 1 （mapr的官方建议）</p>

    <p>如果使用fair的调度模式，设置成相同，应该是可以的，但是如果是FIFO模式，我个人认为在map或是reduce阶段，CPU的核数没有得到充分的利用，有些可惜，所以，FIFO模式下，还是尽量配置的map并发数量多于redcue并发数量</p>
  </li>
  <li>
    <p>mapred.reduce.tasks
 The default number of reduce tasks per job. Typically set to 99% of the cluster’s reduce capacity, so that if a node fails the reduces can  still be executed in a single wave.</p>

    <pre><code> 一个Job会使用task tracker的reduce任务槽数量
 默认值：1
 优化值：
 1.0.95 * mapred.tasktracker.tasks.maximum
 理由：启用95%的reduce任务槽运行task, recude task运行一轮就可以完成。剩余5%的任务槽永远失败任务，重新执行
 2.1.75 * mapred.tasktracker.tasks.maximum
 理由：因为reduce task数量超过reduce槽数，所以需要两轮才能完成所有reduce task。具体快的原理还没有完全理解.  io.sort.mb/io.sort.factor/ io.sort.spill.percent  和Map类似
</code></pre>
  </li>
  <li>
    <p>mapred.reduce.parallel.copies    </p>

    <p>Reduce copy数据的线程数量，默认值是5</p>

    <p>Reduce task在做shuffle时，实际上就是从不同的已经完成的map上去下载属于自己这个reduce的部分数据，由于map通常有许多个，所以对一个reduce来说，下载也可以是并行的从多个map下载
 Reduce到每个完成的Map Task copy数据（通过RPC调用），默认同时启动5个线程到map节点取数据。这个配置还是很关键的，如果你的map输出数据很大，有时候会发现map早就100%了，reduce一直在1% 2%。。。。。。缓慢的变化，那就是copy数据太慢了，5个线程copy 10G的数据，确实会很慢，这时就要调整这个参数了，但是调整的太大，又会事倍功半，容易造成集群拥堵，所以 Job tuning的同时，也是个权衡的过程，你要熟悉你的数据</p>
  </li>
  <li>
    <p>mapred.job.shuffle.input.buffer.percent</p>

    <p>Reduce在shuffle阶段对下载来的map数据，并不是立刻就写入磁盘的，而是会先缓存在内存中，然后当使用内存达到一定量的时候才刷入磁盘。当指定了JVM的堆内存最大值以后，上面这个配置项就是Reduce用来存放从Map节点取过来的数据所用的内存占堆内存的比例，默认是70%</p>
  </li>
  <li>
    <p>mapred.job.shuffle.merge.percent </p>

    <p>同Map的io.sort.spill.percent 一样，当shuffle时不是等到buffer满后再在flush到磁盘，而是达到一定阈值后写入磁盘，其默认值是0.66</p>
  </li>
  <li>
    <p>mapred.job.reduce.input.buffer.percent </p>

    <p>当reduce task真正进入reduce函数的计算阶段的时候，在读取reduce需要的数据时需要多 少的内存百分比来作为reduce读已经sort好的数据的buffer百分比。默认情况下为0，也就是说，默认情况下，reduce是全部从磁盘开始读 处理数据。如果这个参数大于0，那么就会有一定量的数据被缓存在内存并输送给reduce，当reduce计算逻辑消耗内存很小时，可以分一部分内存用来 缓存数据</p>
  </li>
  <li>
    <p>mapred.inmem.merge.threshold</p>

    <p>从map节点取过来的文件个数，当达到这个个数之后，也进行merger sort，然后写到reduce节点的本地磁盘；mapred.job.shuffle.merge.percent优先判断.默认值1000，完全取决于map输出数据的大小
 默认值1000</p>
  </li>
</ol>

      <div class="readall"><a href="/blog/2014/10/09/Tuning-Hadoop.html" id="post-readall">阅读全文&nbsp;<i class="fa fa-chevron-right"></i></a></div>
    </div>
  </section>
</div>

<div class="posts">
  <section class="post">
    <header class="post-header">
      <h1 class="post-title"><a href="/blog/2014/10/09/Holographic-user-portrait.html">Holographic user portrait</a></h1>
      <p class="post-meta">
        <i class="fa fa-calendar"></i>
        2014年10月09日
        <i class="space"></i>
        <i class="fa fa-tags"></i>
        <a class="post-category" href="/page/category.html#hadoop">hadoop</a>
      </p>
    </header>
    <div class="post-main">
      <p><strong>用户洞察的定义和目的</strong></p>

<p>  标准的用户洞察包含三个步骤：用户的数据管理、用户的需求分析、分析以后建立用户洞察力的应用。</p>

<p>  用户的数据包括：历史销售数据、用户评价数据、用户注册数据、用户保修数据、用户维修数据、用户社交数据。</p>

<p>用户的需求分析即对用户信息的理解，并进行适应性的建模，通过动态的行为和价值分析，识别用户的行为、价值和需求。在用户数据管理与用户需求分析的基础上，策划、开发和提供蕴含用户实质需求的差异化产品、业务或营销方案，开展有针对性的营销和拓展市场。</p>

<p>   因此用户洞察的目的就是通过已有的需求或者已经产生的交易行为记录，对这些用户进行分析，然后做精准营销或者挖掘潜在需求。</p>

<p><img src="/image/user-holograph.png" alt="" /></p>

<p><strong>用户画像模型分析</strong></p>

<p>   用户画像是用户洞察中最为关键的环节，比如说他是谁，他的消费能力如何，他过往的消费历史是什么，他的购物习惯和社交偏好是什么。通过简单的方式或者是非常有限的数据，就能够找到这些内容。建立起丰富的画像标签体系，这个体系需要涵盖我们希望描绘用户的各个方面，原始数据可以动态获取，也可以基于原始数据集。 可以过曾经的行为推测出来他的标签或者是他的特征。举例来说：如果她叫翠花，那么她是女生；如果他是给丈母娘买的，那么猜测他是男生；如果她在2012年10月份买了一段奶粉，2013年2月买了二段奶粉，那么可以推测她有孩子，并且孩子已经两岁半了；如果他买了高端的A.O史密斯热水器，那么他的消费能力相对比较强；如果一位用户关注科技，比如说可穿戴设备，那么他一定是科技达人；如果他买了净水器，他可能对空气净化器有要求。通过这些历史数据、SNS数据或者行为数据，探究同一个客户有没有二次购买的可能、以及关联购买能力。</p>

<p><strong>基于交易数据的用户洞察</strong></p>

<p>  根据电商的交易数据对个人消费者用户进行分析，从消费内容的数据中洞悉该消费者的消费习惯和其他信息：根据用户消费能力级别数据可以了解消费者的消费能力；根据用户的购买次数分析其购物习惯，判定他是否是网购达人；根据用户的上网时间分布数据，分析消费者的职业、作息等信息；根据用户单次消费金额数据分析消费者的购买能力和收入状况等。例如，一名消费者在2013年12月份到2014年4月份有五次消费，从他这五次交易的数据里面可以看到：他买了两次手机，还买了豆浆机、电视机；第一次购买手机的时候花了四千块，但第二次购买手机的时候只花了1950元。可以看出他热爱生活，是一个十足的数码控，消费能力很强，网购习惯很明显。当然这些数据不一定很准，但是也不需要它很准。只要能够做出画像就足够了。</p>

<p>   通过分析，认识了他，知道他叫什么，也知道他在哪里，然后进行的就是用户洞察力的行为应用：
电话他、短信他、邮件他、私信他，用打折、促销、返券、送积分、给链接、门店地址等方式吸引他、勾引他，全方位地缠绕他。这种全方位的营销到底能不能产生消费不敢肯定，但是这样的促销行为对一部分人一定会产生影响。</p>

<p> 基于论坛数据的用户洞察在对论坛的研究方面，提取了某消费者论坛的监测数据，样本量是24万的用
户，其中签到的人达到了80万，购买信息数达到200万。对论坛用户年龄的分析，可以发现绝大部分都是85后和90后，这两部分用户占到70%以上，因此可以认定这是一个新型的消费领域。从学历方面来分析这些用户，80%以上的用户为专科以上学历，这些人渴望成功，渴望归属和友情，他们勇敢、时尚，但是消费能力不够。</p>

<p>   在这个论坛用户结构中，普通人占到50%多，而骨灰级的粉丝占到40.3%，粉丝如何运营就是目前面临的最大问题。现在很多企业在做社会化的营销，在做粉丝运营，但是坦率来讲，做的都不是特别理想。目前家电行业都在向互联网思维转变，在产品和技术方面，企业并没有多大的问题，如果说企业还有哪些欠缺的话，应该就是思维上的转变、专业人才的培育与经验的积累。</p>

<p>   在粉丝运营中，找准时间可以让营销事半功倍。之前所有电商网站购销行为，基本上都是在晚上11点到12点。但是论坛则不一样，如果企业有官网或者是官微，那么可以发现粉丝活跃时间是在午休时间，如果要做活动宣传、广告或者品牌推广，午休时间是个好选择。如果是做抢购、秒杀、团购、促销的活动，最好放在晚上。</p>

<p><strong>基于社会化用户的深度挖掘成为企业关注重点</strong></p>

<p>   如果只是基于在现有数据的条件下，进行深入挖掘以及用户分析，可能还远远不够，这时需要建立一个目标交互引流的分析模型，这个模型通过对线上用户精确的界定抓取，用于企业的用户运营、品牌分析。随着互联网和社会化的发展，企业需要对线上用户进行精确的分析和洞察。</p>

<p>   具体如何操作？首先要分析在互联网的行为下，用户对产品和品牌究竟有什么样的看法和观点，即满意度如何；还有产品是否迭代，原因是什么，企业的痛点又在哪里，有没有可能推送新的迭代创意或者是颠覆性的想法。通过这些方面，进行社会化的数据抓取，得到标准化的期望值，进行简单的画像，然后再进行分析，抽取相关的报表报告，提供给企业进行产品的设计和创意的输出。</p>

<p><strong>构建基于社会化大数据的用户洞察和交互体系</strong></p>

<p>   总结前面的分析，可以看出基于大数据的用户洞察和交互体系可以归纳为：基于营销为目的：分析用户的历史销售记录，基于用户群体，获取SNS的数据，处理清洗SNS数据，挖掘用户潜在需求，针对潜在客户进行精准的个性化、差异化营销和推荐，形成销售业绩。
   基于洞察为目的：通过社会化媒体和电商平台找到关键的用户意见领袖和SNS用户，对他们细分得到相关的群体，通过社会化运营，可以对他进行调研，广告推送，或者搭建新一代用户调研系统，未来这将是能够解决产品跟进、快速升级的好平台。</p>

      <div class="readall"><a href="/blog/2014/10/09/Holographic-user-portrait.html" id="post-readall">阅读全文&nbsp;<i class="fa fa-chevron-right"></i></a></div>
    </div>
  </section>
</div>

<div class="posts">
  <section class="post">
    <header class="post-header">
      <h1 class="post-title"><a href="/blog/2014/09/27/ubuntueclipse.html">ubuntu下eclipse菜单栏无法展开</a></h1>
      <p class="post-meta">
        <i class="fa fa-calendar"></i>
        2014年09月27日
        <i class="space"></i>
        <i class="fa fa-tags"></i>
        <a class="post-category" href="/page/category.html#linux">linux</a>
      </p>
    </header>
    <div class="post-main">
      <p>因为ubuntu13下菜单栏被代理到状态栏下，下载安装eclipse3.8，打开后菜单栏点击无法打开，网上认为可能是unity的bug</p>

<p>解决办法去掉eclipse的菜单代理：</p>

<p>1.在终端中进入快捷方式的目录，然后运行</p>

<p>sudo gedit eclipse.desktop</p>

<p>2.修改Exec的值，将原有的Exec=eclipse修改为：</p>

<p>Exec=env UBUNTU_MENUPROXY=0  eclipse</p>

<p>——问题解决</p>

<p>顺带学习ubuntu 程序快捷方式</p>

<p>进入 /usr/share/applications 目录下，这里有所有程序的图形化的快捷方式，
你可以直接复制到桌面，即可</p>

<p>搜索ls <em>程序名</em></p>

<p>方法一：进入 /usr/share/applications 目录下，这里有所有程序的图形化的快捷方式，你可以直接复制到桌面，即可</p>

<p>方法二：使用命令的方式： gnome-desktop-item-edit</p>


      <div class="readall"><a href="/blog/2014/09/27/ubuntueclipse.html" id="post-readall">阅读全文&nbsp;<i class="fa fa-chevron-right"></i></a></div>
    </div>
  </section>
</div>

<div class="posts">
  <section class="post">
    <header class="post-header">
      <h1 class="post-title"><a href="/blog/2014/09/27/sqoop.html">sqoop安装</a></h1>
      <p class="post-meta">
        <i class="fa fa-calendar"></i>
        2014年09月27日
        <i class="space"></i>
        <i class="fa fa-tags"></i>
        <a class="post-category" href="/page/category.html#hadoop">hadoop</a>
      </p>
    </header>
    <div class="post-main">
      <p>1、下载sqoop压缩包，并解压</p>

<p>压缩包分别是：sqoop-1.2.0-CDH3B4.tar.gz，hadoop-0.20.2-CDH3B4.tar.gz， Mysql JDBC驱动包mysql-connector-java-5.1.10-bin.jar</p>

<pre><code>[root@node1 ~]# ll
drwxr-xr-x 15 root  root      4096 Feb 22  2011 hadoop-0.20.2-CDH3B4
-rw-r--r--  1 root  root    724225 Sep 15 06:46 mysql-connector-java-5.1.10-bin.jar
drwxr-xr-x 11 root  root      4096 Feb 22  2011 sqoop-1.2.0-CDH3B4
</code></pre>

<p>2、将sqoop-1.2.0-CDH3B4拷贝到/home/hadoop目录下，并将Mysql JDBC驱动包和hadoop-0.20.2-CDH3B4下的hadoop-core-0.20.2-CDH3B4.jar至sqoop-1.2.0-CDH3B4/lib下，最后修改一下属主。</p>

<pre><code>[root@node1 ~]# cp mysql-connector-java-5.1.10-bin.jar sqoop-1.2.0-CDH3B4/lib
[root@node1 ~]# cp hadoop-0.20.2-CDH3B4/hadoop-core-0.20.2-CDH3B4.jar sqoop-1.2.0-CDH3B4/lib
[root@node1 ~]# chown -R hadoop:hadoop sqoop-1.2.0-CDH3B4
[root@node1 ~]# mv sqoop-1.2.0-CDH3B4 /home/hadoop
[root@node1 ~]# ll /home/hadoop
total 35748
-rw-rw-r--  1 hadoop hadoop      343 Sep 15 05:13 derby.log
drwxr-xr-x 13 hadoop hadoop     4096 Sep 14 16:16 hadoop-0.20.2
drwxr-xr-x  9 hadoop hadoop     4096 Sep 14 20:21 hive-0.10.0
-rw-r--r--  1 hadoop hadoop 36524032 Sep 14 20:20 hive-0.10.0.tar.gz
drwxr-xr-x  8 hadoop hadoop     4096 Sep 25  2012 jdk1.7
drwxr-xr-x 12 hadoop hadoop     4096 Sep 15 00:25 mahout-distribution-0.7
drwxrwxr-x  5 hadoop hadoop     4096 Sep 15 05:13 metastore_db
-rw-rw-r--  1 hadoop hadoop      406 Sep 14 16:02 scp.sh
drwxr-xr-x 11 hadoop hadoop     4096 Feb 22  2011 sqoop-1.2.0-CDH3B4
drwxrwxr-x  3 hadoop hadoop     4096 Sep 14 16:17 temp
drwxrwxr-x  3 hadoop hadoop     4096 Sep 14 15:59 user
</code></pre>

<p>3、配置configure-sqoop，注释掉对于HBase和ZooKeeper的检查</p>

<pre><code>[root@node1 bin]# pwd
/home/hadoop/sqoop-1.2.0-CDH3B4/bin
[root@node1 bin]# vi configure-sqoop 

#!/bin/bash
#
# Licensed to Cloudera, Inc. under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
.
.
.
# Check: If we can't find our dependencies, give up here.
if [ ! -d "${HADOOP_HOME}" ]; then
  	echo "Error: $HADOOP_HOME does not exist!"
  	echo 'Please set $HADOOP_HOME to the root of your Hadoop installation.'
  	exit 1
fi
#if [ ! -d "${HBASE_HOME}" ]; then
#  echo "Error: $HBASE_HOME does not exist!"
#  echo 'Please set $HBASE_HOME to the root of your HBase installation.'
#  exit 1
#fi
#if [ ! -d "${ZOOKEEPER_HOME}" ]; then
#  echo "Error: $ZOOKEEPER_HOME does not exist!"
#  echo 'Please set $ZOOKEEPER_HOME to the root of your ZooKeeper installation.'
#  exit 1
#fi
</code></pre>

<p>4、修改/etc/profile和.bash_profile文件，添加Hadoop_Home,调整PATH
	[hadoop@node1 ~]$ vi .bash_profile </p>

<pre><code># .bash_profile

# Get the aliases and functions
if [ -f ~/.bashrc ]; then
        . ~/.bashrc
fi

# User specific environment and startup programs

HADOOP_HOME=/home/hadoop/hadoop-0.20.2
PATH=$HADOOP_HOME/bin:$PATH:$HOME/bin
export HIVE_HOME=/home/hadoop/hive-0.10.0
export MAHOUT_HOME=/home/hadoop/mahout-distribution-0.7
export PATH HADOOP_HOME
</code></pre>

<p>三、测试Sqoop</p>

<ol>
  <li>
    <p>查看mysql中的数据库：</p>

    <pre><code> [hadoop@node1 bin]$ ./sqoop list-databases --connect jdbc:mysql://192.168.1.152:3306/ --username sqoop --password sqoop
</code></pre>
  </li>
  <li>
    <p>将mysql的表导入到hive中：</p>

    <pre><code> [hadoop@node1 bin]$ ./sqoop import --connect jdbc:mysql://192.168.1.152:3306/sqoop --username sqoop --password sqoop --table test --hive-import -m 1
</code></pre>
  </li>
</ol>

      <div class="readall"><a href="/blog/2014/09/27/sqoop.html" id="post-readall">阅读全文&nbsp;<i class="fa fa-chevron-right"></i></a></div>
    </div>
  </section>
</div>

<div class="posts">
  <section class="post">
    <header class="post-header">
      <h1 class="post-title"><a href="/blog/2014/09/27/sql-frist-day.html">sql frist day</a></h1>
      <p class="post-meta">
        <i class="fa fa-calendar"></i>
        2014年09月27日
        <i class="space"></i>
        <i class="fa fa-tags"></i>
        <a class="post-category" href="/page/category.html#db">db</a>
      </p>
    </header>
    <div class="post-main">
      <p>计算周月的第一天， 以星期天为一周的开始</p>

<pre><code>	SELECT LAST_DAY('2013-2-1');
	SELECT concat(date_format(LAST_DAY('2013-02-01'),'%Y-%m-'),'01');
	select DATE_SUB('2014-03-08',INTERVAL (WEEKDAY('2014-03-08')+1)%7 DAY);
	select DATE_SUB('2014-03-08',INTERVAL ((WEEKDAY('2014-03-08')+1)%7)-6 DAY);
</code></pre>

      <div class="readall"><a href="/blog/2014/09/27/sql-frist-day.html" id="post-readall">阅读全文&nbsp;<i class="fa fa-chevron-right"></i></a></div>
    </div>
  </section>
</div>

<div class="pagination">
  
  
  <a class="pagination-item newer" href="/page/6"><i class="fa fa-arrow-left"></i>&nbsp;&nbsp;上一页</a>
  
    
  
  <a class="pagination-item older" href="/page/8">下一页&nbsp;&nbsp;<i class="fa fa-arrow-right"></i></a>
  
</div>
    <footer>Copyright&nbsp;&copy;&nbsp;2015 <a href="index.html">willgo</a><br/><i class="fa fa-cogs" style="color:blueviolet;">&nbsp;</i>Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a>
</br> <a href="http://m.kuaidi100.com" target="_blank">快递查询</a> 
</footer>

    </div>
  </div>    
  <div id="top"><a id="rocket" href="javascript:;" title="返回顶部"><i></i></a></div>
  
</body>
</html>
