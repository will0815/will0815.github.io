<!DOCTYPE HTML>
<html>
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0">
  <meta name="description" content="Will go, Just do it.">
  <meta name="author" content="Will.Quan">
  <meta name="keywords" content="Will go, Just do it., willgo, 最好的从未错过, Will.Quan">
  <title>Will go, Just do it.</title>
  <link rel="canonical" href="index.html">
  <link rel="icon" href="/res/img/favicon.ico" type="image/x-icon">
  <link rel="shortcut icon" href="/res/img/favicon.ico" type="image/x-icon">
  <link rel="stylesheet" href="/res/css/public.css">
  <link rel="stylesheet" href="/res/css/light.css">
  <script src="/res/js/light.js"></script>
  <script type="text/javascript">
var _bdhmProtocol = (("https:" == document.location.protocol) ? " https://" : " http://");
document.write(unescape("%3Cscript src='" + _bdhmProtocol + "hm.baidu.com/h.js%3Fda48b6233123178f300913a0e707883e' type='text/javascript'%3E%3C/script%3E"));
</script>

</head>
<body>
  <div id="blog">
    <div class="sidebar">
  <div class="profilepic">
    <a href="/"><img src="/res/img/icon.png" alt="logo"></img></a>
  </div>
  <h1 class="title"><a href="/">willgo</a></h1>
  <h2 class="sub-title">最好的从未错过</h2>
  <nav id="nav">
    <ul>
    
      <li><a href="/page/timing.html"><i class="fa fa-clock-o"></i>&nbsp;资料时间</a></li>
    
      <li><a href="/page/category.html"><i class="fa fa-tags"></i>&nbsp;文章分类</a></li>
    
      <li><a href="/page/read.html"><i class="fa fa-book"></i>&nbsp;逗绊读书</a></li>
    
      <li><a href="/page/life.html"><i class="fa fa-eyedropper"></i>&nbsp;生活记录</a></li>
    
      <li><a href="/page/about.html"><i class="fa fa-paper-plane-o"></i>&nbsp;假装关于</a></li>
    
    </ul>
  </nav>  
  <nav id="sub-nav">
    <a class="weibo " href="http://weibo.com/u/2369015654" title="新浪微博" target="_blank"><i class="fa fa-weibo"></i></a>
    <a class="github" href="https://github.com/will0815/" title="GitHub" target="_blank"><i class="fa fa-github fa-2x"></i></a>
    <a class="rss" href="/page/feed.xml" title="RSS订阅" target="_blank"><i class="fa fa-rss"></i></a>
  </nav>
  <div id="license">
    <a href="http://creativecommons.org/licenses/by-nc-sa/3.0/cn/" target="_blank" title="本站所有作品采用：&#10;知识共享《署名 非商业性使用 相同方式共享 3.0》&#10;进行许可" >
    <img alt="License" height="31" width="88" src="/res/img/license.png" /></a>
  </div>
</div>

    <div class="main">
        <div style="font-family: segoepr;font-size: 40px;
              margin-top:-5px; color:#fff;">
            <script type="text/javascript" 
            src="http://open.iciba.com/ds_open.php?id=11519&name=willgo&auth=BE45CDE6481CC46336529A1B407855B1" charset="utf-8">
            </script>
        </div>
    
<div class="posts">
  <section class="post">
    <header class="post-header">
      <h1 class="post-title"><a href="/blog/2014/09/27/javascript-session-.html">javascript session 记录</a></h1>
      <p class="post-meta">
        <i class="fa fa-calendar"></i>
        2014年09月27日
        <i class="space"></i>
        <i class="fa fa-tags"></i>
        <a class="post-category" href="/page/category.html#f2e">f2e</a>
      </p>
    </header>
    <div class="post-main">
      <ol>
  <li>
    <p>function
全局方法:    this—&gt;window对象
对象内方法： this—&gt;当前对象
构造方法：   初始化构造方法需要new关键字， 否则就会当成全局方法定义
构造方法首字母大写</p>
  </li>
  <li>
    <p>apply和call方法
apply（指定调用方法中的this, 方法的参数数组）;
call(指定调用方法中的this, 方法的参数列表);
foo.call(this, arg1,arg2,arg3) == foo.apply(this, arguments)==this.foo(arg1, arg2, arg3)
在JavaScript中,代码总是有一个上下文对象,代码处理该对象之内. 上下文对象是通过this变量来体现的, 这个this变量永远指向当前代码所处的对象中.</p>
  </li>
</ol>

<p>此类方法的应用场景：
A, B类都有一个message属性(面向对象中所说的成员),A有获取消息的getMessage方法,B有设置消息的setMessage方法
//创建一个B类实例对象
var b = new B();
//给对象a动态指派b的setMessage方法,注意,a本身是没有这方法的!
b.setMessage.call(a, “a的消息”);
//下面将显示”a的消息”
alert(a.getMessage());</p>

<ol>
  <li>
    <p>argments
方法的参数数组，一般在实现不定长参数的方法时会用到这个属性
可以通过它获得参数长度，参数值。
一般情况不通过它修改参数值。</p>
  </li>
  <li>
    <p>闭包问题</p>
  </li>
  <li>
    <p>module</p>
  </li>
  <li>
    <p>继承，
javascript的继承发生在对象与对象之间，
可以通过以下方法实现：
objectCreate(o) {
var F = function() {};
F.prototype = o;
return new F();
}</p>
  </li>
</ol>

      <div class="readall"><a href="/blog/2014/09/27/javascript-session-.html" id="post-readall">阅读全文&nbsp;<i class="fa fa-chevron-right"></i></a></div>
    </div>
  </section>
</div>

<div class="posts">
  <section class="post">
    <header class="post-header">
      <h1 class="post-title"><a href="/blog/2014/09/27/java-SSH-.html">java SSH 远程操作</a></h1>
      <p class="post-meta">
        <i class="fa fa-calendar"></i>
        2014年09月27日
        <i class="space"></i>
        <i class="fa fa-tags"></i>
        <a class="post-category" href="/page/category.html#java">java</a>
      </p>
    </header>
    <div class="post-main">
      <pre><code>public class RmtShellExecutor {
private static Logger logger = Logger.getLogger(RmtShellExecutor.class);
private static Connection conn;
private static String ip;
private static String usr;

private static boolean connection() throws IOException, URISyntaxException {
try {
conn = new Connection(ip);
conn.connect();
} catch (IOException e) {
logger.error("Login remote machine failed" + ip, e);
throw new IOException("Login remote machine failed" + ip, e);
}
return conn.authenticateWithPublicKey(usr, new File(RmtShellExecutor.class.getClassLoader().getResource("id_rsa").toURI())
, null);
}

public static Map&lt;String, Object&gt; exec(String ipStr, String userNameStr, String cmds) throws Exception {
ip = ipStr;
usr = userNameStr;
Map&lt;String, Object&gt; map = new HashMap&lt;String, Object&gt;();
InputStream stdOut = null;
InputStream stdErr = null;
String outStr = "";
String outErr = "";
try {
if (connection()) {
Session session = conn.openSession();
session.execCommand(cmds);
stdOut = new StreamGobbler(session.getStdout());
outStr = processStream(stdOut, charset);
stdErr = new StreamGobbler(session.getStderr());
outErr = processStream(stdErr, charset);
map.put(Constants.STATUS, Constants.PROCESSING);
System.out.println("outStr=" + outStr);
System.out.println("outErr=" + outErr);
ret = session.getExitStatus();
} else {
logger.error("Login remote machine failed" + ip);
throw new Exception("Login remote machine failed" + ip);
}
} catch(Exception e){
logger.error("Login remote machine failed" + ip, e);
throw new Exception("Login remote machine failed" + ip, e);
} finally {
if (conn != null) {
conn.close();
}
IOUtils.closeQuietly(stdOut);
IOUtils.closeQuietly(stdErr);
}
return map;
}

public static String processStream(InputStream in, String charset) throws Exception {
byte[] buf = new byte[1024];
StringBuilder sb = new StringBuilder();
while (in.read(buf) != -1) {
sb.append(new String(buf, charset));
}
return sb.toString();
}
}
</code></pre>

      <div class="readall"><a href="/blog/2014/09/27/java-SSH-.html" id="post-readall">阅读全文&nbsp;<i class="fa fa-chevron-right"></i></a></div>
    </div>
  </section>
</div>

<div class="posts">
  <section class="post">
    <header class="post-header">
      <h1 class="post-title"><a href="/blog/2014/09/27/hive-.html">hive 安装</a></h1>
      <p class="post-meta">
        <i class="fa fa-calendar"></i>
        2014年09月27日
        <i class="space"></i>
        <i class="fa fa-tags"></i>
        <a class="post-category" href="/page/category.html#hadoop">hadoop</a>
      </p>
    </header>
    <div class="post-main">
      <p>Hive安装</p>

<ol>
  <li>
    <p>下载Hive</p>
  </li>
  <li>
    <p>把Hive移动到/home/hadoop目录下并解压</p>

    <pre><code> hadoop@ubuntu:~/下载$ mv hive-0.9.0.tar.gz /home/hadoop/
 hadoop@ubuntu:~$ cd /home/hadoop/
 hadoop@ubuntu:~$ tar -zxvf hive-0.9.0.tar.gz 
</code></pre>
  </li>
  <li>
    <p>用root用户给hive-0.9.0授权</p>

    <pre><code> hadoop@ubuntu:~$ su -
 密码：
 root@ubuntu:~# cd /home/hadoop/
 root@ubuntu:/home/hadoop# sudo chown -R hadoop:hadoop hive-0.9.0
</code></pre>
  </li>
</ol>

<p>4.添加hive-0.9.0环境变量</p>

<pre><code>	/etc/profile
	在以上 文件中添加如下内容：

#set java environment
HIVE_HOME=/home/hadoop/hive-0.9.0
HADOOP_HOME=/home/hadoop/hadoop-1.1.1
JAVA_HOME=/home/hadoop/jdk1.7.0
PATH=$JAVA_HOME/bin:$HIVE_HOME/bin:$HADOOP_HOME/bin:$PATH
CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$HIVE_HOME/lib:$JAVA_HOME/lib/tools.jar
export HADOOP_HOME
export JAVA_HOME
export HIVE_HOME
export PATH
export CLASSPATH 
</code></pre>

<ol>
  <li>配置 Hive 配置文件</li>
</ol>

<p>a.配置 hive-conf.sh</p>

<p>在“/home/hadoop/hive-0.9.0/bin”目录下,“hive-conf.sh”,然后在里面添加下面内容。</p>

<pre><code>#set java environment
HIVE_HOME=/home/hadoop/hive-0.9.0
HADOOP_HOME=/home/hadoop/hadoop-1.1.1
JAVA_HOME=/home/hadoop/jdk1.7.0
PATH=$JAVA_HOME/bin:$HIVE_HOME/bin:$HADOOP_HOME/bin:$PATH
CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$HIVE_HOME/lib:$JAVA_HOME/lib/tools.jar
export HADOOP_HOME
export JAVA_HOME
export HIVE_HOME
export PATH
export CLASSPATH 
</code></pre>

<p>b.配置 hive-default.xml 和 hive-site.xml</p>

<p>在“/home/hadoop/hive-0.9.0/conf”目录下,没有这两个文件,只有一个“hive-default.xml.template”,所以我们要复制两个“hive-default.xml.template”,并分别命名为“hive-default.xml”和“hive-site.xml” 因为我们当前是 root 用户,。所以还要把两个的文件的授权给 hadoop 用户。</p>

<pre><code>root@ubuntu:/home/hadoop/hive-0.9.0/conf# cp hive-default.xml.template hive-default.xml
root@ubuntu:/home/hadoop/hive-0.9.0/conf# chown -R hadoop:hadoop hive-default.xml
root@ubuntu:/home/hadoop/hive-0.9.0/conf# cp hive-default.xml.template hive-site.xml
root@ubuntu:/home/hadoop/hive-0.9.0/conf# chown -R hadoop:hadoop hive-site.xml
root@ubuntu:/home/hadoop/hive-0.9.0/conf# ls -l
</code></pre>

<p>备注: “hive-default.xml”用于保留默认配置,“hive-site.xml”用于个性化配置,可覆盖默认配置。</p>

<ol>
  <li>启动 Hive</li>
</ol>

<p>此时切换用户至 hadoop 用户,在命令行输入“hive”命令进行测试。</p>

<pre><code>hadoop@ubuntu:~$ hive
WARNING: org.apache.hadoop.metrics.jvm.EventCounter is deprecated. Please use org.apache.hadoop.log.metrics.EventCounter in all the log4j.properties files.
Logging initialized using configuration in jar:file:/home/hadoop/hive-0.9.0/lib/hive-common-0.9.0.jar!/hive-log4j.properties
Hive history file=/tmp/hadoop/hive_job_log_hadoop_201303041031_876597921.txt
hive&gt;
</code></pre>


      <div class="readall"><a href="/blog/2014/09/27/hive-.html" id="post-readall">阅读全文&nbsp;<i class="fa fa-chevron-right"></i></a></div>
    </div>
  </section>
</div>

<div class="posts">
  <section class="post">
    <header class="post-header">
      <h1 class="post-title"><a href="/blog/2014/09/27/hbaseenable.html">hbase表无法enable</a></h1>
      <p class="post-meta">
        <i class="fa fa-calendar"></i>
        2014年09月27日
        <i class="space"></i>
        <i class="fa fa-tags"></i>
        <a class="post-category" href="/page/category.html#hadoop">hadoop</a>
      </p>
    </header>
    <div class="post-main">
      <p>在enable表时，发现耗时很长都未结束，就ctrl+c退出hbase shell，再进入继续enable表，但此时出现如下错误：</p>

<pre><code>ERROR: org.apache.hadoop.hbase.TableNotDisabledException:
</code></pre>

<p>错误说表未禁用，但进行disable表操作时，又出现如下错误：
ERROR: org.apache.hadoop.hbase.TableNotEnabledException
那肯定是刚才强行退出导致出问题了，根据大家的经验，指出这是因为zookeeper保持了要enable的这个表信息，只需要登录zk删除该记录就行。
登录到zookeeper也有两种方式，一是到zk目录运行脚本连接，而是直接在hbase安装目录bin下运行：</p>

<pre><code>hbase zkcli 之后就能连接上zookeeper了，跟着删除/hbase/table下对应的表项就行：

delete /hbase/table/user_video_recommend2 之后就可以成功的enable表了。 如果此时仍然失败，则可以运行一下命了修复META：

hbase hbck -fixMeta -fixAssignments
</code></pre>

      <div class="readall"><a href="/blog/2014/09/27/hbaseenable.html" id="post-readall">阅读全文&nbsp;<i class="fa fa-chevron-right"></i></a></div>
    </div>
  </section>
</div>

<div class="posts">
  <section class="post">
    <header class="post-header">
      <h1 class="post-title"><a href="/blog/2014/09/27/hbase-time.html">hbase time集群时间不一致无法启动</a></h1>
      <p class="post-meta">
        <i class="fa fa-calendar"></i>
        2014年09月27日
        <i class="space"></i>
        <i class="fa fa-tags"></i>
        <a class="post-category" href="/page/category.html#hadoop">hadoop</a>
      </p>
    </header>
    <div class="post-main">
      <p>hbase 分布式时可能因为各个机器的时间不一致导致无法启动
两种解决方案：</p>

<ol>
  <li>
    <p>联网更新时间：</p>

    <pre><code> sudo tzconfig， 如果命令不存在请使用 dpkg-reconfigure tzdata 然后按照提示选择 Asia对应的序号，选完后会显示一堆新的提示—输入城市名， 如Shanghai或Chongqing，最后再用 sudo date -s “” 来修改本地时间。 按照提示进行选择时区，然后： www.2cto.com sudo cp /usr/share/zoneinfo/Asia/ShangHai /etc/localtime 上面的命令是防止系统重启后时区改变。
</code></pre>
  </li>
</ol>

<p>网上同步时间</p>

<ol>
  <li>
    <p>安装ntpdate工具</p>

    <pre><code> # sudo apt-get install ntpdate
</code></pre>
  </li>
  <li>
    <p>设置系统时间与网络时间同步</p>

    <pre><code> # ntpdate cn.pool.ntp.org
</code></pre>
  </li>
  <li>
    <p>将系统时间写入硬件时间</p>

    <p># hwclock –systohc</p>
  </li>
  <li>
    <p>修改hbase的时间检查， 其默认的时间为30000ms</p>

    <pre><code> hbase.master.maxclockskew
 180000
</code></pre>
  </li>
</ol>

      <div class="readall"><a href="/blog/2014/09/27/hbase-time.html" id="post-readall">阅读全文&nbsp;<i class="fa fa-chevron-right"></i></a></div>
    </div>
  </section>
</div>

<div class="posts">
  <section class="post">
    <header class="post-header">
      <h1 class="post-title"><a href="/blog/2014/09/27/hbase-import-export.html">hbase import export</a></h1>
      <p class="post-meta">
        <i class="fa fa-calendar"></i>
        2014年09月27日
        <i class="space"></i>
        <i class="fa fa-tags"></i>
        <a class="post-category" href="/page/category.html#hadoop">hadoop</a>
      </p>
    </header>
    <div class="post-main">
      <ol>
  <li>
    <p>导出HFlie
使用org.apache.hadoop.hbase.mapreduce.Export类
参数: MR配置参数 需导出的表 导出路径 时间戳版本（默认为1）时间戳起始结束时间 可加正则的RowFilter或PrefixFilter</p>

    <pre><code> Export [-D &lt;property=value&gt;]* &lt;tablename&gt; &lt;outputdir&gt; [&lt;versions&gt; [&lt;starttime&gt; [&lt;endtime&gt;]] [^[regex pattern] or [Prefix] to filter]]
 Note: -D properties will be applied to the conf used.
 For example:
 -D mapred.output.compress=true
 -D mapred.output.compression.codec=org.apache.hadoop.io.compress.GzipCodec
 -D mapred.output.compression.type=BLOCK
 Additionally, the following SCAN properties can be specified
 to control/limit what is exported..
 -D hbase.mapreduce.scan.column.family=&lt;familyName&gt;
 -D hbase.mapreduce.include.deleted.rows=true
 For performance consider the following properties:
 -Dhbase.client.scanner.caching=100
 -Dmapred.map.tasks.speculative.execution=false
 -Dmapred.reduce.tasks.speculative.execution=false
 可直接运行也可以利用Driver如：
 hbase org.apache.hadoop.hbase.mapreduce.Driver export &lt;hbase 表&gt; &lt;导出路径&gt;
 hbase org.apache.hadoop.hbase.mapreduce.Driver export my_table /tmp/my_file
</code></pre>
  </li>
  <li>
    <p>导入HFlie</p>

    <pre><code> 使用org.apache.hadoop.hbase.mapreduce.Import类
 参数: Import [options] &lt;tablename&gt; &lt;inputdir&gt;
 By default Import will load data directly into HBase. To instead generate
 HFiles of data to prepare for a bulk data load, pass the option:
 -Dimport.bulk.output=/path/for/output
 For performance consider the following options:
 -Dmapred.map.tasks.speculative.execution=false
 -Dmapred.reduce.tasks.speculative.execution=false
 可直接运行也可以用Driver如：
 hbase org.apache.hadoop.hbase.mapreduce.Driver import &lt;hbase 表&gt; &lt;导入路径&gt;
 hbase org.apache.hadoop.hbase.mapreduce.Driver import my_table /tmp/my_file
</code></pre>
  </li>
  <li>
    <p>批量导入HFlie（导入到已存在的表）</p>

    <pre><code> org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles
 参数: completebulkload &lt;/path/to/hfileoutputformat-output&gt; &lt;tablename&gt;
 可直接运行也可以用Driver如：
 hbase org.apache.hadoop.hbase.mapreduce.Driver completebulkload &lt;导入路径&gt; &lt;hbase 表&gt;
 hbase org.apache.hadoop.hbase.mapreduce.Driver completebulkload /tmp/my_file my_table
</code></pre>

    <p>后面几个工具MR没有测试：</p>
  </li>
  <li>
    <p>导入TSV文件</p>

    <pre><code> org.apache.hadoop.hbase.mapreduce.ImportTsv类
 参数: importtsv -Dimporttsv.columns=a,b,c &lt;tablename&gt; &lt;inputdir&gt;
 必须用-Dimporttsv.columns指定导入的TSV文件的列，列与列之间用逗号隔开
 一个列对应hbase的columnfamily，

 By default importtsv will load data directly into HBase. To instead generate
 HFiles of data to prepare for a bulk data load, pass the option:
 -Dimporttsv.bulk.output=/path/for/output
 Note: if you do not use this option, then the target table must already exist in HBase
 Other options that may be specified with -D include:
 -Dimporttsv.skip.bad.lines=false - fail if encountering an invalid line
 '-Dimporttsv.separator=|' - eg separate on pipes instead of tabs
 -Dimporttsv.timestamp=currentTimeAsLong - use the specified timestamp for the import
 -Dimporttsv.mapper.class=my.Mapper - A user-defined Mapper to use instead of org.apache.hadoop.hbase.mapreduce.TsvImporterMapper
 For performance consider the following options:
 -Dmapred.map.tasks.speculative.execution=false
 -Dmapred.reduce.tasks.speculative.execution=false
 可直接运行也可以用Driver如：
 hbase org.apache.hadoop.hbase.mapreduce.Driver importtsv Dimporttsv.columns=name,age &lt;hbase 表&gt; &lt;导入路径&gt;
 hbase org.apache.hadoop.hbase.mapreduce.Driver importtsv Dimporttsv.columns=name,age my_table /tmp/my_tsvFile
</code></pre>
  </li>
  <li>
    <p>表复制</p>

    <p>org.apache.hadoop.hbase.mapreduce.CopyTable类</p>

    <p>Usage: CopyTable [general options] [–starttime=X] [–endtime=Y] [–new.name=NEW] [–peer.adr=ADR] <tablename>
 Options:
 rs.class hbase.regionserver.class of the peer cluster
 specify if different from current cluster
 rs.impl hbase.regionserver.impl of the peer cluster
 starttime beginning of the time range (unixtime in millis)
 without endtime means from starttime to forever
 endtime end of the time range. Ignored if no starttime specified.
 versions number of cell versions to copy
 new.name new table's name
 peer.adr Address of the peer cluster given in the format
 hbase.zookeeer.quorum:hbase.zookeeper.client.port:zookeeper.znode.parent
 families comma-separated list of families to copy
 To copy from cf1 to cf2, give sourceCfName:destCfName.
 To keep the same name, just give "cfName"
 all.cells also copy delete markers and deleted cells
 Args:
 tablename Name of the table to copy
 Examples:
 To copy 'TestTable' to a cluster that uses replication for a 1 hour window:
 $ bin/hbase org.apache.hadoop.hbase.mapreduce.CopyTable
 --starttime=1265875194289
 --endtime=1265878794289
 --peer.adr=server1,server2,server3:2181:/hbase
 --families=myOldCf:myNewCf,cf2,cf3 TestTable
 For performance consider the following general options:
 -Dhbase.client.scanner.caching=100
 -Dmapred.map.tasks.speculative.execution=false</tablename></p>
  </li>
  <li>
    <p>Compares the data</p>

    <p>org.apache.hadoop.hbase.mapreduce.replication.VerifyReplication类
 Usage: verifyrep [–starttime=X] [–stoptime=Y] [–families=A] <peerid> <tablename>
 Options:
 starttime beginning of the time range
 without endtime means from starttime to forever
 stoptime end of the time range
 families comma-separated list of families to copy
 Args:
 peerid Id of the peer used for verification, must match the one given for replication
 tablename Name of the table to verify
 Examples:
 To verify the data replicated from TestTable for a 1 hour window with peer #5
 $ bin/hbase org.apache.hadoop.hbase.mapreduce.replication.VerifyReplication --starttime=1265875194289 --stoptime=1265878794289 5 TestTable</tablename></peerid></p>
  </li>
</ol>

<p>实例记录：</p>

<ol>
  <li>Export Production HBase table as following:
hbase org.apache.hadoop.hbase.mapreduce.Export ‘mytable’ /hbase_bak/mytable</li>
  <li>
    <p>Copy HDFS files to local:
hadoop fs -copyToLocal /hbase_bak/mytable /home/user/hbase_bak/mytable</p>
  </li>
  <li>
    <p>Copy the local files from production to production extension server</p>
  </li>
  <li>Copy local file to HDFS:
hadoop fs -copyFromLocal /home/user/hbase_bak/mytable /hbase_bak/mytable</li>
  <li>Create table in production extension
create ‘mytable’, {NAME=&gt;’log’, VERSION=&gt; 1}</li>
  <li>Import the HDFS files into HBase:
hbase org.apache.hadoop.hbase.mapreduce.Import ‘mytable’ /hbase_bak/mytable</li>
  <li>Verify the result:
hbase shell
count mytable</li>
</ol>

<p>Find both the row count are same in production and production extension.</p>


      <div class="readall"><a href="/blog/2014/09/27/hbase-import-export.html" id="post-readall">阅读全文&nbsp;<i class="fa fa-chevron-right"></i></a></div>
    </div>
  </section>
</div>

<div class="pagination">
  
  
  <a class="pagination-item newer" href="/page/6"><i class="fa fa-arrow-left"></i>&nbsp;&nbsp;上一页</a>
  
    
  
  <a class="pagination-item older" href="/page/8">下一页&nbsp;&nbsp;<i class="fa fa-arrow-right"></i></a>
  
</div>
    <footer>Copyright&nbsp;&copy;&nbsp;2015 <a href="index.html">willgo</a><br/><i class="fa fa-cogs" style="color:blueviolet;">&nbsp;</i>Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a>
</br> <a href="http://m.kuaidi100.com" target="_blank">快递查询</a> 
</footer>

    </div>
  </div>    
  <div id="top"><a id="rocket" href="javascript:;" title="返回顶部"><i></i></a></div>
  
</body>
</html>
