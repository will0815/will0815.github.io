<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>RSS - willgo</title>
    <description>willgo - 最好的从未错过</description>
    <link>index.html</link>
    <atom:link href="index.html/page/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Sat, 16 May 2015 12:21:04 +0800</pubDate>
    <lastBuildDate>Sat, 16 May 2015 12:21:04 +0800</lastBuildDate>
    <generator>Will.Quan</generator>
    
      <item>
        <title>HBase config optimize</title>
        <description>
</description>
        <pubDate>Sat, 16 May 2015 00:00:00 +0800</pubDate>
        <link>index.html/blog/2015/05/16/HBase-config-optimize.html</link>
        <guid isPermaLink="true">index.html/blog/2015/05/16/HBase-config-optimize.html</guid>
        
        <category>hadoop</category>
        
      </item>
    
      <item>
        <title>DB演进理解</title>
        <description>&lt;h2 id=&quot;db&quot;&gt;DB演进理解&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;RDBMS&lt;br /&gt;
这是最熟悉的数据存储方式，一般情况下数据存储在一台机器上，这种形式方便提供完善ACID特性和丰富查询模型。这就是传统的关系型数据库的基础模式，但是面对现如今越来越大的数据量，这种模式的扩展变得难以满足。实践证明通过增加节点这种简单廉价的扩展方式是可行的。RDBMS的横向扩展主要有主从复制（Master-slave）、分表、分区等。 &lt;br /&gt;
横向扩展RDBMS – Master/Slave &lt;br /&gt;
利用数据库的复制或镜像功能，同时在多台数据库上保存相同的数据，从而将读操作和写操作分开，写操作集中在一台主数据库上，读操作集中在多台从数据库上。&lt;br /&gt;
问题：&lt;br /&gt;
读写同步需要一定的时间，导致关键的读取有出错风险；&lt;br /&gt;
主节点将数据复制到从节点，数据量很大是可能造成问题，  &lt;/p&gt;

    &lt;p&gt;横向扩展RDBMS - Sharding &lt;br /&gt;
 在满足ACID特性的数据库中进行扩展是非常难的。基于这个原因，对数据进行扩展，这个数据库本身就必须拥有简单的模型，将数据分割为N片，然后在单独的片中执行查询。数据分割的单元被称为“shard”。将N片数据分配个M个DBMS进行操作。DBMS并不会去管理数据片，程序开发负责数据片的处理。
 不同的分片方法有：&lt;br /&gt;
 - 垂直分区（Vertical Partitioning）：将不需要进行联合查询的数据表分散到不同的数据库服务器上。&lt;br /&gt;
 - 水平分区(sharding) 将同一个表的记录拆分到不同的表甚至是服务器上，这往往需要一个稳定的算法来保证读取时能正确从不同的服务器上取得数据。如Range-Based Partitioning, Key or Hash-Based partitioning等。&lt;br /&gt;
 优点与不足&lt;br /&gt;
 - 对读取和写入都有很好的扩展&lt;br /&gt;
 - 不透明，程序需要识别分区 &lt;br /&gt;
 - 不再有跨分区的关系/joins &lt;br /&gt;
 - 参照完整性损失  &lt;/p&gt;

    &lt;p&gt;其他RDBMS扩展方法 &lt;br /&gt;
 Multi-Master replication：所有成员都响应客户端数据查询。多主复制系统负责将任意成员做出的数据更新传播给组内其他成员，并解决不同成员间并发修改可能带来的冲突。&lt;br /&gt;
 INSERT only, not UPDATES/DELETES：数据进行版本化处理。&lt;br /&gt;
 No JOINs, thereby reducing query time：Join的开销很大,而且频繁访问会使开销随着时间逐渐增加。&lt;br /&gt;
 非规范化（Denormalization）可以降低数据仓库的复杂性，以提高效率和改善性能。&lt;br /&gt;
 In-memory databases：磁盘数据库解决的是大容量存储和数据分析问题，内存数据库解决的是实时处理和高并发问题。主流常规的RDBMS更多的是磁盘密集型，而不是内存密集型。  &lt;/p&gt;

    &lt;p&gt;ACID Transactions&lt;br /&gt;
 一个完善的数据库系统都是希望支持“ACID transactions,”其包括:&lt;br /&gt;
 Atomic : Either the whole process is done or none is.（原子性）&lt;br /&gt;
 Consistent : Database constraints are preserved.（一致性）&lt;br /&gt;
 Isolated : It appears to the user as if only one process executes at a time.（隔离性）&lt;br /&gt;
 Durable : Effects of a process do not get lost if the system crashes.（持久性）  &lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;NoSQL  &lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;NoSQL现在被理解为 Not Only SQL 的缩写，是对非关系型的数据库管理系统的统称
NoSQL 与 RDBMS 不同点，&lt;br /&gt;
-不使用SQL作为查询语言。&lt;br /&gt;
-不需要固定的表模式(table schema)。&lt;br /&gt;
- 放宽一个或多个 ACID 属性（CAP定理）  &lt;/p&gt;

&lt;p&gt;补充：CAP理论&lt;br /&gt;
CAP理论是数据系统设计的基本理论，目前几乎所有的数据系统的设计都遵循了这个理论。CAP理论指出，分布式系统只能满足以下三项中的两项而不可能满足全部三项，&lt;br /&gt;
一致性（Consistency)（所有节点在同一时间具有相同的数据）&lt;br /&gt;
可用性（Availability）（保证每个请求不管成功或者失败都有响应）&lt;br /&gt;
分区容忍性（Partition tolerance）（系统中任意信息的丢失或失败不会影响系统的继续运作） &lt;/p&gt;

&lt;p&gt;一致性有两种类型：&lt;br /&gt;
- strong consistency – ACID(Atomicity Consistency Isolation Durability)：对于关系型数据库，要求更新过的数据能被后续所有的访问都看到，这是强一致性。&lt;br /&gt;
- weak consistency – BASE(Basically Available Soft-state Eventual consistency )
– Basically Available - system seems to work all the time (基本可用)&lt;br /&gt;
– Soft State - it doesn’t have to be consistent all the time （不要求所有时间都一致）&lt;br /&gt;
– Eventually Consistent - becomes consistent at some later time （最终一致性）  &lt;/p&gt;

&lt;p&gt;对于分布式数据系统(scale out)，分区容忍性是基本要求，否则就失去了价值。因此只能在一致性和可用性上做取舍，如何处理这种取舍正是目前NoSQL数据库的核心焦点。牺牲一致性而换取高可用性。当然，牺牲一致性，只是不再要求关系数据库中的强一致性，而是只要系统能达到最终一致性即可。通常是通过数据的多份异步复制来实现系统的高可用和数据的最终一致性的。  &lt;/p&gt;

&lt;p&gt;NoSQL 的两种主要实现方式&lt;br /&gt;
1.Key/Value&lt;br /&gt;
Amazon S3 (Dynamo)&lt;br /&gt;
Voldemort &lt;br /&gt;
Scalaris &lt;br /&gt;
Memcached (in-memory key/value store)&lt;br /&gt;
Redis &lt;br /&gt;
2. 弱模式型（column-based, document-based or graph-based.）&lt;br /&gt;
Cassandra (column-based)&lt;br /&gt;
CouchDB (document-based)&lt;br /&gt;
MongoDB(document-based) &lt;br /&gt;
Neo4J (graph-based)&lt;br /&gt;
HBase (column-based)   &lt;/p&gt;

&lt;p&gt;K/V模式&lt;br /&gt;
优点:&lt;br /&gt;
very fast&lt;br /&gt;
very scalable&lt;br /&gt;
simple model&lt;br /&gt;
able to distribute horizontally&lt;br /&gt;
劣势: &lt;br /&gt;
 many data structures (objects) can’t be easily modeled as key value pairs （需要多余转换）&lt;br /&gt;
Schema-Less 模式&lt;br /&gt;
优点&lt;br /&gt;
Schema-less data model is richer than key/value pairs
eventual consistency&lt;br /&gt;
many are distributed&lt;br /&gt;
still provide excellent performance and scalability&lt;br /&gt;
劣势&lt;br /&gt;
typically no ACID transactions or joins  &lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;HBase  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;HBase is an open-source, distributed, column-oriented database built on top of HDFS (or KFS) based on BigTable! &lt;br /&gt;
按照CAP理论，HBase属于C+P类型的系统。HBase是强一致性的（仅支持单行事务）。每一行由单个区域服务器（region server）host，行锁（row locks）和多版本并发控制(multiversion concurrency control)的组合被用来保证行的一致性。&lt;br /&gt;
查找:&lt;br /&gt;
快速定位使用row key + column family + column + timestamp.&lt;br /&gt;
按范围查找：start row key – end row key.&lt;br /&gt;
全表扫描&lt;br /&gt;
交互方式&lt;br /&gt;
- Java, REST, or Thrift APIs.&lt;br /&gt;
- Scripting via JRuby.  &lt;/p&gt;

&lt;p&gt;HBase 一些特性  &lt;/p&gt;

&lt;pre&gt;&lt;code&gt;- No real indexes（row key 即数据的索引）  
- Automatic partitioning（region自动split）  
- Scale linearly and automatically with new nodes（扩展容易 节点添加方便）  
- Commodity hardware（廉价硬件）  
- Fault tolerance(较强的容错性)  
- Batch processing(批处理能力优秀)  
&lt;/code&gt;&lt;/pre&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Hive    &lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;Provide higher-level language (HQL, like SQL) to facilitate large-data processing&lt;/li&gt;
      &lt;li&gt;Higher-level language “compiles down” to Hadoop Map/Reduce jobs&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Hive + HBase  &lt;/p&gt;

    &lt;p&gt;Reasons to use Hive on HBase:&lt;br /&gt;
 A lot of data sitting in HBase due to its usage in a real-time environment, but never used for analysis&lt;br /&gt;
 Give access to data in HBase usually only queried through MapReduce to people that don’t code (business analysts)&lt;br /&gt;
 When needing a more flexible storage solution, so that rows can be updated live by either a Hive job or an application and can be seen immediately to the other&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

</description>
        <pubDate>Sat, 16 May 2015 00:00:00 +0800</pubDate>
        <link>index.html/blog/2015/05/16/DB-yanjin.html</link>
        <guid isPermaLink="true">index.html/blog/2015/05/16/DB-yanjin.html</guid>
        
        <category>db</category>
        
      </item>
    
      <item>
        <title>CDH 安装问题</title>
        <description>&lt;p&gt;&lt;em&gt;CDH5安装参照：http://www.tuicool.com/articles/ENjmeaY&lt;/em&gt;&lt;br /&gt;
Q1:安装过程中提示找不到MySQL 驱动jar包&lt;br /&gt;
	：在/usr/shar/java 中添加mysql-connector-java-5.1.30-bin.jar&lt;br /&gt;
Q2：安装hive提示找不到MySQL驱动jar包&lt;br /&gt;
：在/opt/cloudera/parcels/CDH-5.1.3-1.cdh5.1.3.p0.12/lib/hive/lib/ 中添加 mysql-connector-java-5.1.30-bin.jar&lt;br /&gt;
Q3: cloudera-server 或cloudera-agent  status 提示服务已经停止，pid仍然存在&lt;br /&gt;
	：查看对应的log 可能是端口占用或是数据库连接问题&lt;br /&gt;
Q4:MR2和HDFS安装好后 web ui无法访问，查看对应端口已经处于listen状态，&lt;br /&gt;
	：分别在对应的配置中的端口和地址选项中将“将 ResourceManager 绑定到通配符地址”项， 勾选上&lt;br /&gt;
没有这个选项的可以在高级设置中在XML文件中添加&lt;br /&gt;
Q5：jdk版本不对应&lt;br /&gt;
	： CDH默认获取的jdk 位于：/usr/java   可将需要的jdk 拷贝到这个目录中。  &lt;/p&gt;

&lt;p&gt;Q6  sqoop导入数据时：User does not belong to hive&lt;br /&gt;
	I had same issue with permissions -&amp;gt; chgrp: changing ownership of ‘/user/hive/warehouse/test/_log24310.txt’: User does not belong to hive.
1.Added the existing user named cloudera to existing group named hive with command: usermod -a -G hive cloudera 
2.Restarted the system 
3.Used Load Command and after that did a select * from table_name -&amp;gt; No data was getting displayed. 
4.Executed select count(*) from table_name and a MapReduce job got started. 
5.Executed select * from table and now results was returned correctly. 
6.Opened a impala shell using impala-shell command. 
7.Executed a select * from table_name and no results was getting returned. 
8.Executed command invalidate metadata in the impala-shell 
9.Executed command refresh table_name 
10.Executed command show tables 
11.Executed command select * from table_name and now results are getting displayed both in the impala-shell and hive shell. 
  可以不必理会&lt;/p&gt;

&lt;p&gt;Q6：spark 安装不上&lt;br /&gt;
	未解决，网上有对应bug&lt;/p&gt;

&lt;p&gt;整个安装过程中遇到各种问题，主要解决得途径还是查看对应的log文件对应解决：&lt;br /&gt;
/var/log/hadoop*&lt;br /&gt;
/var/log/cloudera*&lt;br /&gt;
/opt/cm-5.1.3/log  &lt;/p&gt;

&lt;h6 id=&quot;about&quot;&gt;===========================about==================================================================================================================================&lt;/h6&gt;
&lt;p&gt;问题导读  &lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1.CM的安装目录在什么位置？  
2.hadoop配置文件在什么位置？  
3.Cloudera manager运行所需要的信息存在什么位置？  
4.CM结构和功能是什么？  
1. 相关目录  
/var/log/cloudera-scm-installer : 安装日志目录。  
/var/log/* : 相关日志文件（相关服务的及CM的）。  
/usr/share/cmf/ : 程序安装目录。  
/usr/lib64/cmf/ : Agent程序代码。  
/var/lib/cloudera-scm-server-db/data : 内嵌数据库目录。  
/usr/bin/postgres : 内嵌数据库程序。  
/etc/cloudera-scm-agent/ : agent的配置目录。  
/etc/cloudera-scm-server/ : server的配置目录。  
/opt/cloudera/parcels/ : Hadoop相关服务安装目录。  
/opt/cloudera/parcel-repo/ : 下载的服务软件包数据，数据格式为parcels。  
/opt/cloudera/parcel-cache/ : 下载的服务软件包缓存数据。  
/etc/hadoop/* : 客户端配置文件目录。  
&lt;/code&gt;&lt;/pre&gt;

&lt;ol&gt;
  &lt;li&gt;配置    &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hadoop配置文件&lt;br /&gt;
配置文件放置于/var/run/cloudera-scm-agent/process/目录下。如：/var/run/cloudera-scm-agent/process/193-hdfs-NAMENODE/core-site.xml。这些配置文件是通过Cloudera Manager启动相应服务（如HDFS）时生成的，内容从数据库中获得（即通过界面配置的参数）。&lt;br /&gt;
在CM界面上更改配置是不会立即反映到配置文件中，这些信息会存储于数据库中，等下次重启服务时才会生成配置文件。且每次启动时都会产生新的配置文件。&lt;br /&gt;
CM Server主要数据库为scm基中放置配置的数据表为configs。里面包含了服务的配置信息，每一次配置的更改会把当前页面的所有配置内容添加到数据库中，以此保存配置修改历史。&lt;br /&gt;
scm数据库被配置成只能从localhost访问，如果需要从外部连接此数据库，修改vim /var/lib/cloudera-scm-server-db/data/pg_hba.conf文件,之后重启数据库。运行数据库的用户为cloudera-scm。  &lt;/p&gt;

&lt;p&gt;查看配置内容&lt;br /&gt;
1.直接查询scm数据库的configs数据表的内容。  &lt;br /&gt;
2.访问REST API： http://hostname:7180/api/v4/cm/deployment，返回JSON格式部署配置信息。  &lt;/p&gt;

&lt;p&gt;配置生成方式&lt;br /&gt;
CM为每个服务进程生成独立的配置目录（文件）。所有配置统一在服务端查询数据库生成（因为scm数据库只能在localhost下访问）生成配置文件，再由agent通过网络下载包含配置文件的zip包到本地解压到指定的目录。  &lt;/p&gt;

&lt;p&gt;配置修改&lt;br /&gt;
CM对于需要修改的配置预先定义，对于没有预先定义的配置,则通过在高级配置项中使用xml配置片段的方式进行配置。而对于/etc/hadoop/下的配置文件是客户端的配置，可以在CM通过部署客户端生成客户端配置。  &lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;数据库&lt;br /&gt;
Cloudera manager主要的数据库为scm,存储Cloudera manager运行所需要的信息：配置，主机，用户等。  &lt;/li&gt;
  &lt;li&gt;CM结构 &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;CM分为Server与Agent两部分及数据库（自带更改过的嵌入Postgresql）。它主要做三件事件：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1.管理监控集群主机。
2.统一管理配置。
3.管理维护Hadoop平台系统。
实现采用C/S结构，Agent为客户端负责执行服务端发来的命令，执行方式一般为使用python调用相应的服务shell脚本。Server端为Java REST服务，提供REST API，Web管理端通过REST API调用Server端功能，Web界面使用富客户端技术（Knockout）。
1.Server端主体使用Java实现。
2.Agent端主体使用Python, 服务的启动通过调用相应的shell脚本进行启动，如果启动失败会重复4次调用启动脚本。
3.Agent与Server保持心跳，使用Thrift RPC框架。
&lt;/code&gt;&lt;/pre&gt;

&lt;ol&gt;
  &lt;li&gt;升级  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;在CM中可以通过界面向导升级相关服务。升级过程为三步：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1.下载服务软件包。
2.把所下载的服务软件包分发到集群中受管的机器上。
3.安装服务软件包，使用软链接的方式把服务程序目录链接到新安装的软件包目录上。
&lt;/code&gt;&lt;/pre&gt;

&lt;ol&gt;
  &lt;li&gt;卸载  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;sudo /usr/share/cmf/uninstall-scm-express.sh, 然后删除/var/lib/cloudera-scm-server-db/目录，不然下次安装可能不成功。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;开启postgresql远程访问  &lt;/p&gt;

    &lt;p&gt;CM内嵌数据库被配置成只能从localhost访问，如果需要从外部查看数据，数据修改vim /var/lib/cloudera-scm-server-db/data/pg_hba.conf文件,之后重启数据库。运行数据库的用户为cloudera-scm。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Sat, 16 May 2015 00:00:00 +0800</pubDate>
        <link>index.html/blog/2015/05/16/CDH-install-question.html</link>
        <guid isPermaLink="true">index.html/blog/2015/05/16/CDH-install-question.html</guid>
        
        <category>hadoop</category>
        
      </item>
    
      <item>
        <title>Linux学习笔记9-AWK</title>
        <description>&lt;h2 id=&quot;linux9-awk&quot;&gt;Linux学习笔记9-AWK&lt;/h2&gt;
&lt;p&gt;AWK是所有shell过滤工具中最难掌握的，不知道为什么，也许是其复杂的语法或含义不明确的错误提示信息。在学习awk语言过程中，就会慢慢掌握诸如Bailing out 和awk:cmd.Line:等错误信息。可以说awk是一种自解释的编程语言，之所以要在shell中使用awk是因为awk本身是学习的好例子，但结合awk与其他工具诸如grep和sed，将会使shell编程更加
容易。&lt;br /&gt;
调用：&lt;br /&gt;
有三种方式调用awk，第一种是命令行方式，如：&lt;br /&gt;
awk [-F field-separator] ‘awk命令’ input-file(s)&lt;br /&gt;
[-F域分隔符]是可选的，因为awk使用空格作为缺省的域分隔符，因此如果
要浏览域间有空格的文本，不必指定这个选项，但如果要浏览诸如passwd文件，此文件各域以冒号作为分隔符，则必须指明- F选项&lt;br /&gt;
awk -F:’awk命令’ input-file&lt;br /&gt;
第二种方法是将所有awk命令插入一个文件，并使awk程序可执行，然后用awk命令解释器作为脚本的首行，以便通过键入脚本名称来调用它&lt;br /&gt;
awk -f awk-script-file input-files&lt;br /&gt;
第三种方式是将所有的awk命令插入一个单独文件. -f选项指明在文件awk_scriptfile中的awk脚本，inputfile(s)是使用awk进行浏览的文件
名。  &lt;/p&gt;

&lt;p&gt;awk脚本&lt;br /&gt;
在命令中调用awk时，awk脚本由各种操作和模式组成。&lt;br /&gt;
如果设置了-F选项，则awk每次读一条记录或一行，并使用指定的分隔符分隔指定域，但如果未设置-F选项，awk假定空格为域分隔符，并保持这个设置直到发现一新行。当新行出现时，awk命令获悉已读完整条记录，然后在下一个记录启动读命令，这个读进程将持续到文件尾或文件不再存在。&lt;/p&gt;

</description>
        <pubDate>Sun, 01 Feb 2015 00:00:00 +0800</pubDate>
        <link>index.html/blog/2015/02/01/Linux-point9.html</link>
        <guid isPermaLink="true">index.html/blog/2015/02/01/Linux-point9.html</guid>
        
        <category>linux</category>
        
      </item>
    
      <item>
        <title>Linux学习笔记8-grep 家族</title>
        <description>&lt;h2 id=&quot;linux8-grep-&quot;&gt;Linux学习笔记8-grep 家族&lt;/h2&gt;
&lt;p&gt;grep（全局正则表达式版本）允许对文本文件进行模式查找。如果找到匹配模式，grep打印包含模式的所有行。grep支持基本正则表达式，也支持其扩展集   &lt;br /&gt;
grep一般格式为：&lt;br /&gt;
grep [选项]基本正则表达式[文件]&lt;br /&gt;
这里基本正则表达式可为字符串。  &lt;/p&gt;

&lt;p&gt;在grep命令中输入字符串参数时，最好将其用双引号括起来。例如：“mystring”。这样做有两个原因，
一是以防被误解为shell命令，&lt;br /&gt;
二是可以用来查找多个单词组成的字符串，例如：  &lt;br /&gt;
“jet plane”，如果不用双引号将其括起来，那么单词plane将被误认为是一个文件，查询结果将返回“文件不存在”的错误信息。&lt;br /&gt;
在调用变量时，也应该使用双引号，诸如： grep “MYVAR”文件名，如果不这样，将没有返回结果。&lt;br /&gt;
在调用模式匹配时，应使用单引号。  &lt;/p&gt;

&lt;p&gt;常用的grep选项有：&lt;br /&gt;
-c 只输出匹配行的计数。&lt;br /&gt;
-i 不区分大小写（只适用于单字符）。&lt;br /&gt;
-h 查询多文件时不显示文件名。&lt;br /&gt;
-l 查询多文件时只输出包含匹配字符的文件名。&lt;br /&gt;
-n 显示匹配行及行号。&lt;br /&gt;
-s 不显示不存在或无匹配文本的错误信息。&lt;br /&gt;
-v 显示不包含匹配文本的所有行。  &lt;/p&gt;

&lt;p&gt;grep “will”*.doc 在当前目录下所有.doc文件中查找”will”字符串。  &lt;/p&gt;

&lt;p&gt;精确匹配：&lt;br /&gt;
grep “will&lt;tab&gt;&quot;*.doc 在当前目录下所有.doc文件精确匹配&quot;will&quot;   还可以写为：grep &quot;will\&amp;gt;&quot;*.doc  
大小写敏感：  
缺省情况下， grep是大小写敏感的，如要查询大小写不敏感字符串，必须使用- i开关。  &lt;/tab&gt;&lt;/p&gt;

</description>
        <pubDate>Sun, 01 Feb 2015 00:00:00 +0800</pubDate>
        <link>index.html/blog/2015/02/01/Linux-point8.html</link>
        <guid isPermaLink="true">index.html/blog/2015/02/01/Linux-point8.html</guid>
        
        <category>linux</category>
        
      </item>
    
      <item>
        <title>Linux学习笔记7-正则表达式介绍</title>
        <description>&lt;h2 id=&quot;linux7-&quot;&gt;Linux学习笔记7-正则表达式介绍&lt;/h2&gt;
&lt;p&gt;当从一个文件或命令输出中抽取或过滤文本时，可以使用正则表达式(RE)，正则表达式是一些特殊或不很特殊的字符串模式的集合。&lt;br /&gt;
为了抽取或获得信息，我们给出抽取操作应遵守的一些规则。这些规则由一些特殊字符或进行模式匹配操作时使用的元字符组成。也可以使用规则字符作为模式中的一部分进行搜寻。&lt;br /&gt;
例如，A将查询A，x将查找字母x  &lt;br /&gt;
基本元字符&lt;br /&gt;
^ 只只匹配行首&lt;br /&gt;
$ 只只匹配行尾&lt;br /&gt;
* 只一个单字符后紧跟*，匹配0个或多个此单字符&lt;br /&gt;
[ ] 只匹配[ ]内字符。可以是一个单字符，也可以是字符序列。可以使用-表示[ ]内字符序列范围，如用[1-5]代替[12345]&lt;br /&gt;
\ 只用来屏蔽一个元字符的特殊含义。因为有时在shell中一些元字符有
特殊含义。\可以使其失去应有意义&lt;br /&gt;
. 只匹配任意单字符&lt;br /&gt;
pattern{n} 只用来匹配前面pattern出现次数。n为次数&lt;br /&gt;
pattern{n,} m 只含义同上，但次数最少为n&lt;br /&gt;
pattern{n,m} 只含义同上，但pattern出现次数在n与m之间  &lt;/p&gt;

&lt;p&gt;1.使用句点匹配单字符&lt;br /&gt;
句点”.”可以匹配任意单字符。例如，如果要匹配一个字符串，以beg开头，中间夹一个任意字符，那么可以表示为beg.n，”.”可以匹配字符串头，也可以是中间任意字符。&lt;br /&gt;
在ls -l命令中，可以匹配一定权限：&lt;br /&gt;
…x..x ..x  : 匹配用户本身，用户组及其他组成员的执行权限。&lt;br /&gt;
“.”允许匹配ASCII集中任意字符，或为字母，或为数字。&lt;br /&gt;
2.在行首以^匹配字符串或字符序列&lt;br /&gt;
^只允许在一行的开始匹配字符或单词。例如，使用ls -l命令，并匹配目录。之所以可以这样做是因为ls -l命令结果每行第一个字符是d，即代表一个目录。  &lt;br /&gt;
^…4XC….  &lt;br /&gt;
以上模式表示，在每行开始，匹配任意3个字符，后跟4XC，最后为任意4个字符。^在正则表达式中使用频繁，因为大量的抽取操作通常在行首.&lt;br /&gt;
3.在行尾以$匹配字符串或字符  &lt;br /&gt;
$与^正相反，它在行尾匹配字符串或字符， $符号放在匹配单词后。假定要匹配以单词test结尾的所有行，操作为：&lt;br /&gt;
test$&lt;br /&gt;
^$&lt;br /&gt;
具体分析为匹配行首，又匹配行尾，中间没有任何模式，因此为空行。&lt;br /&gt;
如果只返回包含一个字符的行，操作如下：&lt;br /&gt;
^.$&lt;br /&gt;
4.使用&lt;em&gt;匹配字符串中的单字符或其重复序列  &lt;br /&gt;
使用此特殊字符匹配任意字符或字符串的重复多次表达式。例如：&lt;br /&gt;
compu&lt;/em&gt;t&lt;br /&gt;
将匹配字符u一次或多次：&lt;br /&gt;
compuut 
compuuut&lt;br /&gt;
compuuuuut&lt;br /&gt;
5.使用\屏蔽一个特殊字符的含义  &lt;br /&gt;
有时需要查找一些字符或字符串，而它们包含了系统指定为特殊字符的一个字符。什么是特殊字符？一般意义上讲，下列字符可以认为是特殊字符：&lt;br /&gt;
$ . ‘ “ * || ^ [] 0 \ + ?&lt;br /&gt;
假定要匹配包含字符“ .”的各行而“，”代表匹配任意单字符的特殊字符，因此需要屏蔽其含义。操作如下：&lt;br /&gt;
.
6.使用[]匹配一个范围或集合  &lt;br /&gt;
使用[ ]匹配特定字符串或字符串集，可以用逗号将括弧内要匹配的不同字符串分开，但并不强制要求这样做（一些系统提倡在复杂的表达式中使用逗号），这样做可以增加模式的可读性。&lt;br /&gt;
使用”-“表示一个字符串范围，表明字符串范围从”-“左边字符开始，到”-“右边字符结束。&lt;br /&gt;
如果熟知一个字符串匹配操作，应经常使用[]模式。&lt;br /&gt;
假定要匹配任意一个数字，可以使用：&lt;br /&gt;
[0123456789]&lt;br /&gt;
然而，通过使用“-”符号可以简化操作：&lt;br /&gt;
[0-9]&lt;br /&gt;
7.使用{}匹配模式结果出现的次数&lt;br /&gt;
使用*可匹配所有匹配结果任意次，但如果只要指定次数，就应使用\ { \ }，此模式有三种形式，即：&lt;br /&gt;
pattern{n} 匹配模式出现n次。&lt;br /&gt;
pattern{n,} 匹配模式出现最少n次。&lt;br /&gt;
pattern{n,m} 匹配模式出现n到m次之间，n , m为0 -255中任意整数。
请看第一个例子，匹配字母A出现两次，并以B结尾，操作如下：&lt;br /&gt;
A{2}B&lt;br /&gt;
匹配值为AAB&lt;br /&gt;
匹配A至少4次，使用：&lt;br /&gt;
A{4,} B&lt;br /&gt;
可以得结果AAAAB或AAAAAAAB，但不能为AAAB。  &lt;/p&gt;
</description>
        <pubDate>Sun, 01 Feb 2015 00:00:00 +0800</pubDate>
        <link>index.html/blog/2015/02/01/Linux-point7.html</link>
        <guid isPermaLink="true">index.html/blog/2015/02/01/Linux-point7.html</guid>
        
        <category>linux</category>
        
      </item>
    
      <item>
        <title>Linux学习笔记6-命令执行顺序</title>
        <description>&lt;h2 id=&quot;linux6-&quot;&gt;Linux学习笔记6-命令执行顺序&lt;/h2&gt;
&lt;p&gt;场景：执行某个命令的时候，有时需要依赖于前一个命令是否执行成功。例如，假设你希望&lt;br /&gt;
将一个目录中的文件全部拷贝到另外一个目录中后，然后删除源目录中的全部文件。在删除之前，你希望能够确信拷贝成功，否则就有可能丢失所有的文件。  &lt;br /&gt;
1.&amp;amp;&amp;amp;&lt;br /&gt;
&amp;amp;&amp;amp;的一般形式：&lt;br /&gt;
命令1 &amp;amp;&amp;amp; 命令2&lt;br /&gt;
以上表示：命令1返回真(返回0，成功执行)后，命令2才能被执行&lt;br /&gt;
如：mv /apps/bin /apps/dev/bin &amp;amp;&amp;amp; rm -r /apps/bin&lt;br /&gt;
2.||  &lt;br /&gt;
||一般形式：&lt;br /&gt;
命令1||命令2&lt;br /&gt;
以上表示：命令1未执行成功，那么就执行命令2。&lt;br /&gt;
3.()和{}&lt;br /&gt;
如果希望把几个命令合在一起执行，shell提供了两种方法。既可以在当前shell也可以在子shell中执行一组命令。&lt;br /&gt;
为了在当前shell中执行一组命令，可以用命令分隔符隔开每一个命令，并把所有的命令&lt;br /&gt;
用圆括号()括起来。&lt;br /&gt;
它的一般形式为：&lt;br /&gt;
(命令1;命令2;. . .)&lt;br /&gt;
如果使用{}来代替()，那么相应的命令将在子shell而不是当前shell中作为一个整体被执行，只有在{}中所有命令的输出作为一个整体被重定向时，其中的命令才被放到子shell中执行，否则在当前shell执行。它的一般形式为：  &lt;br /&gt;
{命令1;命令2;. . . }  &lt;/p&gt;
</description>
        <pubDate>Sun, 01 Feb 2015 00:00:00 +0800</pubDate>
        <link>index.html/blog/2015/02/01/Linux-point6.html</link>
        <guid isPermaLink="true">index.html/blog/2015/02/01/Linux-point6.html</guid>
        
        <category>linux</category>
        
      </item>
    
      <item>
        <title>Linux学习笔记10-sed</title>
        <description>&lt;h2 id=&quot;linux10-sed&quot;&gt;Linux学习笔记10-sed&lt;/h2&gt;
&lt;p&gt;sed是一个非交互性文本流编辑器。它编辑文件或标准输入导出的文本拷贝。标准输入可能是来自键盘、文件重定向、字符串或变量，或者是一个管道的文本。sed可以做些什么呢,与同是编辑器的vi有何不同？  &lt;/p&gt;

&lt;p&gt;可以在命令行输入sed命令，也可以在一个文件中写入命令，然后调用sed，这与awk基本相同。使用sed需要记住的一个重要事实是，无论命令是什么， sed并不与初始化文件打交道，它操作的只是一个拷贝，然后所有的改动如果没有重定向到一个文件，将输出到屏幕。&lt;br /&gt;
因为sed是一个非交互性编辑器，必须通过行号或正则表达式指定要改变的文本行。&lt;br /&gt;
和grep与awk一样，sed是一种重要的文本过滤工具，或者使用一行命令或者使用管道与grep与awk相结合  &lt;/p&gt;

&lt;p&gt;读取数据&lt;br /&gt;
sed从文件的一个文本行或从标准输入的几种格式中读取数据，将之拷贝到一个编辑缓冲区，然后读命令行或脚本的第一条命令，并使用这些命令查找模式或定位行号编辑它。重复此过程直到命令结束。  &lt;/p&gt;

&lt;p&gt;调用sed&lt;br /&gt;
调用sed有三种方式：&lt;br /&gt;
1.在命令行键入命令；&lt;br /&gt;
	sed [选项] sed命令输入文件。&lt;br /&gt;
	在命令行使用sed命令时，实际命令要加单引号。sed也允许加双引号。&lt;br /&gt;
2.将sed命令插入脚本文件，然后调用sed；&lt;br /&gt;
	sed [选项] -f sed脚本文件输入文件&lt;br /&gt;
3.将sed命令插入脚本文件，并使sed脚本可执行。&lt;br /&gt;
	sed脚本文件[选项] 输入文件&lt;br /&gt;
不管是使用shell命令行方式或脚本文件方式，如果没有指定输入文件， sed从标准输入中接受输入，一般是键盘或重定向结果。&lt;br /&gt;
sed选项如下：&lt;br /&gt;
n 不打印；sed不写编辑行到标准输出，缺省为打印所有行（编辑和未编辑）。p命令可以用来打印编辑行。&lt;br /&gt;
c 下一命令是编辑命令。使用多项编辑时加入此选项。如果只用到一条sed命令，此选项无用，但指定它也没有关系。&lt;br /&gt;
f 如果正在调用sed脚本文件，使用此选项。此选项通知sed一个脚本文件支持所有的sed&lt;br /&gt;
命令，例如：sed -f myscript.sed input_file，这里myscript.sed即为支持sed命令的文件。  &lt;/p&gt;

&lt;p&gt;保存sed输出&lt;br /&gt;
由于不接触初始化文件，如果想要保存改动内容，简单地将所有输出重定向到一个文件即可。下面的例子重定向sed命令的所有输出至文件‘myoutfile’，当对结果很满意时使用这种方法。&lt;br /&gt;
sed ‘some-sed-commands’ input-file &amp;gt; myoutfile  &lt;/p&gt;

&lt;p&gt;使用sed在文件中查询文本&lt;br /&gt;
sed浏览输入文件时，缺省从第一行开始，有两种方式定位文本：&lt;br /&gt;
1) 使用行号，可以是一个简单数字，或是一个行号范围。&lt;br /&gt;
2) 使用正则表达式。&lt;br /&gt;
如：&lt;br /&gt;
x                       x为一行号，如1&lt;br /&gt;
x , y                   表示行号范围从x到y，如2，5表示从第2行到第5行&lt;br /&gt;
/pattern/               查询包含模式的行。例如/disk/或/[a-z]/&lt;br /&gt;
/pattern/pattern/       查询包含两个模式的行。例如/disk/  disks/&lt;br /&gt;
/pattern/, x            在给定行号上查询包含模式的行。如/ribbon/,3&lt;br /&gt;
x,/pattern/             通过行号和模式查询匹配行。3./vdu/&lt;br /&gt;
x,y!                    查询不包含指定行号x和y的行。1 , 2 !  &lt;/p&gt;

&lt;p&gt;基本sed编辑命令
p    打印匹配行
=    显示文件行号
a\   在定位行号后附加新文本信息
i\   在定位行号后插入新文本信息
d    删除定位行
c\   用新文本替换定位文本
s    使用替换模式替换相应模式
r    从另一个文件中读文本
w    写文本到一个文件
q    第一个模式匹配完成后推出或立即推出
l    显示与八进制A S C I I代码等价的控制字符
{}   在定位行执行的命令组
n    从另一个文件中读文本下一行，并附加在下一行
g    将模式2粘贴到/pattern n/
y    传送字符
n    延续到下一输入行；允许跨行的模式匹配语句&lt;/p&gt;
</description>
        <pubDate>Sun, 01 Feb 2015 00:00:00 +0800</pubDate>
        <link>index.html/blog/2015/02/01/Linux-point10.html</link>
        <guid isPermaLink="true">index.html/blog/2015/02/01/Linux-point10.html</guid>
        
        <category>linux</category>
        
      </item>
    
      <item>
        <title>Hive基础架构</title>
        <description>&lt;p&gt;Hive基础架构&lt;br /&gt;
Hive是基于Hadoop的一个数据仓库工具，可以将结构化的数据文件映射为一张数据库表，并提供完整的sql查询功能，可以将sql语句转换为MapReduce任务进行运行。其优点是学习成本低，可以通过类SQL语句快速实现简单的MapReduce统计，不必开发专门的MapReduce应用，十分适合数据仓库的统计分析,Hive是建立在 Hadoop 上的数据仓库基础构架。它提供了一系列的工具，可以用来进行数据提取转化加载（ETL），这是一种可以存储、查询和分析存储在 Hadoop 中的大规模数据的机制。Hive 定义了简单的类 SQL 查询语言，称为 HQL，它允许熟悉 SQL 的用户查询数据。同时，这个语言也允许熟悉 MapReduce 开发者的开发自定义的 mapper 和 reducer 来处理内建的 mapper 和 reducer 无法完成的复杂的分析工作。 &lt;br /&gt;
Hive技术架构图：&lt;br /&gt;
&lt;img src=&quot;/image/hive1.png&quot; alt=&quot;&quot; /&gt;&lt;br /&gt;
服务端组件： &lt;br /&gt;
Driver组件：该组件包括Complier、Optimizer和Executor，它的作用是将HiveQL（类SQL）语句进行解析、编译优化，生成执行计划，然后调用底层的mapreduce计算框架。 &lt;br /&gt;
Metastore组件：元数据服务组件，这个组件存储hive的元数据，hive的元数据存储在关系数据库里，hive支持的关系数据库有derby、mysql。元数据对于hive十分重要，因此hive支持把metastore服务独立出来，安装到远程的服务器集群里，从而解耦hive服务和metastore服务，保证hive运行的健壮性。&lt;br /&gt;
Thrift服务：thrift是facebook开发的一个软件框架，它用来进行可扩展且跨语言的服务的开发，hive集成了该服务，能让不同的编程语言调用hive的接口。 &lt;br /&gt;
客户端组件： &lt;br /&gt;
CLI：command line interface，命令行接口。 &lt;br /&gt;
Thrift客户端：上面的架构图里没有写上Thrift客户端，但是hive架构的许多客户端接口是建立在thrift客户端之上，包括JDBC和ODBC接口。 &lt;br /&gt;
WEBGUI：hive客户端提供了一种通过网页的方式访问hive所提供的服务。这个接口对应hive的hwi组件（hive web interface），使用前要启动hwi服务。 &lt;br /&gt;
一个Hive hsql 执行流程&lt;br /&gt;
&lt;img src=&quot;/image/hive2.png&quot; alt=&quot;&quot; /&gt;  &lt;/p&gt;
</description>
        <pubDate>Sun, 01 Feb 2015 00:00:00 +0800</pubDate>
        <link>index.html/blog/2015/02/01/Hive-structure.html</link>
        <guid isPermaLink="true">index.html/blog/2015/02/01/Hive-structure.html</guid>
        
        <category>hadoop</category>
        
      </item>
    
      <item>
        <title>Hive分区分桶</title>
        <description>&lt;h2 id=&quot;hive&quot;&gt;Hive分区分桶&lt;/h2&gt;
&lt;p&gt;Hive 分区表&lt;br /&gt;
在Hive Select查询中一般会扫描整个表内容，会消耗很多时间做没必要的工作。有时候只需要扫描表中关心的一部分数据，因此建表时引入了partition概念。分区表指的是在创建表时指定的partition的分区空间。 &lt;br /&gt;
Hive可以对数据按照某列或者某些列进行分区管理，所谓分区我们可以拿下面的例子进行解释。 存储日志，其中必然有个属性是日志产生的日期。在产生分区时，就可以按照日志产生的日期列进行划分。把每一天的日志当作一个分区。   
将数据组织成分区，主要可以提高数据的查询速度。至于用户存储的每一条记录到底放到哪个分区，由用户决定。即用户在加载数据的时候必须显示的指定该部分数据放到哪个分区。 
1、一个表可以拥有一个或者多个分区，每个分区以文件夹的形式单独存在表文件夹的目录下。 &lt;br /&gt;
2、表和列名不区分大小写。 &lt;br /&gt;
3、分区是以字段的形式在表结构中存在，通过describe table命令可以查看到字段存在， 但是该字段不存放实际的数据内容，仅仅是分区的表示（伪列） 。&lt;br /&gt;
语法  &lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1. 创建一个分区表，以 ds 为分区列： 
create table invites (id int, name string) partitioned by (ds string) row format delimited fields terminated by &#39;t&#39; stored as textfile; 
2. 将数据添加到时间为 2013-08-16 这个分区中： 
load data local inpath &#39;/home/hadoop/Desktop/data.txt&#39; overwrite into table invites partition (ds=&#39;2013-08-16&#39;); 
3. 将数据添加到时间为 2013-08-20 这个分区中： 
load data local inpath &#39;/home/hadoop/Desktop/data.txt&#39; overwrite into table invites partition (ds=&#39;2013-08-20&#39;); 
4. 从一个分区中查询数据： 
select * from invites where ds =&#39;2013-08-12&#39;; 
5.  往一个分区表的某一个分区中添加数据： 
insert overwrite table invites partition (ds=&#39;2013-08-12&#39;) select id,max(name) from test group by id; 
可以查看分区的具体情况，使用命令： 
hadoop fs -ls /home/hadoop.hive/warehouse/invites 
或者： 
show partitions tablename; 静态/动态分区   （1）静态分区   

create table if not exists sopdm.wyp2(id int,name string,tel string)
partitioned by(age int)
row format delimited
fields terminated by &#39;,&#39;
stored as textfile;
--overwrite是覆盖，into是追加
insert into table sopdm.wyp2
partition(age=&#39;25&#39;)
select id,name,tel from sopdm.wyp; （2）动态分区  

--设置为true表示开启动态分区功能（默认为false）
set hive.exec.dynamic.partition=true;
--设置为nonstrict,表示允许所有分区都是动态的（默认为strict）
set hive.exec.dynamic.partition.mode=nonstrict;
--insert overwrite是覆盖，insert into是追加
set hive.exec.dynamic.partition.mode=nonstrict;
insert overwrite table sopdm.wyp2
partition(age)
select id,name,tel,age from sopdm.wyp;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;hive中创建分区表没有复杂的分区类型(范围分区、列表分区、hash分区、混合分区等)。分区列也不是表中的一个实际的字段，而是一个或者多个伪列。意思是说在表的数据文件中实际上并不保存分区列的信息与数据。&lt;br /&gt;
下面的语句创建了一个简单的分区表：  &lt;/p&gt;

&lt;pre&gt;&lt;code&gt;create table partition_test
(member_id string,
name string
)
partitioned by (
stat_date string,
province string)
ROW FORMAT DELIMITED FIELDS TERMINATED BY &#39;,&#39;; 这个例子中创建了stat_date和province两个字段作为分区列。通常情况下需要先预先创建好分区，然后才能使用该分区，例如： 
 
alter table partition_test add partition (stat_date=&#39;20110728&#39;,province=&#39;zhejiang&#39;);  这样就创建好了一个分区。这时我们会看到hive在HDFS存储中创建了一个相应的文件夹 每一个分区都会有一个独立的文件夹，下面是该分区所有的数据文件。在这个例子中stat_date是主层次，province是副层次，所有stat_date=&#39;20110728&#39;，而province不同的分区都会在/user/hive/warehouse/partition_test/stat_date=20110728 下面，而stat_date不同的分区都会在/user/hive/warehouse/partition_test/ 下面.    注意，因为分区列的值要转化为文件夹的存储路径，所以如果分区列的值中包含特殊值，如 &#39;%&#39;, &#39;:&#39;, &#39;/&#39;, &#39;#&#39;,它将会被使用%加上2字节的ASCII码进行转义，    特别要注意，在其他数据库中，一般向分区表中插入数据时系统会校验数据是否符合该分区，如果不符合会报错。而在hive中，向某个分区中插入什么样的数据完全是由人来控制的，因为分区键是伪列，不实际存储在文件中，   
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;动态分区，  &lt;br /&gt;
因为按照上面的方法向分区表中插入数据，如果源数据量很大，那么针对一个分区就要写一个insert，非常麻烦。况且在之前的版本中，必须先手动创建好所有的分区后才能插入，这就更麻烦了，你必须先要知道源数据中都有什么样的数据才能创建分区。&lt;br /&gt;
使用动态分区可以很好的解决上述问题。动态分区可以根据查询得到的数据自动匹配到相应的分区中去。 &lt;br /&gt;
使用动态分区要先设置hive.exec.dynamic.partition参数值为true，默认值为false，即不允许使用：  &lt;/p&gt;

&lt;pre&gt;&lt;code&gt;hive&amp;gt; 	
hive.exec.dynamic.partition=false
hive&amp;gt; set hive.exec.dynamic.partition=true; 动态分区的使用方法很简单，假设我想向stat_date=&#39;20110728&#39;这个分区下面插入数据，至于province插入到哪个子分区下面让数据库自己来判断，那可以这样写：  

hive&amp;gt; insert overwrite table partition_test partition(stat_date=&#39;20110728&#39;,province)
&amp;gt; select member_id,name,province from partition_test_input where stat_date=&#39;20110728&#39;; stat_date叫做静态分区列，province叫做动态分区列。select子句中需要把动态分区列按照分区的顺序写出来，静态分区列不用写出来。这样stat_date=&#39;20110728&#39;的所有数据，会根据province的不同分别插入到/user/hive/warehouse/partition_test/stat_date=20110728/下面的不同的子文件夹下，如果源数据对应的province子分区不存在，则会自动创建，非常方便，而且避免了人工控制插入数据与分区的映射关系存在的潜在风险。   注意，动态分区不允许主分区采用动态列而副分区采用静态列，这样将导致所有的主分区都要创建副分区静态列所定义的分区   动态分区可以允许所有的分区列都是动态分区列，但是要首先设置一个参数  hive.exec.dynamic.partition.mode ：

hive&amp;gt; set hive.exec.dynamic.partition.mode;
hive.exec.dynamic.partition.mode=strict 它的默认值是strick，即不允许分区列全部是动态的，这是为了防止用户有可能原意是只在子分区内进行动态建分区，但是由于疏忽忘记为主分区列指定值了，这将导致一个dml语句在短时间内创建大量的新的分区（对应大量新的文件夹），对系统性能带来影响。   所以我们要设置：  

hive&amp;gt; set hive.exec.dynamic.partition.mode=nostrick;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;再介绍3个参数：&lt;br /&gt;
hive.exec.max.dynamic.partitions.pernode （缺省值100）：每一个mapreduce job允许创建的分区的最大数量，如果超过了这个数量就会报错&lt;br /&gt;
hive.exec.max.dynamic.partitions （缺省值1000）：一个dml语句允许创建的所有分区的最大数量&lt;br /&gt;
hive.exec.max.created.files （缺省值100000）：所有的mapreduce job允许创建的文件的最大数量  &lt;/p&gt;

&lt;p&gt;Hive 桶&lt;br /&gt;
对于每一个表（table）或者分区， Hive可以进一步组织成桶，也就是说桶是更为细粒度的数据范围划分。Hive也是 针对某一列进行桶的组织。Hive采用对列值哈希，然后除以桶的个数求余的方式决定该条记录存放在哪个桶当中。&lt;br /&gt;
把表（或者分区）组织成桶（Bucket）有两个理由：&lt;br /&gt;
（1）获得更高的查询处理效率。桶为表加上了额外的结构，Hive 在处理有些查询时能利用这个结构。具体而言，连接两个在（包含连接列的）相同列上划分了桶的表，可以使用 Map 端连接 （Map-side join）高效的实现。比如JOIN操作。对于JOIN操作两个表有一个相同的列，如果对这两个表都进行了桶操作。那么将保存相同列值的桶进行JOIN操作就可以，可以大大较少JOIN的数据量。&lt;br /&gt;
（2）使取样（sampling）更高效。在处理大规模数据集时，在开发和修改查询的阶段，如果能在数据集的一小部分数据上试运行查询，会带来很多方便。  &lt;/p&gt;

&lt;p&gt;创建带桶的 table:  &lt;/p&gt;

&lt;pre&gt;&lt;code&gt;create table bucketed_user(id int,name string) clustered by (id) sorted by(name) into 4 buckets row format delimited fields terminated by &#39;\t&#39; stored as textfile; 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;首先，使用CLUSTERED BY 子句来指定划分桶所用的列和要划分的桶的个数： &lt;/p&gt;

&lt;pre&gt;&lt;code&gt;CREATE TABLE bucketed_user (id INT) name STRING)  CLUSTERED BY (id) INTO 4 BUCKETS;  在这里，我们使用用户ID来确定如何划分桶(Hive使用对值进行哈希并将结果除 以桶的个数取余数。 对于map端连接的情况，两个表以相同方式划分桶。处理左边表内某个桶的 mapper知道右边表内相匹配的行在对应的桶内。因此，mapper只需要获取那个桶 (这只是右边表内存储数据的一小部分)即可进行连接。这一优化方法并不一定要求 两个表必须桶的个数相同，两个表的桶个数是倍数关系也可以。 桶中的数据可以根据一个或多个列另外进行排序。由于这样对每个桶的连接变成了高效的归并排序(merge-sort), 因此可以进一步提升map端连接的效率。以下语法声明一个表使其使用排序桶：    
CREATE TABLE bucketed_users (id INT, name STRING)  CLUSTERED BY (id) SORTED BY (id ASC) INTO 4 BUCKETS;  我们如何保证表中的数据都划分成桶了呢？把在Hive外生成的数据加载到划分成桶的表中，当然是可以的。其实让Hive来划分桶更容易。这一操作通常针对已有的表。    Hive并不检查数据文件中的桶是否和表定义中的桶一致(无论是对于桶 的数量或用于划分桶的列）。如果两者不匹配，在査询时可能会碰到错 误或未定义的结果。因此，建议让Hive来进行划分桶的操作。   
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;索引&lt;br /&gt;
索引可以加快含有group by语句的查询的计算速度  &lt;/p&gt;

&lt;pre&gt;&lt;code&gt;create index employees_index on table employees(country)
as  &#39;org.apache.hadoop.hive.ql.index.compact.CompactIndexHandler&#39;
with deferred rebuild
in table employees_index_table ;
&lt;/code&gt;&lt;/pre&gt;
</description>
        <pubDate>Sun, 01 Feb 2015 00:00:00 +0800</pubDate>
        <link>index.html/blog/2015/02/01/Hive-partition.html</link>
        <guid isPermaLink="true">index.html/blog/2015/02/01/Hive-partition.html</guid>
        
        <category>hadoop</category>
        
      </item>
    
  </channel>
</rss>
