<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>RSS - willgo</title>
    <description>willgo - 最好的从未错过</description>
    <link>index.html</link>
    <atom:link href="index.html/page/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Sun, 20 Sep 2015 17:23:40 +0800</pubDate>
    <lastBuildDate>Sun, 20 Sep 2015 17:23:40 +0800</lastBuildDate>
    <generator>Will.Quan</generator>
    
      <item>
        <title>Sqoop密码和SA用户问题记录</title>
        <description>&lt;p&gt;sqoop 定时job 运行时需要输入密码问题
1.可以使用except 脚本，实现交互输入密码&lt;br /&gt;
2. 配置sqoop，将密码保存至sqoop的metadata中 &lt;br /&gt;
sqoop-site.xml：  &lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;property&amp;gt;  
     &amp;lt;name&amp;gt;sqoop.metastore.client.record.password&amp;lt;/name&amp;gt;  
     &amp;lt;value&amp;gt;true&amp;lt;/value&amp;gt;  
     &amp;lt;description&amp;gt;If true, allow saved passwords in the metastore. &amp;lt;/description&amp;gt;
&amp;lt;/property&amp;gt;  
&lt;/code&gt;&lt;/pre&gt;

&lt;ol&gt;
  &lt;li&gt;sqoop 出现Caused by: java.sql.SQLException: User not found: SA  错误  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;未知问题，看起来像是并发执行sqoop引起的问题  导致无法连接sqoop的metadata库&lt;br /&gt;
不知如何解决， 可以采取的办法&lt;br /&gt;
1.换个用户执行 貌似可以（。。。。。）&lt;br /&gt;
2.更改metadata数据库  如修改成MySQL   如下：  &lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;property&amp;gt;  
    &amp;lt;name&amp;gt;sqoop.metastore.client.autoconnect.url&amp;lt;/name&amp;gt;  
    &amp;lt;value&amp;gt;jdbc:mysql://192.168.117.7:3306/sqoop&amp;lt;/value&amp;gt;  
&amp;lt;/property&amp;gt;  
&amp;lt;property&amp;gt;  
    &amp;lt;name&amp;gt;sqoop.metastore.client.autoconnect.username&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;scm987&amp;lt;/value&amp;gt;  
&amp;lt;/property&amp;gt;  
&amp;lt;property&amp;gt;  
    &amp;lt;name&amp;gt;sqoop.metastore.client.autoconnect.password&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;scm258&amp;lt;/value&amp;gt;  
&amp;lt;/property&amp;gt;  


&amp;lt;property&amp;gt;
     &amp;lt;name&amp;gt;sqoop.metastore.client.enable.autoconnect&amp;lt;/name&amp;gt;
     &amp;lt;value&amp;gt;true&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
</description>
        <pubDate>Sun, 20 Sep 2015 00:00:00 +0800</pubDate>
        <link>index.html/blog/2015/09/20/sqoopmimashuruwenti.html</link>
        <guid isPermaLink="true">index.html/blog/2015/09/20/sqoopmimashuruwenti.html</guid>
        
        <category>hadoop</category>
        
      </item>
    
      <item>
        <title>Sql on hadoop 选择笔记</title>
        <description>&lt;p&gt;Hive展现出他强大的批处理能力 但在实时交互式查询时方面却难以满足，现在已经出现的低延时交互式处理方案已经有很多  如：Hive on Tez, Hive on Spark, Spark SQL, Impala等   &lt;/p&gt;

&lt;p&gt;Hive/Tez/Stinger &lt;br /&gt;
目前的主要推动者是hortonworks和Yahoo!。2015 Hadoop Summit(San Jose)上，Yahoo分享了他们目前生产环境中Hive on Tez的一些情况。显示和Hive 0.10(RCFile)相比，目前的Hive on Tez在1TB的数据量查询的加速比平均为6.2倍。目前的Hive on Tez已经是production-ready。Tez这个执行引擎和Spark比较类似，原来的MR只能执行Map和Reduce两种操作，现在的Tez可以把Job解析成DAG来执行。除此之外还有一些进一步优化Hive执行效率的工作，例如Vectorized Execution和ORCFile等。Dropbox也透露他们的Hive集群下一步的升级目标就是Hive on Tez。  &lt;/p&gt;

&lt;p&gt;Hive on Spark &lt;br /&gt;
目前的主要推动者是Cloudera，可以认为是Hive社区这边搞的”Spark SQL”。刚刚release了第一个使用版本，目前不能用于生产环境。Hive on Spark既能利用到现在广泛使用的Hive的前端，又能利用到广泛使用的Spark作为后端执行引擎。对于现在既部署了Hive，又部署了Spark的公司来说，节省了运维成本。    &lt;/p&gt;

&lt;p&gt;对于上面提到的Hive on Tez和Hive on Spark两种系统都具备的优点是： &lt;br /&gt;
1，现存的Hive jobs可以透明、无缝迁移到Hive on ***平台，可以利用Hive现有的ODBC/JDBC，metastore, hiveserver2, UDF，auditing, authorization, monitoring系统，不需要做任何更改和测试，迁移成本低。 &lt;br /&gt;
2，无论后端执行引擎是MapReduce也好，Tez也好，Spark也好，整个Hive SQL解析、生成执行计划、执行计划优化的过程都是非常类似的。而且大部分公司都积累了一定的Hive运维和使用经验，那么对于bug调试、性能调优等环节会比较熟悉，降低了运维成本。   &lt;/p&gt;

&lt;p&gt;Spark SQL &lt;br /&gt;
主要的推动者是Databricks。提到Spark SQL不得不提的就是Shark。Shark可以理解为Spark社区这边搞的一个”Hive on Spark”，把Hive的物理执行计划使用Spark计算引擎去执行。这里面会有一些问题，Hive社区那边没有把物理执行计划到执行引擎这个步骤抽象出公共API，所以Spark社区这边要自己维护一个Hive的分支，而且Hive的设计和发展不太会考虑到如何优化Spark的Job。但是前面提到的Hive on Spark却是和Hive一起发布的，是由Hive社区控制的。 &lt;br /&gt;
所以后来Spark社区就停止了Shark的开发转向Spark SQL（“坑了”一部分当时信任Shark的人）。  Spark SQL是把SQL解析成RDD的transformation和action，而且通过catalyst可以自由、灵活的选择最优执行方案。对数据库有深入研究的人就会知道，SQL执行计划的优化是一个非常重要的环节，  Spark SQL在这方面的优势非常明显，提供了一个非常灵活、可扩展的架构。但是Spark SQL是基于内存的，元数据放在内存里面，不适合作为数据仓库的一部分来使用。所以有了Spark SQL的HiveContext，就是兼容Hive的Spark SQL。它支持HiveQL, Hive Metastore, Hive SerDes and Hive UDFs以及JDBC driver。这样看起来很完美，但是实际上也有一些缺点：Spark SQL依赖于Hive的一个snapshot，所以它总是比Hive的发布晚一个版本，很多Hive新的feature和bug fix它就无法包括。而且目前看Spark社区在Spark的thriftserver方面的投入不是很大，所以感觉它不是特别想朝着这个方向发展。还有一个重要的缺点就是Spark SQL目前还不能通过分析SQL来预测这个查询需要多少资源从而申请对应的资源，所以在共享集群上无法高效地分配资源和调度任务。 &lt;br /&gt;
特别是目前Spark社区把Spark SQL朝向DataFrame发展，目标是提供一个类似R或者Pandas的接口，把这个作为主要的发展方向。DataFrame这个功能使得Spark成为机器学习和数据科学领域不可或缺的一个组件，但是在数据仓库（ETL，交互式分析，BI查询）领域感觉已经不打算作为他们主要的发展目标了。  &lt;/p&gt;

&lt;p&gt;Impala &lt;br /&gt;
主要的推动者是Cloudera，自从推出以来一直不温不火。Impala是一种MPP架构的执行引擎，查询速度非常快，是交互式BI查询最好的选择，即使是在并发性非常高的情况下也能保证查询延迟，所以在multi-tenant, shared clusters上表现比较好。Impala的另外一个重要的优点就是支持的SQL是在以上这些系统中是最标准的，也就是跟SQL99是最像的，所以对于传统企业来说可能是个不错的选择。  Impala的主要缺点是社区不活跃，由C++开发，可维护性差，目前系统稳定性还有待提高。  &lt;/p&gt;

&lt;p&gt;Presto&lt;br /&gt;
是Facebook开发的，目前也得到了Teradata的支持。目前Presto的主要使用者还是互联网公司，像Facebook，Netflix等。Presto的代码用了Dependency Injection, 比较难理解和debug。另外还有一些系统，像Apache Drill，Apache Tajo等，都是非常小众的系统了。   &lt;/p&gt;

&lt;p&gt;总的来说，目前来看Hive依然是批处理/ETL 类应用的首选。Hive on Spark能够降低Hive的延迟，但是还是达不到交互式BI查询的需求。目前交互式BI查询最好的选择是Impala。Spark SQL/DataFrame是Spark用户使用SQL或者DataFrame API构建Spark pipeline的一种选择，并不是一个通用的支持交互式查询的引擎，更多的会用在基于Spark的机器学习任务的数据处理和准备的环节。     &lt;/p&gt;
</description>
        <pubDate>Sun, 20 Sep 2015 00:00:00 +0800</pubDate>
        <link>index.html/blog/2015/09/20/nosql-onhadoop.html</link>
        <guid isPermaLink="true">index.html/blog/2015/09/20/nosql-onhadoop.html</guid>
        
        <category>nosql</category>
        
      </item>
    
      <item>
        <title>JVM笔记</title>
        <description>&lt;p&gt;JVM根据实现不同结构有所不同，大多数将内存分为：&lt;br /&gt;
Method Area                        方法区&lt;br /&gt;
Heap			                     堆
Program Counter register               程序计数器&lt;br /&gt;
Java method stack                     Java方法栈 &lt;br /&gt;
native method stack                   本地方法栈 （Hot Spot 中将Java method和native合称为方法区）&lt;br /&gt;
direct memory                       直接内存区（此区域并不归JVM管理）    &lt;/p&gt;

&lt;p&gt;指令 数据 方法 实例 属性。。。&lt;br /&gt;
方法是指令操作码的一部分保存在stack中&lt;br /&gt;
方法内部变量作为指令的操作数部分，跟在指令的操作码之后，保存在Stack中（实际上是简单类型保存在Stack中，对象类型在Stack中保存地址，在Heap 中保存值）；指令操作码和指令操作数构成了完整的Java 指令。对象实例包括其属性值作为数据，保存在数据区Heap 中&lt;br /&gt;
非静态的对象属性作为对象实例的一部分保存在Heap 中，而对象实例必须通过Stack中保存的地址指针才能访问到。因此能否访问到对象实例以及它的非静态属性值完全取决于能否获得对象实例在Stack中的地址指针。   &lt;/p&gt;

&lt;p&gt;静态/非静态方法 &lt;br /&gt;
非静态方法有一个隐含的传入参数，该参数是JVM给它的，和我们怎么写代码无关，这个隐含的参数就是对象实例在Stack中的地址指针。So我们在调用前都要new,获得Stack中的地址指针。 &lt;br /&gt;
静态方法无此隐含参数，因此也不需要new对象，只要class文件被ClassLoader load进入JVM的Stack，该静态方法即可被调用。当然此时静态方法是存取不到Heap 中的对象属性的。   &lt;/p&gt;

&lt;p&gt;静/动态属性 &lt;br /&gt;
实例以及动态属性都是保存在Heap 中的， Heap 必须通过Stack中的地址指针才能够被指令（类的方法）访问到。 &lt;br /&gt;
静态属性是保存在Stack中的，而不同于动态属性保存在Heap 中。正因为都是在Stack中，而Stack中指令和数据都是定长的，因此很容易算出偏移量，也因此不管什么指令，都可以访问到类的静态属性。也正因为静态属性被保存在Stack中，所以具有了全局属性。 &lt;br /&gt;
在JVM中，静态属性保存在Stack指令内存区，动态属性保存在Heap数据内存区。   &lt;/p&gt;

&lt;p&gt;栈&lt;br /&gt;
Stack（栈）是JVM的内存指令区。Stack的速度很快，管理很简单，并且每次操作的数据或者指令字节长度是已知的。所以Java 基本数据类型，Java 指令代码，常量都保存在Stack中。&lt;br /&gt;
是 Java 程序的运行区，是在线程创建时创建，它的生命期是跟随线程的生命期，线程结束栈内存也就释放，对于栈来说不存在垃圾回收问题。&lt;br /&gt;
栈中数据都是以栈帧（stack frame）的形式存在，栈帧是一个内存块一个有关方法和运行期数据的数据集，遵循 先进后出原则（方法 A 被调用时就产生了一个栈帧 F1，并被压入到栈中，A 方法又调用了 B 方法，于是产生栈帧 F2 也被压入栈，执行完毕后，先弹出 F2栈帧，再弹出 F1 栈帧）
栈帧中主要保存三类数据：&lt;br /&gt;
local variable 本地变量（输入参数和输出参数以及方法内的变量）&lt;br /&gt;
operand stack栈操作  （出栈进栈记录）&lt;br /&gt;
frame data栈帧数据   （类文件 方法等）  &lt;/p&gt;

&lt;p&gt;Heap&lt;br /&gt;
JVM的内存数据区 管理复杂，用于保存对象的实例 &lt;br /&gt;
Heap 中分配一定的内存来保存对象实例，实际上也只是保存对象实例的属性值，属性的类型和对象本身的类型标记等，并不保存对象的方法（方法是指令，保存在Stack中）,在Heap 中分配一定的内存保存对象实例和对象的序列化比较类似。而对象实例在Heap 中分配好以后，需要在Stack中保存一个4字节的Heap 内存地址，用来定位该对象实例在Heap 中的位置，便于找到该对象实例。   &lt;/p&gt;

&lt;p&gt;Java中堆是由所有的线程共享的一块内存区域。 &lt;br /&gt;
Heap 中分配一定的内存来保存对象实例，实际上也只是保存对象实例的属性值，属性的类型和对象本身的类型标记等，并不保存对象的方法（方法是指令，保存在Stack中）,在Heap 中分配一定的内存保存对象实例和对象的序列化比较类似。而对象实例在Heap 中分配好以后，需要在Stack中保存一个4字节的Heap 内存地址，用来定位该对象实例在Heap 中的位置，便于找到该对象实例。&lt;br /&gt;
Java中堆是由所有的线程共享的一块内存区域。   &lt;/p&gt;

&lt;p&gt;Perm &lt;br /&gt;
Perm代主要保存class,method,filed对象，这部门的空间一般不会溢出，除非一次性加载了很多的类，不过在涉及到热部署的应用服务器的时候，有时候会遇到java.lang.OutOfMemoryError : PermGen space 的错误，造成这个错误的很大原因就有可能是每次都重新部署，但是重新部署后，类的class没有被卸载掉，这样就造成了大量的class对象保存在了perm中，这种情况下，一般重新启动应用服务器可以解决问题。    &lt;/p&gt;

&lt;p&gt;Tenured &lt;br /&gt;
Tenured区主要保存生命周期长的对象，一般是一些老的对象，当一些对象在Young复制转移一定的次数以后，对象就会被转移到Tenured区，一般如果系统中用了application级别的缓存，缓存中的对象往往会被转移到这一区间。  &lt;/p&gt;

&lt;p&gt;Young&lt;br /&gt;
Young区被划分为三部分，Eden区和两个大小严格相同的Survivor区，其中Survivor区间中，某一时刻只有其中一个是被使用的，另外一个留做垃圾收集时复制对象用，在Young区间变满的时候，minor GC就会将存活的对象移到空闲的Survivor区间中，根据JVM的策略，在经过几次垃圾收集后，任然存活于Survivor的对象将被移动到Tenured区间。  &lt;/p&gt;

&lt;p&gt;The pc Register 程序计数器寄存器 &lt;br /&gt;
JVM支持多个线程同时运行。每个JVM都有自己的程序计数器。在任何一个点，每个JVM线程执行单个方法的代码，这个方法是线程的当前方法。如果方法不是native的，程序计数器寄存器包含了当前执行的JVM指令的地址，如果方法是 native的，程序计数器寄存器的值不会被定义。 JVM的程序计数器寄存器的宽度足够保证可以持有一个返回地址或者native的指针。  &lt;/p&gt;

&lt;p&gt;Method Area 方法区&lt;br /&gt;
Object Class Data(类定义数据) 是存储在方法区的。除此之外，常量、静态变量、JIT 编译后的代码也都在方法区。正因为方法区所存储的数据与堆有一种类比关系，所以它还被称为 Non-Heap。方法区也可以是内存不连续的区域组成的，并且可设置为固定大小，也可以设置为可扩展的，这点与堆一样。
方法区内部有一个非常重要的区域，叫做运行时常量池（Runtime Constant Pool，简称 RCP）。在字节码文件中有常量池（Constant Pool Table），用于存储编译器产生的字面量和符号引用。每个字节码文件中的常量池在类被加载后，都会存储到方法区中。值得注意的是，运行时产生的新常量也可以被放入常量池中，比如 String 类中的 intern() 方法产生的常量。    &lt;/p&gt;

&lt;p&gt;内存分配：&lt;br /&gt;
1、对象优先在EDEN分配&lt;br /&gt;
2、大对象直接进入老年代 &lt;br /&gt;
3、长期存活的对象将进入老年代 &lt;br /&gt;
4、适龄对象也可能进入老年代：动态对象年龄判断&lt;br /&gt;
动态对象年龄判断：&lt;br /&gt;
虚拟机并不总是要求对象的年龄必须达到MaxTenuringThreshold才能晋升到老年代，当Survivor空间的相同年龄的所有对象大小总和大于Survivor空间的一半，年龄大于或等于该年龄的对象就可以直接进入老年代，无需等到MaxTenuringThreshold中指定的年龄  &lt;/p&gt;

&lt;p&gt;1、对象优先在Eden分配，这里大部分对象具有朝生夕灭的特征，Minor GC主要清理该处&lt;br /&gt;
2、大对象（占内存大）、老对象（使用频繁）&lt;br /&gt;
3、Survivor无法容纳的对象，将进入老年代，Full GC的主要清理该处  &lt;/p&gt;

&lt;p&gt;JVM的GC机制&lt;br /&gt;
第一个线程负责回收Heap的Young区&lt;br /&gt;
第二个线程在Heap不足时，遍历Heap，将Young 区升级为Older区&lt;br /&gt;
Older区的大小等于-Xmx减去-Xmn，不能将-Xms的值设的过大，因为第二个线程被迫运行会降低JVM的性能。 &lt;br /&gt;
堆内存GC&lt;br /&gt;
       JVM(采用分代回收的策略)，用较高的频率对年轻的对象(young generation)进行YGC，而对老对象(tenured generation)较少(tenured generation 满了后才进行)进行Full GC。这样就不需要每次GC都将内存中所有对象都检查一遍。&lt;br /&gt;
非堆内存不GC&lt;br /&gt;
      GC不会在主程序运行期对PermGen Space进行清理，所以如果你的应用中有很多CLASS(特别是动态生成类，当然permgen space存放的内容不仅限于类)的话,就很可能出现PermGen Space错误。&lt;br /&gt;
内存申请过程 &lt;br /&gt;
1.JVM会试图为相关Java对象在Eden中初始化一块内存区域；&lt;br /&gt;
2.当Eden空间足够时，内存申请结束。否则到下一步；&lt;br /&gt;
3.JVM试图释放在Eden中所有不活跃的对象（minor collection），释放后若Eden空间仍然不足以放入新对象，则试图将部分Eden中活跃对象放入Survivor区；&lt;br /&gt;
4.Survivor区被用来作为Eden及old的中间交换区域，当OLD区空间足够时，Survivor区的对象会被移到Old区，否则会被保留在Survivor区；&lt;br /&gt;
5.当old区空间不够时，JVM会在old区进行major collection；&lt;br /&gt;
6.完全垃圾收集后，若Survivor及old区仍然无法存放从Eden复制过来的部分对象，导致JVM无法在Eden区为新对象创建内存区域，则出现”Out of memory错误”；  &lt;/p&gt;

&lt;p&gt;对象衰老过程 &lt;br /&gt;
1.新创建的对象的内存都分配自eden。Minor collection的过程就是将eden和在用survivor space中的活对象copy到空闲survivor space中。对象在young generation里经历了一定次数(可以通过参数配置)的minor collection后，就会被移到old generation中，称为tenuring。&lt;br /&gt;
&lt;img src=&quot;/image/jvm-memery.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;类的加载方式 &lt;br /&gt;
1）：本地编译好的class中直接加载&lt;br /&gt;
2）：网络加载：java.net.URLClassLoader可以加载url指定的类&lt;br /&gt;
3）：从jar、zip等等压缩文件加载类，自动解析jar文件找到class文件去加载util类&lt;br /&gt;
4）：从java源代码文件动态编译成为class文件  &lt;/p&gt;

&lt;p&gt;类加载的时机&lt;br /&gt;
1. 类加载的 生命周期 ：加载（Loading）–&amp;gt;验证（Verification）–&amp;gt;准备（Preparation）–&amp;gt;解析（Resolution）–&amp;gt;初始化（Initialization）–&amp;gt;使用（Using）–&amp;gt;卸载（Unloading）&lt;br /&gt;
2. 加载：这有虚拟机自行决定。&lt;br /&gt;
3. 初始化阶段：&lt;br /&gt;
a) 遇到new、getstatic、putstatic、invokestatic这4个字节码指令时，如果类没有进行过初始化，出发初始化操作。 &lt;br /&gt;
b) 使用java.lang.reflect包的方法对类进行反射调用时。&lt;br /&gt;
c) 当初始化一个类的时候，如果发现其父类还没有执行初始化则进行初始化。&lt;br /&gt;
d) 虚拟机启动时用户需要指定一个需要执行的主类，虚拟机首先初始化这个主类。&lt;br /&gt;
注意：接口与类的初始化规则在第三点不同，接口不要气所有的父接口都进行初始化。   &lt;/p&gt;

&lt;p&gt;双亲委派机制 &lt;br /&gt;
JVM在加载类时默认采用的是 双亲委派 机制。通俗的讲，就是某个特定的类加载器在接到加载类的请求时，首先将加载任务委托给父类加载器，依次递归，如果父类加载器可以完成类加载任务，就成功返回；只有父类加载器无法完成此加载任务时，才自己去加载。   &lt;/p&gt;
</description>
        <pubDate>Sun, 20 Sep 2015 00:00:00 +0800</pubDate>
        <link>index.html/blog/2015/09/20/jvm-zaixuexi.html</link>
        <guid isPermaLink="true">index.html/blog/2015/09/20/jvm-zaixuexi.html</guid>
        
        <category>java</category>
        
      </item>
    
      <item>
        <title>HDFS 分层存储</title>
        <description>&lt;p&gt;Hadoop  &lt;br /&gt;
众所周知，商用硬件可以组装起来创建拥有大数据存储和计算能力的Hadoop集群。将数据拆分成多个部分，分别存储在每个单独的机器上，数据处理逻辑也在同样的机器上执行。   &lt;/p&gt;

&lt;p&gt;例如：一个1000个节点组成的Hadoop集群，单节点容量有20TB，最多可以存储20PB的数据。因此，所有的这些机器拥有足够的计算能力来履行Hadoop的口号：“take compute to data”。 &lt;br /&gt;
数据的温度 &lt;br /&gt;
集群中通常存储着各种不同类型的数据集，不同的团队不同的业务通过该集群可以共享地处理他们不同类型的工作任务。通过数据管道，每个数据集每时每刻都会得到增长。 &lt;br /&gt;
数据集有一个共同特点就是初始的使用量会很大。在此期间，数据集被认为是“热(HOT)”的。我们通过分析发现，随着时间的推移，使用率会有一定程度的下降，存储的数据每周仅仅就被访问几次，逐渐就变为“温(WARM)”数据。在此后90天中，当数据使用率跌至一个月几次时，它就被定义为“冷(COLD)”数据。&lt;br /&gt;
因此数据在最初几天被认为是“热”的，此后第一个月仍然保持为“温”的。在这期间，任务或应用会使用几次该数据。随着数据的使用率下降得更多，它就变“冷”了，在此后90天内或许只被使用寥寥几次。最终，当数据一年只有一两次使用频率、极少用到时，它的“温度”就是“冰冻”的了。   &lt;/p&gt;

&lt;p&gt;一般来讲，温度与每个数据集都紧密相关。在这个例子中，温度是与数据的年龄成反比的。一个特定数据集的温度也受其他因素影响的。你也可以通过算法决定数据集的温度。&lt;br /&gt;
HDFS的分层存储&lt;br /&gt;
HDFS从Hadoop2.3开始支持分层存储&lt;br /&gt;
它是如何工作的呢？ &lt;br /&gt;
正常情况下，一台机器添加到集群后，将会有指定的本地文件系统目录来存储这块副本。用来指定本地存储目录的参数是dfs.datanode.dir。另一层中，比如归档(ARCHIVE)层，可以使用名为StorageType的枚举来添加。为了表明这个本地目录属于归档层，该本地目录配置中会带有[ARCHIVE]的前缀。理论上，hadoop集群管理员可以定义多个层级。 &lt;br /&gt;
比如说：如果在一个已有1000个节点，其总存储容量为20PB的集群上，增加100个节点，其中每个节点有200TB的存储容量。相比已有的1000个节点，这些新增节点的计算能力就相对较差。接下来，我们在所有本地目录的配置中增加ARCHIVE的前缀。那么现在位于归档层的这100个节点将会有20PB的存储量。最后整个集群被划分为两层——磁盘(DISK)层和归档(ARCHIVE)层，每一层有20PB的容量，总容量为40PB。   &lt;/p&gt;

&lt;p&gt;基于温度将数据映射到存储层 &lt;br /&gt;
在这个例子中，我们将在拥有更强计算能力节点的DISK层存储高频率使用的“热(HOT)”数据。 &lt;br /&gt;
至于“温(WARM)”数据，我们将其大部分的副本存储在磁盘层。对于复制因子(replication factor)为3的数据，我们将在磁盘层存储其两个副本，在归档层存储一个副本。 &lt;br /&gt;
如果数据已经变“冷(COLD)”,那么我们至少将在磁盘层存储其每个块的一个副本。余下的副本都放入归档层。   &lt;/p&gt;

&lt;p&gt;当一个数据集为认为是“冰冻(FROZEN)”的,这就意味着它几乎已经不被使用，将其存储在具有大量CPU、能执行众多任务节点或容器的节点上是不明智的。我们会把它存储到一个具有最小计算能力的节点上。因此，所有处于“冰冻(FROZEN)”状态块的全部副本都可以被移动到归档层。 &lt;br /&gt;
跨层的数据流 &lt;br /&gt;
当数据第一次添加到集群中，它将被存储到默认的磁盘层。基于数据的温度，它的一个或多个副本将被移动到归档层。移动器就是用来把数据从一个层移动到另一层的。移动器的工作原理类似平衡器，除了它可以跨层地移动块的副本。移动器可接受一条HDFS路径，一个副本数目和目的地层信息。然后它将基于所述层的信息识别将要被移动的副本，并调度数据在源数据节点到目的数据节点的移动。&lt;br /&gt;
Hadoop 2.6中支持分层存储的变化 &lt;br /&gt;
Hadoop 2.6中有许多的改进使其能够进一步支持分层存储。你可以附加一个存储策略到某个目录来指明它是“热(HOT)”的,“温(WARM)”的,“冷(COLD)”的, 还是“冰冻(FROZEN)”的。存储策略定义了每一层可存储的副本数量。我可以改变目录的存储策略并启动该目录的移动器来使得策略生效。 &lt;br /&gt;
使用数据的应用 &lt;br /&gt;
基于数据的温度，数据的部分或者全部副本可能存储在任一层中。但对于通过HDFS来使用数据的应用而言，其位置是透明的。 &lt;br /&gt;
虽然“冰冻”数据的所有副本都在归档层，应用依然可以像访问HDFS的任何数据一样来访问它。由于归档层中的节点并没有计算能力，运行在磁盘层的映射(map)任务将从归档层的节点上读取数据，但这会导致增加应用的网络流量消耗。如果这种情况频繁地发生，你可以指定该数据为“温/冷”,并让移动器移回一个或多个副本到磁盘层。 &lt;br /&gt;
确定数据温度以及完成指定的副本移动至预先定义的分层存储可以全部自动化。 &lt;br /&gt;
总结 &lt;br /&gt;
无计算能力的存储比有计算能力的存储要便宜。我们可以依据数据的温度来确保具计算能力的存储能得到充分地使用。因为每一个分块的数据都会被复制多次（默认是3次），根据数据的温度，许多副本都会被移动到低成本的存储中。HDFS支持分层存储并提供必要的工具来进行跨层的数据移动。   &lt;/p&gt;
</description>
        <pubDate>Sun, 20 Sep 2015 00:00:00 +0800</pubDate>
        <link>index.html/blog/2015/09/20/hdfs-fenceng.html</link>
        <guid isPermaLink="true">index.html/blog/2015/09/20/hdfs-fenceng.html</guid>
        
        <category>hadoop</category>
        
      </item>
    
      <item>
        <title>大数据处理的关键架构层</title>
        <description>&lt;p&gt;&lt;img src=&quot;/image/大数据处理的关键架构层.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
</description>
        <pubDate>Sun, 20 Sep 2015 00:00:00 +0800</pubDate>
        <link>index.html/blog/2015/09/20/guanjianjiagouceng.html</link>
        <guid isPermaLink="true">index.html/blog/2015/09/20/guanjianjiagouceng.html</guid>
        
        <category>hadoop</category>
        
      </item>
    
      <item>
        <title>多源ETL考虑</title>
        <description>&lt;p&gt;ETL解决问题：&lt;br /&gt;
1.数据分散问题&lt;br /&gt;
2.数据不清洁问题&lt;br /&gt;
3.对数据格式不统一  &lt;/p&gt;

&lt;p&gt;ETL 过程模型&lt;br /&gt;
1. 元数据库&lt;br /&gt;
 元数据（metadata）是定义和描述其它数据的数据    关于数据内容、质量、状况和其他特性的信息，在整个数据抽取转换加载过程中起到基础的作用。元数据使用户可以掌握数据的历史情况，如数据从哪里来，流通时间多长，更新频率是多大，数据元素的含义是什么，对它已经进行了哪些计算、转换和筛选等等。&lt;br /&gt;
    元数据具有下列属性：&lt;br /&gt;
（1）描述性，元数据是描述数据的数据，这是元数据的最本质的特征。&lt;br /&gt;
（2）动态性，元数据不是静止不变的，它随着所描述对象的变化而变化。&lt;br /&gt;
（3）多样性，元数据的类型多样。&lt;br /&gt;
（4）复杂性，一方面元数据既可以是集合概念也可以是个体概念，元数据中还可以包括其它的元数据；另一方面对不同的描述对象，有些元数据项是必须有的，而有些却不一定强求，即强制性的元数据与选择性的元数据共存。&lt;br /&gt;
（5）多层次性，这是由元数据所描述对象的多层次和元数据使用对象的多层次性决定的。&lt;br /&gt;
（6）支撑性，元数据相对描述对象而言处于次要的地位，但又是必不可少的，起支撑的作用。&lt;br /&gt;
    元数据库是以一定的组织方式存储在一起的相关的元数据集合。  &lt;/p&gt;

&lt;p&gt;元数据可以应该包括下列7个组件。 &lt;br /&gt;
    （1）环境状况组件。环境状况组件主要是用于监控网络和源数据的状况，它包括：网络状态，各种源数据状态，最佳抽取时间。网络状态和各种源数据状态的数据可以通过定时对网络状况和源数据状况探测自动获取。通过对这两组数据的历史记录分析可以生成最佳抽取时间。&lt;br /&gt;
    （2）基本组件。基本组件包括数据源数据库表结构、数据源数据库表属性、数据仓库表结构、数据仓库表属性，等等。基本构件和其它元数据最大的区别在于它是具有版本标识的数据，具有版本标识的数据在很长的一段时间内可以跟踪数据的变化情况。基本构件主要是对源数据的特征进行描述，它包括：可以提供源数据的数据库名，数据库编号，这些数据库的表，表的编号，表中的属性，属性的编号，以及可以提供源数据的文件系统的文件类型、分隔符、转换为数据库系统中目标表的表名等。&lt;br /&gt;
    （3）数据状态组件。数据状态组件用于标识数据仓库中的数据是“活性”的还是“惰性”的。由于数据仓库中的数据都是基于共享设计的，因而当将数据仓库中的数据作为源数据进行抽取和转换时，其中的某些数据可能包括一些误导信息，因而对于这些表就需要数据状态字段对它进行控制。 &lt;br /&gt;
    （4）存取模式组件。存取模式组件是用于确定异构的源数据什么时候将什么数据迁移到数据仓库中，它包括：存取数据的类型、总数以及频率等。在并行环境下，它可以确定如何物理地分离数据，这样可以极大地提高数据传送的效率。 &lt;br /&gt;
    （5）数据质量要求组件。数据质量规则定义了源数据中的质量要求，它包括了数据源的编号、错误类型编号、可能的修改规则编号等。 &lt;br /&gt;
    （6）映射规则组件。映射规则定义了数据由数据源到数据仓库映射的规则，它包括：源字段的编号；简单的属性到属性的映射；字段类型的转换；多个源表到一个目标表之间复杂的转换；命名的改变；关键字的改变；等等。 &lt;br /&gt;
    （7）抽取日志组件。抽取日志组件记录了对数据仓库中的数据进行的每次操作的时间、操作方式、操作过程以及结果。这些信息对于数据仓库的维护非常有用，拥有这些信息可以对ETL过程中的每一步进行监控。 &lt;br /&gt;
 &lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;数据预抽取&lt;br /&gt;
    按照元数据定义的内容、频率和规则，将保存在有关数据源中的数据抽取出来，存放到另外的数据库中，并将预抽取操作记录在元数据库中。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;数据质量检验 &lt;br /&gt;
    数据质量是数据使用的适合性。数据质量要求是关于数据明示的、通常隐含的或必须履行的需求或期望。
    数据质量检验是依据元数据中定义的各数据质量要求，通过判断，对数据与质量要求的符合性进行评价，并将数据质量检验操作记录在元数据库中。
    数据质量主要有两个方面的问题：一个是单数据源数据质量问题，另一个是多数据源的数据交互集成时的数据质量问题。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

</description>
        <pubDate>Sun, 20 Sep 2015 00:00:00 +0800</pubDate>
        <link>index.html/blog/2015/09/20/duoyuan-ETL.html</link>
        <guid isPermaLink="true">index.html/blog/2015/09/20/duoyuan-ETL.html</guid>
        
        <category>hadoop</category>
        
      </item>
    
      <item>
        <title>数据平台拓展</title>
        <description>&lt;h3 id=&quot;section&quot;&gt;一、数据的位置&lt;/h3&gt;
&lt;p&gt;数据存储数据存储，管理的就是数据的位置，在CPU的位置，数据相对其他数据的位置，CPU善于处理顺序性操作数据指令，即：数据预取。&lt;br /&gt;
随机读取操作即成为瓶颈，预取到缓存、前端总线的数据都是无效的。传统意义上说，磁盘的存取性能要弱于内存，但是要分随机存取及顺序存取不同的场景下讨论。&lt;br /&gt;
#####1、数据存储和更新&lt;br /&gt;
追加写可以让我们尽量保持顺序存储文件。但是当数据要进行更新的时候，有两种选择，一种是在数据原地进行更新操作，这样我们就有了随机IO操作。另一种是把更新都放到文件末尾，然后需要读取更新数据的时候进行替换。&lt;br /&gt;
#####2、读取数据&lt;br /&gt;
一下子读取整个文件，也是很耗费时间的事情，例如数据库中的全表扫描。当我们读取文件中某一个字段时候，我们需要索引。索引的方式有多种，我们可以用一种简单的固定数值大小的有序数组来做索引，数组里存的是当前数据在文件中的存储偏移量。其他索引技术，如hash索引，位图索引等。&lt;br /&gt;
索引相当于在数据之上又加了一层树状结构，可以迅速的读取数据。但是打破了我们前面讲的数据的追加写，这些数据都是根据索引随机写入的。在数据库上建立索引的时候都会遇到这个问题，在传统的机械式磁盘上，这个问题会造成千倍的性能差异。  &lt;br /&gt;
有三种方法可以解决上述问题：&lt;br /&gt;
1）把索引放到内存中，可以随机存储和读取，把数据顺序存储到硬盘上。MongoDB，Cassandra都是采取这种方式。这种方式有一个弊端是存储的数据量受限于内存的大小，数据量一大，索引也增大，数据就饱和了。   &lt;/p&gt;

&lt;p&gt;2）第二种方式是把大的索引结构，拆成很多小的索引来存储。在内存中批量进来的数据，当积累到一个预定的量，就排序然后顺序写到磁盘上，本身就是一个小的索引，数据存储完，最后加一块小的全局索引数据即可。这样读取数据的时候，要遍历一些小的索引，会有随机读取。本质是用部分小的随机读换取了整体的数据顺序存储。我们通过在内存中保存一个元索引或者Bloom filter来实现处理那些小索引的低延迟。&lt;br /&gt;
日志结构的归并树（log structed merge tree）是一种典型的实现，
	有三个特征：&lt;br /&gt;
	a)一组小的、不变的索引集。 &lt;br /&gt;
	b)只能追加写 ，合并重复的文件。 &lt;br /&gt;
	c)少量的内存索引消耗换来读取的性能提升。这是一种写优化索引结构。HBase、Cassandra、Bigtable都是通过这种比较小的内存开销来实现读取和存储的平衡。  &lt;/p&gt;

&lt;p&gt;3）列式存储或者面向列的存储。纯列式存储和谷歌bigtable那种列式存储还是有所不同的，，虽然占用了同一个名字。列式存储很好理解，就是把数据按照列顺序存储到文件中，读取的时候只读需要的列。列式存储需要保持每一列数据都有相同的顺序，即行N在每一列都有相同的偏移。这很重要，因为同一查询中可能要返回多个列的数据，同时可能我们要对多列直接进行连接。每一列保持同样的顺序我们可以用非常简单的循环实现上述操作，且都是高效的CPU和缓存操作。&lt;br /&gt;
列式存储的缺点是更新数据的时候需要更新每一个列文件中的相应数据，一个常用的方法就是类似LSM那种批量内存写的方式。当查询只是返回某几列数据，列式存储可以大规模减少磁盘IO。除此之外，列式存储的数据往往属于同一类型，可以进行高效的压缩，一些低延迟，高压缩率的扫描宽度、位填充算法都试用。即使对于未压缩的数据流，同时可以进行针对其编码格式的预取。 &lt;br /&gt;
列式存储尤其适用于大表扫描，求均值、最大最小值、分组等聚合查询场景。列式存储天然的保持了一列中数据的顺序性，方便两列数据进行关联，而heap-file index结构关联时候，一份数据可以按顺序读取，则另一份数据就会有随机读取了。&lt;br /&gt;
典型优势总结：&lt;br /&gt;
1）列式压缩，低IO &lt;br /&gt;
2）列中每行数据保持顺序，可以按照行id进行关联合并&lt;br /&gt;
3）压缩后的数据依然可以进行预取&lt;br /&gt;
4）数据延迟序列化&lt;br /&gt;
通过heap-file结构把索引存储在内存，是很多NoSQL数据库及一些关系型数据库的首选，例如Riak，CouchBase和MongoDB，模型简单并且运行良好。 &lt;br /&gt;
要处理更大量的数据，LSM技术应用更为广泛，提供了同时满足高效存储和读取效率的基于磁盘的存取结构。HBase、Cassandra、RocksDB, LevelDB，甚至MongoDB最新版也支持这种技术。   &lt;/p&gt;

&lt;p&gt;列式存储在MPP数据库里面应用广泛，例如RedShift、Vertica及hadoop上的Parquet等。这种结构适合需要大表扫描的数据处理问题，数据聚合类操作（最大最小值）更是他的主战场。  &lt;/p&gt;

&lt;h3 id=&quot;section-1&quot;&gt;二、并行化&lt;/h3&gt;
&lt;p&gt;把数据放到分布式集群中运算，有两点最为重要：分区（partition）和副本（replication）。
分区又被称为sharding，在随机访问和暴力扫描任务下都表现不错。通过hash函数把数据分布到多个机器上，很像单机上使用的hashtable，只不过这儿每一个桶都被放到了不同的机器上。这样可以通过hash函数直接去存储数据的机器上把数据取出来，这种模式有很强的扩展性，也是唯一可以根据客户端请求数线性扩展的模式。请求会被独立分发到某一机器上单独处理。  &lt;br /&gt;
我们通过分区可以实现批量任务的并行化，例如聚合函数或者更复杂的聚类或者其他机器学习算法，我们通过广播的方式在所有机器上使任务同时执行。我们还可以运行分治策略来使得高计算的任务在一个更短的时间内解决。批处理系统处理大型的计算问题有不错的效果，但是它的并发性不好，因为执行任务的时候会非常消耗集群的资源。所以分区方式在两个极端情况非常简单：1）直接hash访问 2）广播，然后分而治之，在这两种情况之间还有中间地带，那就是在NoSQL数据库中常用的二级索引技术。&lt;/p&gt;

&lt;p&gt;二级索引是指不是构建在主键上的索引，意味着数据不会因为索引的值而进行分区。不能直接通过hash函数去路由到数据本身。我们必须把请求广播到所有节点上，这样会限制了并发性，每一个请求都会卷入所有的节点。因此好多基于key-value的数据库拒绝引入二级索引，虽然它很有价值，例如Hbase和Voldemort。也有些数据库系统包含它了，因为它有用，例如Cassandra、MongoDB、Riak等。重要的是我们要理解好他的效益及他对并发性所造成的影响。        &lt;br /&gt;
解决上述并发性瓶颈的一个途径是数据副本，例如异步从数据库和Cassandra、MongoDB中的数据副本。实际上副本数据可以是透明的（只是数据恢复时候使用）、只读的（增加读的并发性），可读写的（增加分区容错性）。这些选择都会对系统的一致性造成影响。 &lt;/p&gt;

&lt;p&gt;这些对一致性的折中，给我们带来一个值得思考的问题？一致性到底有什么用？实现一致性的代价非常昂贵。在数据库中是用串行化来保证ACID的。他的基本保证是所有操作都是顺序排列的。这样实现起来的代价非常昂贵，所以好多关系型数据库也不把他当成默认选项。所以说要想在包含分布式写操作的系统上实现强一致性，如同坠入深渊。
解决一致性问题的方案也很简单，避免他。假如不能避免它把他隔离到尽可能少的写入和尽可能少的机器上。&lt;br /&gt;
当然有些业务场景是必须要保证数据一致性的，例如银行转账时候。有些业务场景感觉上是必须保持一致性的，但实际上不是，例如标记一个交易是否有潜在的欺诈，我们可以先把它更新到一个新的字段里面，另外再用一条单独的记录数据去关联最开始的那个交易。所以对一个数据平台来说有效的方式是去避免或者孤立需要一致性的请求，一种孤立的方法是采取单一写入者的策略，Datamic就是典型的例子。另一种物理隔离的方法就是去区分请求中可变和不可变的字段，分别查询。Bloom/CALM把这种理念走的更远，默认的配置选项就是无序执行的策略，只有在必要的时候才启用顺序执行读写语句。&lt;/p&gt;
</description>
        <pubDate>Sun, 20 Sep 2015 00:00:00 +0800</pubDate>
        <link>index.html/blog/2015/09/20/data-store-platform.html</link>
        <guid isPermaLink="true">index.html/blog/2015/09/20/data-store-platform.html</guid>
        
        <category>nosql</category>
        
      </item>
    
      <item>
        <title>基于Hadoop生态的企业数据中心考虑</title>
        <description>&lt;p&gt;一．面临的问题&lt;br /&gt;
如今的大数据已经炒得妇孺皆知但实际上真正在企业实践中能够发挥其优点并从中获益的很少。大数据的挑战与机遇并存，大数据的发展将从预期膨胀、炒作阶段转入将逐渐理性发展、落地应用阶段，不可否认大数据是必然趋势，未来的大数据发展依然存在诸多挑战。 &lt;br /&gt;
目前大数据的发展依然存在诸多挑战，如：&lt;br /&gt;
1.各业务部门没有清晰的大数据需求导致数据资产流失；&lt;br /&gt;
2.内部数据孤岛严重，数据价值不能充分挖掘；&lt;br /&gt;
3.数据可用性低，质量差，利用效率低；&lt;br /&gt;
4.数据管理技术和架构无法满足大数据要求；&lt;br /&gt;
5.没有明确的大数据构想；&lt;br /&gt;
大数据现阶段仍在起步阶段，存在诸多挑战，但未来的发展依然非常乐观。大数据的发展呈现的趋势也逐渐清晰：&lt;br /&gt;
1.数据资源化，将成为企业主要资产；&lt;br /&gt;
2.将全面落地于企业业务各个方面；&lt;br /&gt;
3.与传统数据分析融合，定制分析方案；&lt;br /&gt;
4.数据安全的重要性更加明显；&lt;br /&gt;
二. 为何选择Hadoop生态&lt;br /&gt;
1.成本低;&lt;br /&gt;
2.生态圈成熟,社区活跃;&lt;br /&gt;
3.能解决大多数大数据需求。  &lt;/p&gt;

&lt;p&gt;三.实施步骤&lt;br /&gt;
1.讨论确定数据分析的应用方向&lt;br /&gt;
A.数据支持的业务优化(如：根据用户行为热点调整用户界面)&lt;br /&gt;
B.独立数据分析应用(如：投资组合推荐TOP100)&lt;br /&gt;
C.大数据精准运营(如：用户流失预警模型)&lt;br /&gt;
D.决策支持(如：用户来源细分)&lt;br /&gt;
E.。。。&lt;br /&gt;
2.统计各产品现有统计需求&lt;br /&gt;
明确当前公司的数据分析环境。。。&lt;br /&gt;
3.召集各部门产品负责人讨论，协助明确大数据需求&lt;br /&gt;
明确大数据能做什么不能做什么&lt;br /&gt;
4.确定分析数据源&lt;br /&gt;
A.业务数据&lt;br /&gt;
B.SDK收集数据&lt;br /&gt;
C.日志文件&lt;br /&gt;
D.网络爬虫&lt;br /&gt;
5.选择确定技术方案&lt;br /&gt;
以Hadoop为基础，利用Hive/HBase/Sqoop/Flume/MR/Mahout/Spark/Storm/MQ/Mysql/
技术整体实现&lt;br /&gt;
6.设计实施技术架构&lt;br /&gt;
A.数据收集（ETL）&lt;br /&gt;
以Sqoop/MR 从业务数据库拉取数据到数据中心(定期)&lt;br /&gt;
SDK收集数据直接推送至数据消息队列随后持久化到数据中心(统一开发各平台数据收集SDK，保证收集数据一致性)&lt;br /&gt;
爬虫数据保存至数据中心()&lt;br /&gt;
各种数据融合消除数据孤岛()&lt;br /&gt;
B.数据中心设计&lt;br /&gt;
数据中心以HDFS为基础&lt;br /&gt;
Hive构建历史数据仓库&lt;br /&gt;
	Hive数据存储设计：&lt;br /&gt;
	数据只添加不更新/删除，数据以大表的形式存储？ Hive数据仓库设计？&lt;br /&gt;
HBase存储近期数据满足准实时分析需求&lt;br /&gt;
分析数据以MySQL形式形成满足不同业务部需求的数据集市&lt;br /&gt;
C.数据分析处理&lt;br /&gt;
	以自定义Map/Reduce作为主要数据处理工具，对数据仓库数据进行分析处理，结果推送至MySQL，形成数据集市。&lt;br /&gt;
	以Spark/Storm 流式计算处理近实时分析需求&lt;br /&gt;
	以Mahout作为主要数据挖掘分析工具，对数据进行进一步的分析利用&lt;br /&gt;
D.结果数据存储(数据集市)&lt;br /&gt;
	    以MySQL为数据库构建数据集市，面向不同业务方向提供不同主题的数据。各集市数据规模控制在千万级以内&lt;br /&gt;
E.数据深度处理 &lt;br /&gt;
		基于Mahout/?实现数据的深度挖掘。&lt;br /&gt;
F.。。。&lt;br /&gt;
7.开发具体分析 &lt;br /&gt;
整体架构：&lt;br /&gt;
&lt;img src=&quot;/image/shujupingtaisheji.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;大数据的能力可以分为四层： &lt;br /&gt;
第一层是骨骼，硬件和存储的计算能力.&lt;br /&gt;
第二层是血液，数据建模和管理能力，包括数据如何进行采集、如何进行建模，以及如何最快的计算和最高效的存储等等; &lt;br /&gt;
第三层是思想，业务理解和算法能力，很多算法工程师、科学家用大数据的时候可以作出很多关于用户的标签画像，其实这一部分正是他们对于业务的理解和算法能力的体现，但是仅仅这样还不够，大数据应用过 程中需要一个系统、体系完整的解决方案。 &lt;br /&gt;
第四层是大脑，告诉你今天的数据表现是什么样，问题是什么，未来你可以做一些什么，做完之后的效果会是怎么样，因为这个效果可以 做什么样的改善，这本身就是一个非常复杂完整的体系，产品设计和服务能力，大脑进行产品的设计以及很好的服务能力。　&lt;br /&gt;
这四者结合起来，分别表示着大数据的基础建设能力和数据产品能力。　　&lt;br /&gt;
大数据能力背后意味着非常巨大的挑战，大数据挑战本身也是一个非常完整的闭环。　　   &lt;/p&gt;

&lt;p&gt;基础建设方面，包括如何进行很好的数据采 集，如果你的数据只有几百个T或1PB左右，采集起来不那么难，如果面临几百个PB的数据如何采集，在无线时代已经到来的时候，无线的采集应该如何做?采 集之后非常重要的是计算，如果把数据采集、计算都做得很好，可是上面的应用根本没有很好的方法，快速和有效拿到这些数据来使用，其实前面所有的工作都是白费的。　　   &lt;/p&gt;

&lt;p&gt;关于采集、计算、服务是大数据基础建设方面的挑战，所有这些工作最终都要在上面浮现出来，就是我们所理解的数据产品，我们希望数据产品给到大家真的不只是一个报表，而是希望我们真的用数据产品去理解 商业，理解它的目标，并且把数据不是枯燥的以一个表格的方式呈现给用户，而是告诉你这个数据的含义是什么，如何解读它，在这个过程中去追求数据价值的最大化。关于基础建设和数据产品价值两大块挑战的关键词就是质量、效率和价值。　　   &lt;/p&gt;

&lt;p&gt;关于大数据的能力看起来非常具有想象空间，挑战也是巨大的，这是非常复杂的事情，复杂的事情一定要用复杂的方法去解决，所以我们需要有简单、清晰、明确的顶层设计。过去两年，阿里巴巴大数据实战过程 中的一个顶层设计，下面我们做好数据的基础建设，包括数据采集计算以及服务，上面我们面临的客户数据方面首先是阿里自己的小二，要有能力用好数据做数据化营运，第二块是我们非常紧密的伙伴，即商家。我们在做这件事情顶层设计时，同步考虑阿里的小二和商家如何同时用好数据。数据基础建设方面做好one data和one service，数据上面一块做好对内和对外能够共享的one Platform。解决的是关于大数据本身背后的挑战，基础建设和价值两块的挑战。    &lt;/p&gt;

</description>
        <pubDate>Sun, 20 Sep 2015 00:00:00 +0800</pubDate>
        <link>index.html/blog/2015/09/20/dashujupingtaisheji.html</link>
        <guid isPermaLink="true">index.html/blog/2015/09/20/dashujupingtaisheji.html</guid>
        
        <category>hadoop</category>
        
      </item>
    
      <item>
        <title>C4.5+聚类+SVM+关联+EM+PageRank+迭代+kNN+NB+分类笔记</title>
        <description>&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;C4.5算法&lt;br /&gt;
C4.5是做什么  &lt;br /&gt;
C4.5 以决策树的形式构建了一个分类器。&lt;br /&gt;
什么是分类器  &lt;br /&gt;
分类器是进行数据挖掘的一个工具，它处理大量需要进行分类的数据，并尝试预测新数据所属的类别。
ru:假定一个包含很多病人信息的数据集。我们知道每个病人的各种信息，比如年龄、脉搏、血压、最大摄氧量、家族病史等。这些叫做数据属性。&lt;br /&gt;
现在：  &lt;br /&gt;
给定这些属性，我们想预测下病人是否会患癌症。病人可能会进入下面两个分类：会患癌症或者不会患癌症。算法负责告诉我们每个病人的分类。&lt;br /&gt;
做法：&lt;br /&gt;
用一个病人的数据属性集和对应病人的反馈类型，C4.5构建了一个基于新病人属性预测他们类型的决策树。&lt;br /&gt;
什么是决策树呢？  &lt;br /&gt;
决策树学习是创建一种类似与流程图的东西对新数据进行分类。  &lt;br /&gt;
基本原则：&lt;br /&gt;
流程图的每个环节都是一个关于属性值的问题，并根据这些数值，病人就被分类了。你可以找到很多决策树的例子。&lt;br /&gt;
这属于监督学习算法，因为训练数据是已经分好类的。使用分好类的病人数据，C4.5算法不需要自己学习病人是否会患癌症。  &lt;br /&gt;
C4.5算法和决策树系统区别  &lt;br /&gt;
首先，C4.5算法在生成信息树的时候使用了信息增益。  &lt;br /&gt;
其次，尽管其他系统也包含剪枝，C4.5使用了一个单向的剪枝过程来缓解过渡拟合。剪枝给结果带来了很多改进。 再次，C4.5算法既可以处理连续数据也可以处理离散数据。最后，不完全的数据用算法自有的方式进行了处理。  &lt;br /&gt;
决策树最好的卖点是他们方便于翻译和解释。他们速度也很快，是种比较流行的算法。输出的结果简单易懂。  &lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;k 均值聚类算法&lt;br /&gt;
K-聚类算法从一个目标集中创建多个组，每个组的成员都是比较相似的。这是个想要探索一个数据集时比较流行的聚类分析技术。&lt;br /&gt;
什么是聚类分析&lt;br /&gt;
聚类分析属于设计构建组群的算法，这里的组成员相对于非组成员有更多的相似性。在聚类分析的世界里，类和组是相同的意思。&lt;br /&gt;
如：假设我们定义一个病人的数据集。在聚类分析里，这些病人可以叫做观察对象。我们知道每个病人的各类信息，比如年龄、血压、血型、最大含氧量和胆固醇含量等。这是一个表达病人特性的向量。&lt;br /&gt;
基本认为一个向量代表了我们所知道的病人情况的一列数据。这列数据也可以理解为多维空间的坐标。脉搏是一维坐标，血型是其他维度的坐标等等。  &lt;br /&gt;
Kmeans算法更深层次的这样处理问题： &lt;br /&gt;
1.k-means 算法在多维空间中挑选一些点代表每一个 k 类。他们叫做中心点。&lt;br /&gt;
2.每个病人会在这 k 个中心点中找到离自己最近的一个。我们希望病人最靠近的点不要是同一个中心点，所以他们在靠近他们最近的中心点周围形成一个类。&lt;br /&gt;
3.我们现在有 k 个类，并且现在每个病人都是一个类中的一员。 &lt;br /&gt;
4.之后k-means 算法根据它的类成员找到每个 k 聚类的中心&lt;br /&gt;
5.这个中心成为类新的中心点。&lt;br /&gt;
6.因为现在中心点在不同的位置上了，病人可能现在靠近了其他的中心点。换句话说，他们可能会修改自己的类成员身份。  &lt;br /&gt;
7.重复2-6步直到中心点不再改变，这样类成员也就稳定了。这也叫做收敛性。  &lt;br /&gt;
k-means 可以认为是非监督学习的类型。并不是指定分类的个数，也没有观察对象该属于那个类的任何信息   &lt;br /&gt;
k-means 关键卖点是它的简单。它的简易型意味着它通常要比其他的算法更快更有效，尤其是要大量数据集的情况下更是如此。 &lt;br /&gt;
改进：&lt;br /&gt;
k-means 可以对已经大量数据集进行预先聚类处理，然后在针对每个子类做成本更高点的聚类分析。k-means 也能用来快速的处理“K”和探索数据集中是否有被忽视的模式或关系。  &lt;br /&gt;
k means算法的两个关键弱点分别是它对异常值的敏感性和它对初始中心点选择的敏感性。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;支持向量机&lt;br /&gt;
支持向量机（SVM）获取一个超平面将数据分成两类。除了不会使用决策树以外，SVM与 C4.5算法是执行相似的任务的。  &lt;br /&gt;
超平面（hyperplane）是个函数，类似于解析一条线的方程。实际上，对于只有两个属性的简单分类任务来说，超平面可以是一条线的。 &lt;br /&gt;
其实事实证明：
SVM 可以使用一个小技巧，把你的数据提升到更高的维度去处理。一旦提升到更高的维度中，SVM算法会计算出把你的数据分离成两类的最好的超平面。  &lt;br /&gt;
简单的例子。我发现桌子上开始就有一堆红球和蓝球，如果这这些球没有过分的混合在一起，不用移动这些球，你可以拿一根棍子把它们分离开。
当在桌上加一个新球时，通过已经知道的棍字的哪一边是哪个颜色的球，你就可以预测这个新球的颜色了。 &lt;br /&gt;
SVM 算法可以算出这个超平面的方程。
如果事情变得更复杂该怎么办？当然了，事情通常都很复杂。如果球是混合在一起的，一根直棍就不能解决问题了。 &lt;br /&gt;
通过使用核函数（kernel），我们在高维空间也有很棒的操作方法。这张大纸依然叫做超平面，但是现在它对应的方程是描述一个平面而不是一条线了。&lt;br /&gt;
那么在桌上或者空中的球怎么用现实的数据解释呢？桌上的每个球都有自己的位置，我们可以用坐标来表示。打个比方，一个球可能是距离桌子左边缘20cm 距离底部边缘 50 cm，另一种描述这个球的方式是使用坐标(x,y)或者(20,50)表达。x和 y 是代表球的两个维度。&lt;br /&gt;
可以这样理解：如果我们有个病人的数据集，每个病人可以用很多指标来描述，比如脉搏，胆固醇水平，血压等。每个指标都代表一个维度。&lt;br /&gt;
基本上，SVM 把数据映射到一个更高维的空间然后找到一个能分类的超平面。 &lt;br /&gt;
类间间隔(margin)经常会和 SVM 联系起来，类间间隔是什么呢？它是超平面和各自类中离超平面最近的数据点间的距离。在球和桌面的例子中，棍子和最近的红球和蓝球间的距离就是类间间隔(margin)。 &lt;br /&gt;
SVM 的关键在于，它试图最大化这个类间间隔，使分类的超平面远离红球和蓝球。这样就能降低误分类的可能性。 &lt;br /&gt;
SVM 属于监督学习。因为开始需要使用一个数据集让 SVM学习这些数据中的类型。只有这样之后 SVM 才有能力对新数据进行分类。  &lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Apriori 关联算法&lt;br /&gt;
Apriori算法学习数据的关联规则(association rules)，适用于包含大量事务（transcation）的数据库。&lt;br /&gt;
什么是关联规则&lt;br /&gt;
关联规则学习是学习数据库中不同变量中的相互关系的一种数据挖掘技术。
举个 Apriori 算法的例子：我们假设有一个充满超市交易数据的数据库，你可以把数据库想象成一个巨大的电子数据表，表里每一行是一个顾客的交易情况，每一列代表不用的货物项。&lt;br /&gt;
通过使用 Apriori 算法，我们就知道了同时被购买的货物项，这也叫做关联规则。它的强大之处在于，你能发现相比较其他货物来说，有一些货物更频繁的被同时购买—终极目的是让购物者买更多的东西。这些常被一起购买的货物项被称为项集（itemset）。 &lt;br /&gt;
如：“薯条+蘸酱”和“薯条+苏打水”的组合频繁的一起出现。这些组合被称为2-itemsets。在一个足够大的数据集中，就会很难“看到”这些关系了，尤其当还要处理3-itemset 或者更多项集的时候。这正是 Apriori 可以帮忙的地方！&lt;br /&gt;
基本的 Apriori 算法有三步：
1.参与，扫描一遍整个数据库，计算1-itemsets 出现的频率。&lt;br /&gt;
2.剪枝，满足支持度和可信度的这些1-itemsets移动到下一轮流程，再寻找出现的2-itemsets。&lt;br /&gt;
3.重复，对于每种水平的项集 一直重复计算，知道我们之前定义的项集大小为止。 &lt;br /&gt;
Apriori 一般被认为是一种非监督的学习方法，因为它经常用来挖掘和发现有趣的模式和关系。  &lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;EM 最大期望算法 Expectation Maximization&lt;br /&gt;
在数据挖掘领域，最大期望算法（Expectation-Maximization,EM） 一般作为聚类算法（类似 kmeans 算法）用来知识挖掘。&lt;br /&gt;
在统计学上，当估运算带有无法观测隐藏变量的统计模型参数时,EM 算法不断迭代和优化可以观测数据的似然估计值。 &lt;br /&gt;
统计模型？我把模型看做是描述观测数据是如何生成的。例如，一场考试的分数可能符合一种钟形曲线，因此这种分数分布符合钟形曲线（也称正态分布）的假设就是模型。&lt;br /&gt;
什么是分布？分布代表了对所有可测量结果的可能性。例如，一场考试的分数可能符合一个正态分布。这个正态分布代表了分数的所有可能性。换句话说，给定一个分数，你可以用这个分布来预计多少考试参与者可能会得到这个分数。&lt;br /&gt;
那么，你描述正态分布需要的所有东西就是这两个参数：&lt;br /&gt;
1.平均值&lt;br /&gt;
2.方差 &lt;br /&gt;
什么是似然性：&lt;br /&gt;
回到我们之前的钟形曲线例子，假设我们已经拿到很多的分数数据，并被告知分数符合一个钟形曲线。然而，我们并没有给到所有的分数，只是拿到了一个样本。
可以这样做：&lt;br /&gt;
我们不知道所有分数的平均值或者方差，但是我们可以使用样本计算它们。似然性就是用估计的方差和平均值得到的钟形曲线在算出很多分数的概率。&lt;br /&gt;
换句话说，给定一系列可测定的结果，让我们来估算参数。再使用这些估算出的参数，得到结果的这个假设概率就被称为似然性。&lt;br /&gt;
算法的优势是：对于数据挖掘和聚类，观察到遗失的数据的这类数据点对我们来说很重要。我们不知道具体的类，因此这样处理丢失数据对使用 EM 算法做聚类的任务来说是很关键的。
算法的精髓在于：&lt;br /&gt;
通过优化似然性，EM 生成了一个很棒的模型，这个模型可以对数据点指定类型标签—听起来像是聚类算法！ &lt;br /&gt;
EM 算法是怎么帮助实现聚类的呢？EM 算法以对模型参数的猜测开始。然后接下来它会进行一个循环的3步：   &lt;br /&gt;
1.E 过程：基于模型参数，它会针对每个数据点计算对聚类的分配概率。&lt;br /&gt;
2.M 过程：基于 E 过程的聚类分配，更新模型参数。&lt;br /&gt;
3.重复知道模型参数和聚类分配工作稳定（也可以称为收敛）。 &lt;br /&gt;
EM 是非监督学习算法。
EM 算法的一个关键卖点就是它的实现直接。它不但可以优化模型参数，还可以反复的对丢失数据进行猜测。  &lt;br /&gt;
这使算法在聚类和产生带参数的模型上都表现出色。在得知聚类情况和模型参数的情况下，我们有可能解释清楚有相同属性的分类情况和新数据属于哪个类之中。 &lt;br /&gt;
EM算法弱点… &lt;br /&gt;
第一，EM 算法在早期迭代中都运行速度很快，但是越后面的迭代速度越慢。&lt;br /&gt;
第二，EM 算法并不能总是寻到最优参数，很容易陷入局部最优而不是找到全局最优解。   &lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;PageRank算法
PageRank是为了决定一些对象和同网络中的其他对象之间的相对重要程度而设计的连接分析算法(link analysis algorithm)。   &lt;br /&gt;
什么是连接分析算法？&lt;br /&gt;
它是一类针对网络的分析算法，探寻对象间的关系（也可成为连接）。&lt;br /&gt;
举个例子：最流行的 PageRank 算法是 Google 的搜索引擎。尽管他们的搜索引擎不止是依靠它，但  PageRank依然是 Google 用来测算网页重要度的手段之一。 &lt;br /&gt;
万维网上的网页都是互相链接的。如果 Rayli.net 链接到了 CNN 上的一个网页，CNN 网页就增加一个投票，表示 rayli.net 和 CNN 网页是关联的。 &lt;br /&gt;
反过来，来自rayli.net 网页的投票重要性也要根据 rayli.net 网的重要性和关联性来权衡。换句话说，任何给 rayli.net 投票的网页也能提升 rayli.net 网页的关联性。 &lt;br /&gt;
概括一下： &lt;br /&gt;
投票和关联性就是 PageRank 的概念。rayli.net 给CNN 投票增加了 CNN 的 Pagerank，rayli.net 的 PageRank级别同时也影响着它为 CNN 投票多大程度影响了CNN 的 PageRank。&lt;br /&gt;
这排名有点像一个网页流行度的竞争。我们的头脑中都有了一些这些网站的流行度和关联度的信息。
PageRank只是一个特别讲究的方式来定义了这些而已。 &lt;br /&gt;
PageRank还有什么其他应用呢？ PageRank是专门为了万维网设计的。 &lt;br /&gt;
可以考虑一下，以核心功能的角度看，PageRank算法真的只是一个处理链接分析极度有效率的方法。处理的被链接的对象不止只是针对网页。  &lt;br /&gt;
给出PageRank 的三个实现：&lt;br /&gt;
1 C++ OpenSource PageRank Implementation&lt;br /&gt;
2 Python PageRank Implementation&lt;br /&gt;
3 igraph – The network analysis package (R)  &lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;AdaBoost 迭代算法
AdaBoost 是个构建分类器的提升算法。 &lt;br /&gt;
分类器拿走大量数据，并试图预测或者分类新数据元素的属于的类别。
提升(boost) 指的什么？提升是个处理多个学习算法（比如决策树）并将他们合并联合起来的综合的学习算法。目的是将弱学习算法综合或形成一个组，把他们联合起来创造一个新的强学习器。 &lt;br /&gt;
强弱学习器之间有什么区别呢？弱学习分类器的准确性仅仅比猜测高一点。一个比较流行的弱分类器的例子就是只有一层的决策树。 &lt;br /&gt;
另一个，强学习分类器有更高的准确率，一个通用的强学习器的例子就是 SVM。 &lt;br /&gt;
举个 AdaBoost 算法的例子：我们开始有3个弱学习器，我们将在一个包含病人数据的数据训练集上对他们做10轮训练。数据集里包含了病人的医疗记录各个细节。 &lt;br /&gt;
问题来了，那我们怎么预测某个病人是否会得癌症呢？AdaBoost 是这样给出答案的： &lt;br /&gt;
第一轮，AdaBoost 拿走一些训练数据，然后测试每个学习器的准确率。最后的结果就是我们找到最好的那个学习器。另外，误分类的样本学习器给予一个比较高的权重，这样他们在下轮就有很高的概率被选中了。  &lt;br /&gt;
再补充一下，最好的那个学习器也要给根据它的准确率赋予一个权重，并将它加入到联合学习器中（这样现在就只有一个分类器了） &lt;br /&gt;
第二轮， AdaBoost 再次试图寻找最好的学习器。 &lt;br /&gt;
关键部分来了，病人数据样本的训练数据现在被有很高误分配率的权重影响着。换句话说，之前误分类的病人在这个样本里有很高的出现概率。 &lt;br /&gt;
为什么？
这就像是在电子游戏中已经打到了第二级，但当你的角色死亡后却不必从头开始。而是你从第二级开始然后集中注意，尽力升到第三级。 &lt;br /&gt;
同样地，第一个学习者有可能对一些病人的分类是正确的，与其再度试图对他们分类，不如集中注意尽力处理被误分类的病人。   &lt;br /&gt;
最好的学习器也被再次赋予权重并加入到联合分类器中，误分类的病人也被赋予权重，这样他们就有比较大的可能性再次被选中，我们会进行过滤和重复。 &lt;br /&gt;
在10轮结束的时候，我们剩下了一个带着不同权重的已经训练过的联合学习分类器，之后重复训练之前回合中被误分类的数据。&lt;br /&gt;
算法灵活通用，AdaBoost 可以加入任何学习算法，并且它能处理多种数据。&lt;br /&gt;
AdaBoost 有很多程序实现和变体。给出一些： &lt;br /&gt;
▪ scikit-learn&lt;br /&gt;
▪ ICSIBoost&lt;br /&gt;
▪ gbm: Generalized Boosted Regression Models   &lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;kNN：k最近邻算法&lt;br /&gt;
kNN，或 K 最近邻(k-Nearest Neighbors), 诗歌分类算法。然而，它和我们之前描述的分类器不同，因为它是个懒散学习法。&lt;br /&gt;
懒散学习法？  和存储训练数据的算法不同，懒散学习法在训练过程中不需要做许多处理。只有当新的未被分类的数据输入时，这类算法才会去做分类。 &lt;br /&gt;
但在另一方面，积极学习法则会在训练中建立一个分类模型，当新的未分类数据输入时，这类学习器会把新数据也提供给这个分类模型。 &lt;br /&gt;
那么 C4.5，SVM 和 AdaBoost 属于哪类呢？不像 kNN算法，他们都是积极学习算法。 &lt;br /&gt;
给出原因：&lt;br /&gt;
1 C4.5 在训练中建立了一个决策分类树模型。&lt;br /&gt;
2 SVM在训练中建立了一个超平面的分类模型。&lt;br /&gt;
3 AdaBoost在训练中建立了一个联合的分类模型。&lt;br /&gt;
那么 kNN 做了什么？ kNN 没有建立这样的分类模型，相反，它只是储存了一些分类好的训练数据。那么新的训练数据进入时，kNN 执行两个基本步骤：&lt;br /&gt;
1 首先，它观察最近的已经分类的训练数据点—也就是，k最临近点（k-nearest neighbors）&lt;br /&gt;
2 第二部，kNN使用新数据最近的邻近点的分类， 就对新数据分类得到了更好的结果了。 &lt;br /&gt;
你可能会怀疑…kNN 是怎么计算出最近的是什么？ 对于连续数据来说，kNN 使用一个像欧氏距离的距离测度，距离测度的选择大多取决于数据类型。有的甚至会根据训练数据学习出一种距离测度。关于 kNN 距离测度有更多的细节讨论和论文描述。 &lt;br /&gt;
对于离散数据，解决方法是可以把离散数据转化为连续数据。给出两个例子： &lt;br /&gt;
1 使用汉明距离（Hamming distance ）作为两个字符串紧密程度的测度。 &lt;br /&gt;
2 把离散数据转化为二进制表征。 &lt;br /&gt;
当临近的点是不同的类，kNN 怎么给新数据分类呢？当临近点都是同一类的时候，kNN 也就不费力气了。我们用直觉考虑，如果附近点都一致，那么新数据点就很可能落入这同一个类中了。 &lt;br /&gt;
当临近点不是同一类时，kNN 怎么决定分类情况的呢？&lt;br /&gt;
处理这种情况通常有两种办法：&lt;br /&gt;
1 通过这些临近点做个简单的多数投票法。哪个类有更多的票，新数据就属于那个类。&lt;br /&gt;
2 还是做个类似的投票，但是不同的是，要给那些离的更近的临近点更多的投票权重。这样做的一个简单方法是使用反距离(reciprocal distance). 比如，如果某个临近点距离5个单位，那么它的投票权重就是1/5.当临近点越来越远是，倒数距离就越来越小…这正是我们想要的。 &lt;br /&gt;
 kNN 算法提供了已经被分类好的数据集，所以它是个监督学习算法。 &lt;br /&gt;
我们需要注意的5点： &lt;br /&gt;
1 当试图在一个大数据集上计算最临近点时，kNN 算法可能会耗费高昂的计算成本。&lt;br /&gt;
2 噪声数据(Noisy data)可能会影响到 kNN 的分类。 &lt;br /&gt;
3 选择大范围的属性筛选(feature)会比小范围的筛选占有很多优势，所以属性筛选(feature)的规模非常重要。 &lt;br /&gt;
4 由于数据处理会出现延迟，kNN 相比积极分类器，一般需要更强大的存储需求。 &lt;br /&gt;
5 选择一个合适的距离测度对 kNN 的准确性来说至关重要。 &lt;br /&gt;
哪里用过这个方法？有很多现存的 kNN 实现手段：&lt;br /&gt;
▪ MATLAB k-nearest neighbor classification&lt;br /&gt;
▪ scikit-learn KNeighborsClassifier&lt;br /&gt;
▪ k-Nearest Neighbour Classification in R   &lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Naive Bayes 朴素贝叶斯算法 &lt;br /&gt;
朴素贝叶斯（Naive Bayes）并不只是一个算法，而是一系列分类算法，这些算法以一个共同的假设为前提：  &lt;br /&gt;
被分类的数据的每个属性与在这个类中它其他的属性是独立的。 &lt;br /&gt;
独立是什么意思呢？当一个属性值对另一个属性值不产生任何影响时，就称这两个属性是独立的。
举个例子：
比如说你有一个病人的数据集，包含了病人的脉搏，胆固醇水平，体重，身高和邮编这样的属性。如果这些属性值互相不产生影响，那么所有属性都是独立的。对于这个数据集来说，假定病人的身高和邮编相互独立，这是合理的。因为病人的身高和他们的邮编没有任何关系。但是我们不能停在这，其他的属性间是独立的么？ &lt;br /&gt;
很遗憾，答案是否定的。给出三个并不独立的属性关系： &lt;br /&gt;
▪ 如果身高增加，体重可能会增加。 &lt;br /&gt;
▪ 如果胆固醇水平增加，体重可能增加。&lt;br /&gt;
▪ 如果胆固醇水平增加，脉搏也可能会增加。&lt;br /&gt;
以我的经验来看，数据集的属性一般都不是独立的。&lt;br /&gt;
这样就和下面的问题联系起来了…&lt;br /&gt;
为什么要把算法称为朴素的(naive)呢？数据集中所有属性都是独立的这个假设正是我们称为朴素（naive）的原因—— 通常下例子中的所有属性并不是独立的。  &lt;br /&gt;
我们在深入研究一下..
这个等式是什么意思？在属性1和属性2的条件下，等式计算出了A 类的概率。换句话说，如果算出属性1 和2，等式算出的数据属于 A 类的概率大小。
等式这样写解释为：在属性1和属性2条件下，分类 A 的概率是一个分数。
▪ 分数的分子是在分类 A条件下属性1的概率，乘以在分类 A 条件下属性2的概率，再乘以分类 A 的概率
▪ 分数的分母是属性1的概率乘以属性2的概率。
Naive Bayes 的实现可以从Orange, scikit-learn, Weka和 R 里面找到。
最后，看一下第十种算法吧。  &lt;/li&gt;
  &lt;li&gt;CART 分类算法&lt;br /&gt;
CART 代表分类和回归树(classification and regression trees)。它是个决策树学习方法，同时输出分类和回归树。 像 C4.5一样，CART 是个分类器。 &lt;br /&gt;
分类树像决策树一样么？分类树是决策树的一种。分类树的输出是一个类。 &lt;br /&gt;
举个例子，根据一个病人的数据集、你可能会试图预测下病人是否会得癌症。这个分类或者是“会的癌症”或者是“不会得癌症”。 &lt;br /&gt;
那回归树是什么呢？和分类树预测分类不同，回归树预测一个数字或者连续数值，比如一个病人的住院时间或者一部智能手机的价格。&lt;br /&gt;
这么记比较简单：&lt;br /&gt;
分类树输出类、回归树输出数字。 &lt;br /&gt;
为了构造分类和回归树模型，需要给它提供被分类好的训练数据集，因此 CART 是个监督学习算法。
为什么要使用 CART 呢？使用 C4.5的原因大部分也适用于 CART，因为它们都是决策树学习的方法。便于说明和解释这类的原因也适用于 CART。
和 C4.5一样，它们的计算速度都很快，算法也都比较通用流行，并且输出结果也具有可读性。
scikit-learn 在他们的决策树分类器部分实现了 CART 算法；R 语言的 tree package 也有 CART 的实现；Weka 和 MATLAB 也有CART的实现过程。&lt;/li&gt;
&lt;/ol&gt;

</description>
        <pubDate>Sun, 20 Sep 2015 00:00:00 +0800</pubDate>
        <link>index.html/blog/2015/09/20/10-suanfa-biji.html</link>
        <guid isPermaLink="true">index.html/blog/2015/09/20/10-suanfa-biji.html</guid>
        
        <category>hadoop</category>
        
      </item>
    
      <item>
        <title>sqoop 特殊字符导入问题</title>
        <description>&lt;p&gt;Sqoop从MySQL导入数据到hive，示例：&lt;br /&gt;
sqoop import –connect  jdbc:mysql://10.255.2.89:3306/test?charset=utf-8 –  username selectuser –password select##select##  –table test_sqoop_import \&lt;br /&gt;
–columns ‘id,content,updateTime’ \&lt;br /&gt;
–split-by id \&lt;br /&gt;
–hive-import –hive-table basic.test2;  &lt;/p&gt;

&lt;p&gt;如果不加其他参数，导入的数据默认的列分隔符是’\001’，默认的行分隔符是’\n’。&lt;br /&gt;
这样问题就来了，如果导入的数据中有’\n’，hive会认为一行已经结束，后面的数据被分割成下一行。这种情况下，导入之后hive中数据的行数就比原先数据库中的多，而且会出现数据不一致的情况。&lt;br /&gt;
Sqoop也指定了参数 –fields-terminated-by和 –lines-terminated-by来自定义行分隔符和列分隔符。&lt;br /&gt;
可是当你真的这么做时 坑爹呀&lt;br /&gt;
INFO hive.HiveImport: FAILED: SemanticException 1:381 LINES TERMINATED BY only supports newline ’\n’ right now.    
也就是说虽然你通过–lines-terminated-by指定了其他的字符作为行分隔符，但是hive只支持’\n’作为行分隔符。&lt;br /&gt;
简单的解决办法就是加上参数–hive-drop-import-delims来把导入数据中包含的hive默认的分隔符去掉。  &lt;/p&gt;

&lt;p&gt;附 sqoop 命令参考  &lt;/p&gt;

&lt;p&gt;导入数据到 hdfs&lt;br /&gt;
使用 sqoop-import 命令可以从关系数据库导入数据到 hdfs。&lt;br /&gt;
$ sqoop import –connect jdbc:mysql://192.168.56.121:3306/metastore –username hiveuser –password redhat –table TBLS –target-dir /user/hive/result
注意：&lt;br /&gt;
mysql jdbc url 请使用 ip 地址&lt;br /&gt;
如果重复执行，会提示目录已经存在，可以手动删除&lt;br /&gt;
如果不指定 –target-dir，导入到用户家目录下的 TBLS 目录&lt;br /&gt;
你还可以指定其他的参数：  &lt;/p&gt;

&lt;pre&gt;&lt;code&gt;--append	将数据追加到hdfs中已经存在的dataset中。使用该参数，sqoop将把数据先导入到一个临时目录中，然后重新给文件命名到一个正式的目录中，以避免和该目录中已存在的文件重名。  
--as-avrodatafile	将数据导入到一个Avro数据文件中  
--as-sequencefile	将数据导入到一个sequence文件中  
--as-textfile	将数据导入到一个普通文本文件中，生成该文本文件后，可以在hive中通过sql语句查询出结果。  
--boundary-query &amp;lt;statement&amp;gt;	边界查询，也就是在导入前先通过SQL查询得到一个结果集，然后导入的数据就是该结果集内的数据，格式如：--boundary-query &#39;select id,no from t where id = 3&#39;，表示导入的数据为id=3的记录，或者 select min(&amp;lt;split-by&amp;gt;), max(&amp;lt;split-by&amp;gt;) from &amp;lt;table name&amp;gt;，注意查询的字段中不能有数据类型为字符串的字段，否则会报错
--columns&amp;lt;col,col&amp;gt;	指定要导入的字段值，格式如：--columns id,username
--direct	直接导入模式，使用的是关系数据库自带的导入导出工具。官网上是说这样导入会更快
--direct-split-size	在使用上面direct直接导入的基础上，对导入的流按字节数分块，特别是使用直连模式从PostgreSQL导入数据的时候，可以将一个到达设定大小的文件分为几个独立的文件。
--inline-lob-limit	设定大对象数据类型的最大值
-m,--num-mappers	启动N个map来并行导入数据，默认是4个，最好不要将数字设置为高于集群的节点数
--query，-e &amp;lt;sql&amp;gt;	从查询结果中导入数据，该参数使用时必须指定–target-dir、–hive-table，在查询语句中一定要有where条件且在where条件中需要包含 \$CONDITIONS，示例：--query &#39;select * from t where \$CONDITIONS &#39; --target-dir /tmp/t –hive-table t
--split-by &amp;lt;column&amp;gt;	表的列名，用来切分工作单元，一般后面跟主键ID
--table &amp;lt;table-name&amp;gt;	关系数据库表名，数据从该表中获取
--delete-target-dir	删除目标目录
--target-dir &amp;lt;dir&amp;gt;	指定hdfs路径
--warehouse-dir &amp;lt;dir&amp;gt;	与 --target-dir 不能同时使用，指定数据导入的存放目录，适用于hdfs导入，不适合导入hive目录
--where	从关系数据库导入数据时的查询条件，示例：--where &quot;id = 2&quot;
-z,--compress	压缩参数，默认情况下数据是没被压缩的，通过该参数可以使用gzip压缩算法对数据进行压缩，适用于SequenceFile, text文本文件, 和Avro文件
--compression-codec	Hadoop压缩编码，默认是gzip
--null-string &amp;lt;null-string&amp;gt;	可选参数，如果没有指定，则字符串null将被使用
--null-non-string &amp;lt;null-string&amp;gt;	可选参数，如果没有指定，则字符串null将被使用
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;使用 sql 语句&lt;br /&gt;
参照上表，使用 sql 语句查询时，需要指定 $CONDITIONS  &lt;/p&gt;

&lt;p&gt;$ sqoop import –connect jdbc:mysql://192.168.56.121:3306/metastore –username hiveuser –password redhat –query ‘SELECT * from TBLS where $CONDITIONS ‘ –split-by tbl_id -m 4 –target-dir /user/hive/result&lt;br /&gt;
	上面命令通过 -m 1 控制并发的 map 数。&lt;/p&gt;

&lt;p&gt;使用 direct 模式：&lt;br /&gt;
$ sqoop import –connect jdbc:mysql://192.168.56.121:3306/metastore –username hiveuser –password redhat –table TBLS –delete-target-dir –direct –default-character-set UTF-8 –target-dir /user/hive/result&lt;/p&gt;

&lt;p&gt;指定文件输出格式：&lt;br /&gt;
$ sqoop import –connect jdbc:mysql://192.168.56.121:3306/metastore –username hiveuser –password redhat –table TBLS –fields-terminated-by “\t” –lines-terminated-by “\n” –delete-target-dir  –target-dir /user/hive/result&lt;/p&gt;

&lt;p&gt;指定空字符串：&lt;br /&gt;
$ sqoop import –connect jdbc:mysql://192.168.56.121:3306/metastore –username hiveuser –password redhat –table TBLS –fields-terminated-by “\t” –lines-terminated-by “\n” –delete-target-dir –null-string ‘\N’ –null-non-string ‘\N’ –target-dir /user/hive/result&lt;/p&gt;

&lt;p&gt;如果需要指定压缩：  &lt;br /&gt;
$ sqoop import –connect jdbc:mysql://192.168.56.121:3306/metastore –username hiveuser –password redhat –table TBLS –fields-terminated-by “\t” –lines-terminated-by “\n” –delete-target-dir –null-string ‘\N’ –null-non-string ‘\N’ –compression-codec “com.hadoop.compression.lzo.LzopCodec” –target-dir /user/hive/result  &lt;/p&gt;

&lt;p&gt;附：可选的文件参数如下表。   &lt;/p&gt;

&lt;pre&gt;&lt;code&gt;--enclosed-by &amp;lt;char&amp;gt;	给字段值前后加上指定的字符，比如双引号，示例：--enclosed-by &#39;\&quot;&#39;，显示例子：&quot;3&quot;,&quot;jimsss&quot;,&quot;dd@dd.com&quot;
--escaped-by &amp;lt;char&amp;gt;	给双引号作转义处理，如字段值为&quot;测试&quot;，经过 --escaped-by &quot;\\&quot; 处理后，在hdfs中的显示值为：\&quot;测试\&quot;，对单引号无效
--fields-terminated-by &amp;lt;char&amp;gt;	设定每个字段是以什么符号作为结束的，默认是逗号，也可以改为其它符号，如句号.，示例如：--fields-terminated-by
--lines-terminated-by &amp;lt;char&amp;gt;	设定每条记录行之间的分隔符，默认是换行串，但也可以设定自己所需要的字符串，示例如：--lines-terminated-by &quot;#&quot; 以#号分隔
--mysql-delimiters	Mysql默认的分隔符设置，字段之间以,隔开，行之间以换行\n隔开，默认转义符号是\，字段值以单引号&#39;包含起来。
--optionally-enclosed-by &amp;lt;char&amp;gt;	enclosed-by是强制给每个字段值前后都加上指定的符号，而--optionally-enclosed-by只是给带有双引号或单引号的字段值加上指定的符号，故叫可选的
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;创建 hive 表&lt;br /&gt;
生成与关系数据库表的表结构对应的HIVE表：  &lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sqoop create-hive-table --connect jdbc:mysql://192.168.56.121:3306/metastore --username hiveuser --password redhat --table TBLS 
--hive-home &amp;lt;dir&amp;gt;	Hive的安装目录，可以通过该参数覆盖掉默认的hive目录
--hive-overwrite	覆盖掉在hive表中已经存在的数据
--create-hive-table	默认是false，如果目标表已经存在了，那么创建任务会失败
--hive-table	后面接要创建的hive表
--table	指定关系数据库表名  导入数据到 hive   执行下面的命令会将 mysql 中的数据导入到 hdfs 中，然后创建一个hive 表，最后再将 hdfs 上的文件移动到 hive 表的目录下面。   $ sqoop import --connect jdbc:mysql://192.168.56.121:3306/metastore --username hiveuser --password redhat --table TBLS --fields-terminated-by &quot;\t&quot; --lines-terminated-by &quot;\n&quot; --hive-import --hive-overwrite --create-hive-table --hive-table dw_srclog.TBLS --delete-target-dir   说明：   可以在 hive 的表名前面指定数据库名称   可以通过 --create-hive-table 创建表，如果表已经存在则会执行失败   从上面可见，数据导入到 hive 中之后分隔符为默认分隔符，参考上文你可以通过设置参数指定其他的分隔符。   另外，Sqoop 默认地导入空值（NULL）为 null 字符串，而 hive 使用 \N 去标识空值（NULL），故你在 import 或者 export 时候，需要做相应的处理。在 import 时，使用如下命令：   $ sqoop import  ... --null-string &#39;\\N&#39; --null-non-string &#39;\\N&#39;   在导出时，使用下面命令：   $ sqoop import  ... --input-null-string &#39;&#39; --input-null-non-string &#39;&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;增量导入&lt;br /&gt;
–check-column (col)	用来作为判断的列名，如id&lt;br /&gt;
–incremental (mode)	append：追加，比如对大于last-value指定的值之后的记录进行追加导入。lastmodified：最后的修改时间，追加last-value指定的日期之后的记录&lt;br /&gt;
–last-value (value)	指定自从上次导入后列的最大值（大于该指定的值），也可以自己设定某一值&lt;/p&gt;

&lt;p&gt;合并 hdfs 文件&lt;br /&gt;
将HDFS中不同目录下面的数据合在一起，并存放在指定的目录中，示例如：&lt;br /&gt;
sqoop merge –new-data /test/p1/person –onto /test/p2/person –target-dir /test/merged –jar-file /opt/data/sqoop/person/Person.jar –class-name Person –merge-key id&lt;br /&gt;
其中，–class-name 所指定的 class 名是对应于 Person.jar 中的 Person 类，而 Person.jar 是通过 Codegen 生成的&lt;br /&gt;
–new-data &lt;path&gt;	Hdfs中存放数据的一个目录，该目录中的数据是希望在合并后能优先保留的，原则上一般是存放越新数据的目录就对应这个参数。  
--onto &lt;path&gt;	Hdfs中存放数据的一个目录，该目录中的数据是希望在合并后能被更新数据替换掉的，原则上一般是存放越旧数据的目录就对应这个参数。  
--merge-key &amp;lt;col&amp;gt;	合并键，一般是主键ID  
--jar-file &lt;file&gt;	合并时引入的jar包，该jar包是通过Codegen工具生成的jar包  
--class-name &lt;class&gt;	对应的表名或对象名，该class类是包含在jar包中的。  
--target-dir &lt;path&gt;	合并后的数据在HDFS里的存放目录  &lt;/path&gt;&lt;/class&gt;&lt;/file&gt;&lt;/path&gt;&lt;/path&gt;&lt;/p&gt;
</description>
        <pubDate>Sat, 16 May 2015 00:00:00 +0800</pubDate>
        <link>index.html/blog/2015/05/16/sqoop-spacil-chart.html</link>
        <guid isPermaLink="true">index.html/blog/2015/05/16/sqoop-spacil-chart.html</guid>
        
        <category>hadoop</category>
        
      </item>
    
  </channel>
</rss>
