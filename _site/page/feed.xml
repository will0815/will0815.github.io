<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>RSS - willgo</title>
    <description>willgo - 最好的从未错过</description>
    <link>index.html</link>
    <atom:link href="index.html/page/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Sat, 26 Dec 2015 17:10:44 +0800</pubDate>
    <lastBuildDate>Sat, 26 Dec 2015 17:10:44 +0800</lastBuildDate>
    <generator>Will.Quan</generator>
    
      <item>
        <title>牛逼之路</title>
        <description>&lt;转自&gt; [http://www.hollischuang.com/archives/489](http://www.hollischuang.com/archives/489)

#  一、基础篇   
##  1.1 JVM  
 
1.1.1. Java内存模型，Java内存管理，Java堆和栈，垃圾回收    

	http://www.jcp.org/en/jsr/detail?id=133
	http://ifeve.com/jmm-faq/
1.1.2. 了解JVM各种参数及调优  

1.1.3. 学习使用Java工具  
jps, jstack, jmap, jconsole, jinfo, jhat, javap, …

	http://kenai.com/projects/btrace
	http://www.crashub.org/
	https://github.com/taobao/TProfiler
	https://github.com/CSUG/HouseMD
	http://wiki.cyclopsgroup.org/jmxterm
	https://github.com/jlusdy/TBJMap
1.1.4. 学习Java诊断工具  

	http://www.eclipse.org/mat/
	http://visualvm.java.net/oqlhelp.html
1.1.5. 自己编写各种outofmemory，stackoverflow程序  

	HeapOutOfMemory
	Young OutOfMemory
	MethodArea OutOfMemory
	ConstantPool OutOfMemory
	DirectMemory OutOfMemory
	Stack OutOfMemory
	Stack OverFlow
1.1.6. 使用工具尝试解决以下问题，并写下总结  
当一个Java程序响应很慢时如何查找问题  
当一个Java程序频繁FullGC时如何解决问题，如何查看垃圾回收日志  
当一个Java应用发生OutOfMemory时该如何解决，年轻代、年老代、永久代解决办法不同，导致原因也不同    
1.1.7. 参考资料  

	http://docs.oracle.com/javase/specs/jvms/se7/html/
	http://www.cs.umd.edu/~pugh/java/memoryModel/
	http://gee.cs.oswego.edu/dl/jmm/cookbook.html
1.2. Java基础知识

1.2.1. 阅读源代码

	java.lang.String
	java.lang.Integer
	java.lang.Long
	java.lang.Enum
	java.math.BigDecimal
	java.lang.ThreadLocal
	java.lang.ClassLoader &amp;amp; java.net.URLClassLoader
	java.util.ArrayList &amp;amp; java.util.LinkedList
	java.util.HashMap &amp;amp; java.util.LinkedHashMap &amp;amp; java.util.TreeMap
	java.util.HashSet &amp;amp; java.util.LinkedHashSet &amp;amp; java.util.TreeSet
1.2.2. 熟悉Java中各种变量类型  
1.2.3. 熟悉Java String的使用，熟悉String的各种函数  
1.2.4. 熟悉Java中各种关键字  
1.2.5. 学会使用List，Map，Stack，Queue，Set   

	上述数据结构的遍历  
	上述数据结构的使用场景   
	Java实现对Array/List排序   
	java.uti.Arrays.sort()  
	java.util.Collections.sort()     
	Java实现对List去重   
	Java实现对List去重，并且需要保留数据原始的出现顺序  
	Java实现最近最少使用cache，用LinkedHashMap  
1.2.6. Java IO&amp;amp;Java NIO，并学会使用 
  
	java.io.*  
	java.nio.*  
	nio和reactor设计模式   
	文件编码，字符集   
1.2.7. Java反射与javassist  
反射与工厂模式  
java.lang.reflect.*  
1.2.8. Java序列化  
java.io. Serializable   
什么是序列化，为什么序列化  
序列化与单例模式   
google序列化protobuf   
1.2.9. 虚引用，弱引用，软引用   
java.lang.ref.*   
实验这些引用的回收   
1.2.10. 熟悉Java系统属性   
java.util.Properties   
1.2.11. 熟悉Annotation用法   
java.lang.annotation.*   
1.2.12. JMS   
javax.jms.*  
1.2.13. JMX   
java.lang.management.*   
javax.management.*   
1.2.14. 泛型和继承，泛型和擦除   
1.2.15. 自动拆箱装箱与字节码   
1.2.16. 实现Callback   
1.2.17. java.lang.Void类使用   
1.2.18. Java Agent，premain函数   
java.lang.instrument   
1.2.19. 单元测试    
Junit，http://junit.org/   
Jmockit，https://code.google.com/p/jmockit/   
djUnit，http://works.dgic.co.jp/djunit/   
1.2.20. Java实现通过正则表达式提取一段文本中的电子邮件，并将@替换为#输出   
java.lang.util.regex.*   
1.2.21. 学习使用常用的Java工具库   
commons.lang, commons.*…   
guava-libraries   
1.2.22. 什么是API&amp;amp;SPI  
	
	http://en.wikipedia.org/wiki/Application_programming_interface
	http://en.wikipedia.org/wiki/Service_provider_interface
1.2.23. 参考资料

	JDK src.zip 源代码
	http://openjdk.java.net/
	http://commons.apache.org/
	https://code.google.com/p/guava-libraries/
	http://netty.io/
	http://stackoverflow.com/questions/2954372/difference-between-spi-and-api
	http://stackoverflow.com/questions/11404230/how-to-implement-the-api-spi-pattern-in-java
1.3. Java并发编程
 

1.3.1. 阅读源代码，并学会使用  

	java.lang.Thread  
	java.lang.Runnable  
	java.util.concurrent.Callable  
	java.util.concurrent.locks.ReentrantLock  
	java.util.concurrent.locks.ReentrantReadWriteLock  
	java.util.concurrent.atomic.Atomic*  
	java.util.concurrent.Semaphore  
	java.util.concurrent.CountDownLatch
	java.util.concurrent.CyclicBarrier
	java.util.concurrent.ConcurrentHashMap
	java.util.concurrent.Executors
1.3.2. 学习使用线程池，自己设计线程池需要注意什么  
1.3.3. 锁  
什么是锁，锁的种类有哪些，每种锁有什么特点，适用场景是什么  
在并发编程中锁的意义是什么  
1.3.4. synchronized的作用是什么，synchronized和lock  
1.3.5. sleep和wait  
1.3.6. wait和notify  
1.3.7. 写一个死锁的程序  
1.3.8. 什么是守护线程，守护线程和非守护线程的区别以及用法  
1.3.9. volatile关键字的理解  
C++ volatile关键字和Java volatile关键字  
happens-before语义  
编译器指令重排和CPU指令重排  
http://en.wikipedia.org/wiki/Memory_ordering  
http://en.wikipedia.org/wiki/Volatile_variable  
http://preshing.com/20130702/the-happens-before-relation/  
1.3.10. 以下代码是不是线程安全？为什么？如果为count加上volatile修饰是否能够做到线程安全？你觉得该怎么做是线程安全的？  
	
	public class Sample {
	private static int count = 0;
	public static void increment() {
	count++;
	}
	}
1.3.11. 解释一下下面两段代码的差别  
	
	// 代码1
	public class Sample {
	private static int count = 0;
	synchronized public static void increment() {
	count++;
	}
	}
	
	// 代码2
	public class Sample {
	private static AtomicInteger count = new AtomicInteger(0);
	public static void increment() {
	count.getAndIncrement();
	}
	}
1.3.12. 参考资料

	
	http://book.douban.com/subject/10484692/
	http://www.intel.com/content/www/us/en/processors/architectures-software-developer-manuals.html
#二、 进阶篇

2.1. Java底层知识

	2.1.1. 学习了解字节码、class文件格式
	
	http://en.wikipedia.org/wiki/Java_class_file
	http://en.wikipedia.org/wiki/Java_bytecode
	http://en.wikipedia.org/wiki/Java_bytecode_instruction_listings
	http://www.csg.ci.i.u-tokyo.ac.jp/~chiba/javassist/
	http://asm.ow2.org/
2.1.2. 写一个程序要求实现javap的功能（手工完成，不借助ASM等工具）  
如Java源代码：

	public static void main(String[] args) {
	int i = 0;
	i += 1;
	i *= 1;
	System.out.println(i);
	}
	编译后读取class文件输出以下代码：
	public static void main(java.lang.String[]);
	Code:
	Stack=2, Locals=2, Args_size=1
	0:   iconst_0
	1:   istore_1
	2:   iinc    1, 1
	5:   iload_1
	6:   iconst_1
	7:   imul
	8:   istore_1
	9:   getstatic       #2; //Field java/lang/System.out:Ljava/io/PrintStream;
	12:  iload_1
	13:  invokevirtual   #3; //Method java/io/PrintStream.println:(I)V
	16:  return
	LineNumberTable:
	line 4: 0
	line 5: 2
	line 6: 5
	line 7: 9
	line 8: 16
2.1.3. CPU缓存，L1，L2，L3和伪共享  
http://duartes.org/gustavo/blog/post/intel-cpu-caches/  
http://mechanical-sympathy.blogspot.com/2011/07/false-sharing.html  
2.1.4. 什么是尾递归  
2.1.5. 熟悉位运算   
用位运算实现加、减、乘、除、取余   
2.1.6. 参考资料   
	
	http://book.douban.com/subject/1138768/
	http://book.douban.com/subject/6522893/
	http://en.wikipedia.org/wiki/Java_class_file
	http://en.wikipedia.org/wiki/Java_bytecode
	http://en.wikipedia.org/wiki/Java_bytecode_instruction_listings
2.2. 设计模式  

2.2.1. 实现AOP   
CGLIB和InvocationHandler的区别  
http://cglib.sourceforge.net/  
动态代理模式  
Javassist实现AOP  
http://www.csg.ci.i.u-tokyo.ac.jp/~chiba/javassist/  
ASM实现AOP  
http://asm.ow2.org/   
2.2.2. 使用模板方法设计模式和策略设计模式实现IOC   
2.2.3. 不用synchronized和lock，实现线程安全的单例模式  
2.2.4. nio和reactor设计模式  
2.2.5. 参考资料  
http://asm.ow2.org/  
http://cglib.sourceforge.net/  
http://www.javassist.org/  
2.3. 网络编程知识  
2.3.1. Java RMI，Socket，HttpClient  
2.3.2. 用Java写一个简单的静态文件的HTTP服务器  
实现客户端缓存功能，支持返回304  
实现可并发下载一个文件  
使用线程池处理客户端请求  
使用nio处理客户端请求  
支持简单的rewrite规则   
上述功能在实现的时候需要满足“开闭原则”   
2.3.3. 了解nginx和apache服务器的特性并搭建一个对应的服务器   
http://nginx.org/  
http://httpd.apache.org/   
2.3.4. 用Java实现FTP、SMTP协议  
2.3.5. 什么是CDN？如果实现？DNS起到什么作用？  
搭建一个DNS服务器  
搭建一个 Squid 或 Apache Traffic Server 服务器  
http://www.squid-cache.org/  
http://trafficserver.apache.org/  
http://en.wikipedia.org/wiki/Domain_Name_System  
2.3.6. 参考资料  
	
	http://www.ietf.org/rfc/rfc2616.txt
	http://tools.ietf.org/rfc/rfc5321.txt
	http://en.wikipedia.org/wiki/Open/closed_principle
2.4. 框架知识  
spring，spring mvc，阅读主要源码  
ibatis，阅读主要源码  
用spring和ibatis搭建java server  
2.5. 应用服务器知识  
熟悉使用jboss，https://www.jboss.org/overview/  
熟悉使用tomcat，http://tomcat.apache.org/  
熟悉使用jetty，http://www.eclipse.org/jetty/  

# 三、 高级篇

3.1. 编译原理知识

3.1.1. 用Java实现以下表达式解析并返回结果（语法和Oracle中的select sysdate-1 from dual类似）  
sysdate  
sysdate - 1  
sysdate - 1/24  
sysdate - 1/(12*2)   
3.1.2. 实现对一个List通过DSL筛选  

	QList&amp;lt;Map&amp;lt;String, Object&amp;gt;&amp;gt; mapList = new QList&amp;lt;Map&amp;lt;String, Object&amp;gt;&amp;gt;;
	mapList.add({&quot;name&quot;: &quot;hatter test&quot;});
	mapList.add({&quot;id&quot;: -1,&quot;name&quot;: &quot;hatter test&quot;});
	mapList.add({&quot;id&quot;: 0, &quot;name&quot;: &quot;hatter test&quot;});
	mapList.add({&quot;id&quot;: 1, &quot;name&quot;: &quot;test test&quot;});
	mapList.add({&quot;id&quot;: 2, &quot;name&quot;: &quot;hatter test&quot;});
	mapList.add({&quot;id&quot;: 3, &quot;name&quot;: &quot;test hatter&quot;});
	mapList.query(&quot;id is not null and id &amp;gt; 0 and name like &#39;%hatter%&#39;&quot;);
	要求返回列表中匹配的对象，即最后两个对象；

3.1.3. 用Java实现以下程序（语法和变量作用域处理都和JavaScript类似）：
代码：
	
	var a = 1;
	var b = 2;
	var c = function() {
	var a = 3;
	println(a);
	println(b);
	};
	c();
	println(a);
	println(b);
	输出：
	3
	2
	1
	2
3.1.4. 参考资料  
http://en.wikipedia.org/wiki/Abstract_syntax_tree  
https://javacc.java.net/  
http://www.antlr.org/  
3.2. 操作系统知识  
Ubuntu  
Centos  
使用linux，熟悉shell脚本  
3.3. 数据存储知识  
  
3.3.1. 关系型数据库  
MySQL  
如何看执行计划  
如何搭建MySQL主备  
binlog是什么  
Derby，H2，PostgreSQL  
SQLite  
3.3.2. NoSQL  
Cache  
Redis  
Memcached  
Leveldb  
Bigtable  
HBase  
Cassandra  
Mongodb  
图数据库  
neo4j  
3.3.3. 参考资料  
	
	http://db-engines.com/en/ranking
	http://redis.io/
	https://code.google.com/p/leveldb/
	http://hbase.apache.org/
	http://cassandra.apache.org/
	http://www.mongodb.org/
	http://www.neo4j.org/
3.4. 大数据知识  

3.4.1. Zookeeper，在linux上部署zk  
3.4.2. Solr，Lucene，ElasticSearch  
在linux上部署solr，solrcloud，，新增、删除、查询索引  
3.4.3. Storm，流式计算，了解Spark，S4  
在linux上部署storm，用zookeeper做协调，运行storm hello world，local和remote模式运行调试storm topology。  
3.4.4. Hadoop，离线计算   
Hdfs：部署NameNode，SecondaryNameNode，DataNode，上传文件、打开文件、更改文件、删除文件
MapReduce：部署JobTracker，TaskTracker，编写mr job  
Hive：部署hive，书写hive sql，得到结果  
Presto：类hive，不过比hive快，非常值得学习  
3.4.5. 分布式日志收集flume，kafka，logstash  
3.4.6. 数据挖掘，mahout  
3.4.7. 参考资料  

	http://zookeeper.apache.org/
	https://lucene.apache.org/solr/
	https://github.com/nathanmarz/storm/wiki
	http://hadoop.apache.org/
	http://prestodb.io/
	http://flume.apache.org/，http://logstash.net/，http://kafka.apache.org/
	http://mahout.apache.org/
3.5. 网络安全知识  

3.5.1. 什么是DES、AES  
3.5.2. 什么是RSA、DSA  
3.5.3. 什么是MD5，SHA1  
3.5.4. 什么是SSL、TLS，为什么HTTPS相对比较安全  
3.5.5. 什么是中间人攻击、如果避免中间人攻击  
3.5.6. 什么是DOS、DDOS、CC攻击  
3.5.7. 什么是CSRF攻击  
3.5.8. 什么是CSS攻击  
3.5.9. 什么是SQL注入攻击  
3.5.10. 什么是Hash碰撞拒绝服务攻击  
3.5.11. 了解并学习下面几种增强安全的技术  

http://www.openauthentication.org/  
HOTP http://www.ietf.org/rfc/rfc4226.txt  
TOTP http://tools.ietf.org/rfc/rfc6238.txt  
OCRA http://tools.ietf.org/rfc/rfc6287.txt  
http://en.wikipedia.org/wiki/Salt_(cryptography)  
3.5.12. 用openssl签一个证书部署到apache或nginx  
3.5.13. 参考资料
	
	http://en.wikipedia.org/wiki/Cryptographic_hash_function  
	http://en.wikipedia.org/wiki/Public-key_cryptography
	http://en.wikipedia.org/wiki/Transport_Layer_Security
	http://www.openssl.org/
	https://code.google.com/p/google-authenticator/

#四、 扩展篇

4.1. 相关知识

4.1.1. 云计算，分布式，高可用，可扩展  
4.1.2. 虚拟化   
https://linuxcontainers.org/   
http://www.linux-kvm.org/page/Main_Page  
http://www.xenproject.org/  
https://www.docker.io/  
4.1.3. 监控   
http://www.nagios.org/  
http://ganglia.info/  
4.1.4. 负载均衡  
http://www.linuxvirtualserver.org/  
4.1.5. 学习使用git  
https://github.com/  
https://git.oschina.net/  
4.1.6. 学习使用maven  
http://maven.apache.org/   
4.1.7. 学习使用gradle   
http://www.gradle.org/   
4.1.8. 学习一个小语种语言  
Groovy  
Scala  
LISP, Common LISP, Schema, Clojure   
R  
Julia  
Lua  
Ruby  
4.1.9. 尝试了解编码的本质  
  

五、 推荐书籍  
	
	《深入Java虚拟机》
	《深入理解Java虚拟机》
	《Effective Java》
	《七周七语言》
	《七周七数据》
	《Hadoop技术内幕》
	《Hbase In Action》
	《Mahout In Action》
	《这就是搜索引擎》
	《Solr In Action》
	《深入分析Java Web技术内幕》
	《大型网站技术架构》
	《高性能MySQL》
	《算法导论》
	《计算机程序设计艺术》
	《代码大全》
	《JavaScript权威指南》
	
&lt;/转自&gt;
</description>
        <pubDate>Sun, 20 Dec 2015 00:00:00 +0800</pubDate>
        <link>index.html/blog/2015/12/20/javaniubizhilu.html</link>
        <guid isPermaLink="true">index.html/blog/2015/12/20/javaniubizhilu.html</guid>
        
        <category>不懂</category>
        
      </item>
    
      <item>
        <title>flume与hive/impala配合使用时问题记录</title>
        <description>&lt;h3 id=&quot;flumehiveimpala---&quot;&gt;工作中日志收集以及分析  通过Flume+hive/impala实现 期间遇到一些问题  记录：&lt;/h3&gt;

&lt;h3 id=&quot;section&quot;&gt;问题一：&lt;/h3&gt;
&lt;p&gt;使用过程中时常会遇到.tmp文件无法找到的问题。  &lt;br /&gt;
impala 查询时这个问题尤为严重。 hive查询时也会时常出现这个问题。&lt;br /&gt;
明白Flume写文件处理方式，以及hive/impala 数据查询时的实际处理方式，就能明白为什么会出现这些问题   &lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Flume日志写入操作时 会将正在写入的文件加上.tmp后缀。 当文件写入完成（一定时间没有写入/达到设置的阀值）时，会将./tmp文件重命名，去掉.tmp后缀  &lt;/li&gt;
  &lt;li&gt;Hive查询时是根据hive元数据库中的信息进行文件扫描，即对应map/reduce的输入。  &lt;/li&gt;
  &lt;li&gt;Impala共享hive的元数据  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;所以这时就会出现上面遇到的问题&lt;br /&gt;
1. 如果Hive在执行过程中，恰好Flume对写入文件进行了重命名操作，即将xxx.tmp重命名为xxx，这时Hive会报错  .tmp找不到。。。&lt;br /&gt;
2. impala这个错  更加平凡是因为impala同步hive元数据并不是实时操作，所以xxx.tmp被重命名掉的概率更大  &lt;/p&gt;

&lt;h6 id=&quot;section-1&quot;&gt;处理办法：&lt;/h6&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;impala的简单解决办法 即手动同步元数据。 执行refresh 文件对应的表， 或者直接执行 invalidate metadata 刷新元数据  &lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;hive的处理办法 &lt;br /&gt;
因为hdfs java api 读取HDFS文件时，会忽略以”.”和”_”开头的文件&lt;br /&gt;
类似于Linux中.xx是隐藏的一样，所以应用程序读取HDFS文件时默认也不读取.xxx和_xxx这样名称的文件&lt;br /&gt;
所以可以利用这一点 将Flume正在写入的.tmp文件设置为以‘.’开头，这样可以避免.tmp文件被读取到
Flume配置项：hdfs.inUsePrefix&lt;br /&gt;
另一种解决办法 重写hive的PathFilter  &lt;/p&gt;

    &lt;pre&gt;&lt;code&gt; package com.willgo.util;
	
 import java.io.IOException;
 import java.util.ArrayList;
 import java.util.List;
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.fs.PathFilter;
	
 public class FileFilterExcludeTmpFiles implements PathFilter {
     public boolean accept(Path p) {
         String name = p.getName();
         return !name.startsWith(“_”) &amp;amp;&amp;amp; !name.startsWith(“.”) &amp;amp;&amp;amp; !name.endsWith(“.tmp”);
     }
 }
&lt;/code&gt;&lt;/pre&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;hive-site.xml：  &lt;/p&gt;

&lt;pre&gt;&lt;code&gt;	&amp;lt;property&amp;gt;
	    &amp;lt;name&amp;gt;mapred.input.pathFilter.class&amp;lt;/name&amp;gt;
	    &amp;lt;value&amp;gt;com.willgo.util.FileFilterExcludeTmpFiles&amp;lt;/value&amp;gt;
	&amp;lt;/property&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;section-2&quot;&gt;问题二：&lt;/h3&gt;

&lt;p&gt;数据写入延时&lt;br /&gt;
日志上报端 反映日志上报到集群后 查询时有不同情况的延时问题。 但自己测试时有几乎没有延时的问题。。。  &lt;br /&gt;
考虑如下：&lt;br /&gt;
1. Flume channel选择 （but 已是内存）&lt;br /&gt;
2. hdfs写入机制，hdfs文件按block存储，正在写入的block 对文件系统是隐藏的。&lt;br /&gt;
hdfs文件原则：  &lt;br /&gt;
创建文件，这个文件可以立即可见&lt;br /&gt;
写入文件的数据则不被保证可见了，哪怕是执行了刷新操作(flush/sync)。只有数据量大于1个BLOCK时，第一个BLOCK的数据才会被看到，后续的BLOCK也同样的特性。正在写入的BLOCK始终不会被其他用户看到！ &lt;br /&gt;
基于以上两点  遇到的延时问题 基本上判断是第二点引起的。&lt;br /&gt;
###### 处理办法：&lt;br /&gt;
调小Flume写入文件分割的阀值：hdfs.rollInterval。&lt;br /&gt;
现在是按小时分割，是否可以考虑按分钟分割……&lt;br /&gt;
这样又将产生大量小文件的问题以及namenode压力问题，小文件可以采用线下合并以及使用归档的方式处理。&lt;/p&gt;

</description>
        <pubDate>Sat, 28 Nov 2015 00:00:00 +0800</pubDate>
        <link>index.html/blog/2015/11/28/flume-hvie-question.html</link>
        <guid isPermaLink="true">index.html/blog/2015/11/28/flume-hvie-question.html</guid>
        
        <category>hadoop</category>
        
      </item>
    
      <item>
        <title>博客操作记录</title>
        <description>&lt;p&gt;在_config.yml配置站点信息，详细配置如下：&lt;/p&gt;

&lt;p&gt;blog:
	name:                  # 博客名称
	description:           # 博客描述
	title:                 # 网页标题
	url:                   # 博客地址
	duoshuo:               # 多说ID
	tongji:                # 百度统计ID
	qiniu:                 # 七牛云地址
author:
	name:                  # 作者名称
	email:                 # 邮箱地址
	weibo:                 # 微博地址
	github:                # GitHub地址
	douban:
		name:              # 豆瓣地址名称
		key:               # 豆瓣API Key&lt;/p&gt;

&lt;p&gt;多说评论框
_posts文章默认开启评论框，而简版页面默认关闭。
_posts文章可以在开头设置duoshuo: false来关闭。
简版页面可以在开头设置duoshuo: true来开启。&lt;/p&gt;

&lt;p&gt;MathJax数学公式
需要在页面开头添加math: true来开启&lt;/p&gt;

&lt;p&gt;在需要用到公式的地方用[ ]或&lt;script type=&quot;math/tex&quot;&gt; &lt;/script&gt;括起来&lt;/p&gt;

&lt;p&gt;例如：&lt;/p&gt;

&lt;p&gt;行内公式：
\E=mc^2\&lt;/p&gt;

&lt;p&gt;行间公式：
&lt;script type=&quot;math/tex&quot;&gt;E=mc^2&lt;/script&gt;
效果：&lt;/p&gt;

&lt;p&gt;\E=mc^2\&lt;/p&gt;

&lt;p&gt;E=mc2
创建文章/页面&lt;br /&gt;
定位到博客目录，可以运行以下命令  &lt;/p&gt;

&lt;p&gt;创建文章：rake post title=”Post Name”&lt;br /&gt;
创建简版页面：rake life title=”Page Name”&lt;br /&gt;
创建页面： rake page title=”Page name”&lt;br /&gt;
页面的使用 &lt;br /&gt;
修改的都是markdown文件   &lt;/p&gt;

&lt;p&gt;普通页面 &lt;br /&gt;
layout项改为blog &lt;br /&gt;
简版页面 &lt;br /&gt;
layout项改为life &lt;br /&gt;
文章 &lt;br /&gt;
_posts文件夹下的markdown文件的layout项改为post，使用简版页面就改成life&lt;br /&gt;
生成静态博客&lt;br /&gt;
把你的博客推送到GitHub或者其它支持Jekyll的代码托管网站就可以了。&lt;br /&gt;
具体可以到Jekyll官网或GitHub Pages查看详细教程。  &lt;/p&gt;

</description>
        <pubDate>Wed, 28 Oct 2015 00:00:00 +0800</pubDate>
        <link>index.html/blog/2015/10/28/bokecaozuo.html</link>
        <guid isPermaLink="true">index.html/blog/2015/10/28/bokecaozuo.html</guid>
        
        <category>f2e</category>
        
      </item>
    
      <item>
        <title>Sqoop密码和SA用户问题记录</title>
        <description>&lt;p&gt;sqoop 定时job 运行时需要输入密码问题
1.可以使用except 脚本，实现交互输入密码&lt;br /&gt;
2. 配置sqoop，将密码保存至sqoop的metadata中 &lt;br /&gt;
sqoop-site.xml：  &lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;property&amp;gt;  
     &amp;lt;name&amp;gt;sqoop.metastore.client.record.password&amp;lt;/name&amp;gt;  
     &amp;lt;value&amp;gt;true&amp;lt;/value&amp;gt;  
     &amp;lt;description&amp;gt;If true, allow saved passwords in the metastore. &amp;lt;/description&amp;gt;
&amp;lt;/property&amp;gt;  
&lt;/code&gt;&lt;/pre&gt;

&lt;ol&gt;
  &lt;li&gt;sqoop 出现Caused by: java.sql.SQLException: User not found: SA  错误  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;未知问题，看起来像是并发执行sqoop引起的问题  导致无法连接sqoop的metadata库&lt;br /&gt;
不知如何解决， 可以采取的办法&lt;br /&gt;
1.换个用户执行 貌似可以（。。。。。）&lt;br /&gt;
2.更改metadata数据库  如修改成MySQL   如下：  &lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;property&amp;gt;  
    &amp;lt;name&amp;gt;sqoop.metastore.client.autoconnect.url&amp;lt;/name&amp;gt;  
    &amp;lt;value&amp;gt;jdbc:mysql://192.168.117.7:3306/sqoop&amp;lt;/value&amp;gt;  
&amp;lt;/property&amp;gt;  
&amp;lt;property&amp;gt;  
    &amp;lt;name&amp;gt;sqoop.metastore.client.autoconnect.username&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;scm987&amp;lt;/value&amp;gt;  
&amp;lt;/property&amp;gt;  
&amp;lt;property&amp;gt;  
    &amp;lt;name&amp;gt;sqoop.metastore.client.autoconnect.password&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;scm258&amp;lt;/value&amp;gt;  
&amp;lt;/property&amp;gt;  


&amp;lt;property&amp;gt;
     &amp;lt;name&amp;gt;sqoop.metastore.client.enable.autoconnect&amp;lt;/name&amp;gt;
     &amp;lt;value&amp;gt;true&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
</description>
        <pubDate>Sun, 20 Sep 2015 00:00:00 +0800</pubDate>
        <link>index.html/blog/2015/09/20/sqoopmimashuruwenti.html</link>
        <guid isPermaLink="true">index.html/blog/2015/09/20/sqoopmimashuruwenti.html</guid>
        
        <category>hadoop</category>
        
      </item>
    
      <item>
        <title>Sql on hadoop 选择笔记</title>
        <description>&lt;p&gt;Hive展现出他强大的批处理能力 但在实时交互式查询时方面却难以满足，现在已经出现的低延时交互式处理方案已经有很多  如：Hive on Tez, Hive on Spark, Spark SQL, Impala等   &lt;/p&gt;

&lt;p&gt;Hive/Tez/Stinger &lt;br /&gt;
目前的主要推动者是hortonworks和Yahoo!。2015 Hadoop Summit(San Jose)上，Yahoo分享了他们目前生产环境中Hive on Tez的一些情况。显示和Hive 0.10(RCFile)相比，目前的Hive on Tez在1TB的数据量查询的加速比平均为6.2倍。目前的Hive on Tez已经是production-ready。Tez这个执行引擎和Spark比较类似，原来的MR只能执行Map和Reduce两种操作，现在的Tez可以把Job解析成DAG来执行。除此之外还有一些进一步优化Hive执行效率的工作，例如Vectorized Execution和ORCFile等。Dropbox也透露他们的Hive集群下一步的升级目标就是Hive on Tez。  &lt;/p&gt;

&lt;p&gt;Hive on Spark &lt;br /&gt;
目前的主要推动者是Cloudera，可以认为是Hive社区这边搞的”Spark SQL”。刚刚release了第一个使用版本，目前不能用于生产环境。Hive on Spark既能利用到现在广泛使用的Hive的前端，又能利用到广泛使用的Spark作为后端执行引擎。对于现在既部署了Hive，又部署了Spark的公司来说，节省了运维成本。    &lt;/p&gt;

&lt;p&gt;对于上面提到的Hive on Tez和Hive on Spark两种系统都具备的优点是： &lt;br /&gt;
1，现存的Hive jobs可以透明、无缝迁移到Hive on ***平台，可以利用Hive现有的ODBC/JDBC，metastore, hiveserver2, UDF，auditing, authorization, monitoring系统，不需要做任何更改和测试，迁移成本低。 &lt;br /&gt;
2，无论后端执行引擎是MapReduce也好，Tez也好，Spark也好，整个Hive SQL解析、生成执行计划、执行计划优化的过程都是非常类似的。而且大部分公司都积累了一定的Hive运维和使用经验，那么对于bug调试、性能调优等环节会比较熟悉，降低了运维成本。   &lt;/p&gt;

&lt;p&gt;Spark SQL &lt;br /&gt;
主要的推动者是Databricks。提到Spark SQL不得不提的就是Shark。Shark可以理解为Spark社区这边搞的一个”Hive on Spark”，把Hive的物理执行计划使用Spark计算引擎去执行。这里面会有一些问题，Hive社区那边没有把物理执行计划到执行引擎这个步骤抽象出公共API，所以Spark社区这边要自己维护一个Hive的分支，而且Hive的设计和发展不太会考虑到如何优化Spark的Job。但是前面提到的Hive on Spark却是和Hive一起发布的，是由Hive社区控制的。 &lt;br /&gt;
所以后来Spark社区就停止了Shark的开发转向Spark SQL（“坑了”一部分当时信任Shark的人）。  Spark SQL是把SQL解析成RDD的transformation和action，而且通过catalyst可以自由、灵活的选择最优执行方案。对数据库有深入研究的人就会知道，SQL执行计划的优化是一个非常重要的环节，  Spark SQL在这方面的优势非常明显，提供了一个非常灵活、可扩展的架构。但是Spark SQL是基于内存的，元数据放在内存里面，不适合作为数据仓库的一部分来使用。所以有了Spark SQL的HiveContext，就是兼容Hive的Spark SQL。它支持HiveQL, Hive Metastore, Hive SerDes and Hive UDFs以及JDBC driver。这样看起来很完美，但是实际上也有一些缺点：Spark SQL依赖于Hive的一个snapshot，所以它总是比Hive的发布晚一个版本，很多Hive新的feature和bug fix它就无法包括。而且目前看Spark社区在Spark的thriftserver方面的投入不是很大，所以感觉它不是特别想朝着这个方向发展。还有一个重要的缺点就是Spark SQL目前还不能通过分析SQL来预测这个查询需要多少资源从而申请对应的资源，所以在共享集群上无法高效地分配资源和调度任务。 &lt;br /&gt;
特别是目前Spark社区把Spark SQL朝向DataFrame发展，目标是提供一个类似R或者Pandas的接口，把这个作为主要的发展方向。DataFrame这个功能使得Spark成为机器学习和数据科学领域不可或缺的一个组件，但是在数据仓库（ETL，交互式分析，BI查询）领域感觉已经不打算作为他们主要的发展目标了。  &lt;/p&gt;

&lt;p&gt;Impala &lt;br /&gt;
主要的推动者是Cloudera，自从推出以来一直不温不火。Impala是一种MPP架构的执行引擎，查询速度非常快，是交互式BI查询最好的选择，即使是在并发性非常高的情况下也能保证查询延迟，所以在multi-tenant, shared clusters上表现比较好。Impala的另外一个重要的优点就是支持的SQL是在以上这些系统中是最标准的，也就是跟SQL99是最像的，所以对于传统企业来说可能是个不错的选择。  Impala的主要缺点是社区不活跃，由C++开发，可维护性差，目前系统稳定性还有待提高。  &lt;/p&gt;

&lt;p&gt;Presto&lt;br /&gt;
是Facebook开发的，目前也得到了Teradata的支持。目前Presto的主要使用者还是互联网公司，像Facebook，Netflix等。Presto的代码用了Dependency Injection, 比较难理解和debug。另外还有一些系统，像Apache Drill，Apache Tajo等，都是非常小众的系统了。   &lt;/p&gt;

&lt;p&gt;总的来说，目前来看Hive依然是批处理/ETL 类应用的首选。Hive on Spark能够降低Hive的延迟，但是还是达不到交互式BI查询的需求。目前交互式BI查询最好的选择是Impala。Spark SQL/DataFrame是Spark用户使用SQL或者DataFrame API构建Spark pipeline的一种选择，并不是一个通用的支持交互式查询的引擎，更多的会用在基于Spark的机器学习任务的数据处理和准备的环节。     &lt;/p&gt;
</description>
        <pubDate>Sun, 20 Sep 2015 00:00:00 +0800</pubDate>
        <link>index.html/blog/2015/09/20/nosql-onhadoop.html</link>
        <guid isPermaLink="true">index.html/blog/2015/09/20/nosql-onhadoop.html</guid>
        
        <category>nosql</category>
        
      </item>
    
      <item>
        <title>JVM笔记</title>
        <description>&lt;p&gt;JVM根据实现不同结构有所不同，大多数将内存分为：&lt;br /&gt;
Method Area                        方法区&lt;br /&gt;
Heap			                     堆
Program Counter register               程序计数器&lt;br /&gt;
Java method stack                     Java方法栈 &lt;br /&gt;
native method stack                   本地方法栈 （Hot Spot 中将Java method和native合称为方法区）&lt;br /&gt;
direct memory                       直接内存区（此区域并不归JVM管理）    &lt;/p&gt;

&lt;p&gt;指令 数据 方法 实例 属性。。。&lt;br /&gt;
方法是指令操作码的一部分保存在stack中&lt;br /&gt;
方法内部变量作为指令的操作数部分，跟在指令的操作码之后，保存在Stack中（实际上是简单类型保存在Stack中，对象类型在Stack中保存地址，在Heap 中保存值）；指令操作码和指令操作数构成了完整的Java 指令。对象实例包括其属性值作为数据，保存在数据区Heap 中&lt;br /&gt;
非静态的对象属性作为对象实例的一部分保存在Heap 中，而对象实例必须通过Stack中保存的地址指针才能访问到。因此能否访问到对象实例以及它的非静态属性值完全取决于能否获得对象实例在Stack中的地址指针。   &lt;/p&gt;

&lt;p&gt;静态/非静态方法 &lt;br /&gt;
非静态方法有一个隐含的传入参数，该参数是JVM给它的，和我们怎么写代码无关，这个隐含的参数就是对象实例在Stack中的地址指针。So我们在调用前都要new,获得Stack中的地址指针。 &lt;br /&gt;
静态方法无此隐含参数，因此也不需要new对象，只要class文件被ClassLoader load进入JVM的Stack，该静态方法即可被调用。当然此时静态方法是存取不到Heap 中的对象属性的。   &lt;/p&gt;

&lt;p&gt;静/动态属性 &lt;br /&gt;
实例以及动态属性都是保存在Heap 中的， Heap 必须通过Stack中的地址指针才能够被指令（类的方法）访问到。 &lt;br /&gt;
静态属性是保存在Stack中的，而不同于动态属性保存在Heap 中。正因为都是在Stack中，而Stack中指令和数据都是定长的，因此很容易算出偏移量，也因此不管什么指令，都可以访问到类的静态属性。也正因为静态属性被保存在Stack中，所以具有了全局属性。 &lt;br /&gt;
在JVM中，静态属性保存在Stack指令内存区，动态属性保存在Heap数据内存区。   &lt;/p&gt;

&lt;p&gt;栈&lt;br /&gt;
Stack（栈）是JVM的内存指令区。Stack的速度很快，管理很简单，并且每次操作的数据或者指令字节长度是已知的。所以Java 基本数据类型，Java 指令代码，常量都保存在Stack中。&lt;br /&gt;
是 Java 程序的运行区，是在线程创建时创建，它的生命期是跟随线程的生命期，线程结束栈内存也就释放，对于栈来说不存在垃圾回收问题。&lt;br /&gt;
栈中数据都是以栈帧（stack frame）的形式存在，栈帧是一个内存块一个有关方法和运行期数据的数据集，遵循 先进后出原则（方法 A 被调用时就产生了一个栈帧 F1，并被压入到栈中，A 方法又调用了 B 方法，于是产生栈帧 F2 也被压入栈，执行完毕后，先弹出 F2栈帧，再弹出 F1 栈帧）
栈帧中主要保存三类数据：&lt;br /&gt;
local variable 本地变量（输入参数和输出参数以及方法内的变量）&lt;br /&gt;
operand stack栈操作  （出栈进栈记录）&lt;br /&gt;
frame data栈帧数据   （类文件 方法等）  &lt;/p&gt;

&lt;p&gt;Heap&lt;br /&gt;
JVM的内存数据区 管理复杂，用于保存对象的实例 &lt;br /&gt;
Heap 中分配一定的内存来保存对象实例，实际上也只是保存对象实例的属性值，属性的类型和对象本身的类型标记等，并不保存对象的方法（方法是指令，保存在Stack中）,在Heap 中分配一定的内存保存对象实例和对象的序列化比较类似。而对象实例在Heap 中分配好以后，需要在Stack中保存一个4字节的Heap 内存地址，用来定位该对象实例在Heap 中的位置，便于找到该对象实例。   &lt;/p&gt;

&lt;p&gt;Java中堆是由所有的线程共享的一块内存区域。 &lt;br /&gt;
Heap 中分配一定的内存来保存对象实例，实际上也只是保存对象实例的属性值，属性的类型和对象本身的类型标记等，并不保存对象的方法（方法是指令，保存在Stack中）,在Heap 中分配一定的内存保存对象实例和对象的序列化比较类似。而对象实例在Heap 中分配好以后，需要在Stack中保存一个4字节的Heap 内存地址，用来定位该对象实例在Heap 中的位置，便于找到该对象实例。&lt;br /&gt;
Java中堆是由所有的线程共享的一块内存区域。   &lt;/p&gt;

&lt;p&gt;Perm &lt;br /&gt;
Perm代主要保存class,method,filed对象，这部门的空间一般不会溢出，除非一次性加载了很多的类，不过在涉及到热部署的应用服务器的时候，有时候会遇到java.lang.OutOfMemoryError : PermGen space 的错误，造成这个错误的很大原因就有可能是每次都重新部署，但是重新部署后，类的class没有被卸载掉，这样就造成了大量的class对象保存在了perm中，这种情况下，一般重新启动应用服务器可以解决问题。    &lt;/p&gt;

&lt;p&gt;Tenured &lt;br /&gt;
Tenured区主要保存生命周期长的对象，一般是一些老的对象，当一些对象在Young复制转移一定的次数以后，对象就会被转移到Tenured区，一般如果系统中用了application级别的缓存，缓存中的对象往往会被转移到这一区间。  &lt;/p&gt;

&lt;p&gt;Young&lt;br /&gt;
Young区被划分为三部分，Eden区和两个大小严格相同的Survivor区，其中Survivor区间中，某一时刻只有其中一个是被使用的，另外一个留做垃圾收集时复制对象用，在Young区间变满的时候，minor GC就会将存活的对象移到空闲的Survivor区间中，根据JVM的策略，在经过几次垃圾收集后，任然存活于Survivor的对象将被移动到Tenured区间。  &lt;/p&gt;

&lt;p&gt;The pc Register 程序计数器寄存器 &lt;br /&gt;
JVM支持多个线程同时运行。每个JVM都有自己的程序计数器。在任何一个点，每个JVM线程执行单个方法的代码，这个方法是线程的当前方法。如果方法不是native的，程序计数器寄存器包含了当前执行的JVM指令的地址，如果方法是 native的，程序计数器寄存器的值不会被定义。 JVM的程序计数器寄存器的宽度足够保证可以持有一个返回地址或者native的指针。  &lt;/p&gt;

&lt;p&gt;Method Area 方法区&lt;br /&gt;
Object Class Data(类定义数据) 是存储在方法区的。除此之外，常量、静态变量、JIT 编译后的代码也都在方法区。正因为方法区所存储的数据与堆有一种类比关系，所以它还被称为 Non-Heap。方法区也可以是内存不连续的区域组成的，并且可设置为固定大小，也可以设置为可扩展的，这点与堆一样。
方法区内部有一个非常重要的区域，叫做运行时常量池（Runtime Constant Pool，简称 RCP）。在字节码文件中有常量池（Constant Pool Table），用于存储编译器产生的字面量和符号引用。每个字节码文件中的常量池在类被加载后，都会存储到方法区中。值得注意的是，运行时产生的新常量也可以被放入常量池中，比如 String 类中的 intern() 方法产生的常量。    &lt;/p&gt;

&lt;p&gt;内存分配：&lt;br /&gt;
1、对象优先在EDEN分配&lt;br /&gt;
2、大对象直接进入老年代 &lt;br /&gt;
3、长期存活的对象将进入老年代 &lt;br /&gt;
4、适龄对象也可能进入老年代：动态对象年龄判断&lt;br /&gt;
动态对象年龄判断：&lt;br /&gt;
虚拟机并不总是要求对象的年龄必须达到MaxTenuringThreshold才能晋升到老年代，当Survivor空间的相同年龄的所有对象大小总和大于Survivor空间的一半，年龄大于或等于该年龄的对象就可以直接进入老年代，无需等到MaxTenuringThreshold中指定的年龄  &lt;/p&gt;

&lt;p&gt;1、对象优先在Eden分配，这里大部分对象具有朝生夕灭的特征，Minor GC主要清理该处&lt;br /&gt;
2、大对象（占内存大）、老对象（使用频繁）&lt;br /&gt;
3、Survivor无法容纳的对象，将进入老年代，Full GC的主要清理该处  &lt;/p&gt;

&lt;p&gt;JVM的GC机制&lt;br /&gt;
第一个线程负责回收Heap的Young区&lt;br /&gt;
第二个线程在Heap不足时，遍历Heap，将Young 区升级为Older区&lt;br /&gt;
Older区的大小等于-Xmx减去-Xmn，不能将-Xms的值设的过大，因为第二个线程被迫运行会降低JVM的性能。 &lt;br /&gt;
堆内存GC&lt;br /&gt;
       JVM(采用分代回收的策略)，用较高的频率对年轻的对象(young generation)进行YGC，而对老对象(tenured generation)较少(tenured generation 满了后才进行)进行Full GC。这样就不需要每次GC都将内存中所有对象都检查一遍。&lt;br /&gt;
非堆内存不GC&lt;br /&gt;
      GC不会在主程序运行期对PermGen Space进行清理，所以如果你的应用中有很多CLASS(特别是动态生成类，当然permgen space存放的内容不仅限于类)的话,就很可能出现PermGen Space错误。&lt;br /&gt;
内存申请过程 &lt;br /&gt;
1.JVM会试图为相关Java对象在Eden中初始化一块内存区域；&lt;br /&gt;
2.当Eden空间足够时，内存申请结束。否则到下一步；&lt;br /&gt;
3.JVM试图释放在Eden中所有不活跃的对象（minor collection），释放后若Eden空间仍然不足以放入新对象，则试图将部分Eden中活跃对象放入Survivor区；&lt;br /&gt;
4.Survivor区被用来作为Eden及old的中间交换区域，当OLD区空间足够时，Survivor区的对象会被移到Old区，否则会被保留在Survivor区；&lt;br /&gt;
5.当old区空间不够时，JVM会在old区进行major collection；&lt;br /&gt;
6.完全垃圾收集后，若Survivor及old区仍然无法存放从Eden复制过来的部分对象，导致JVM无法在Eden区为新对象创建内存区域，则出现”Out of memory错误”；  &lt;/p&gt;

&lt;p&gt;对象衰老过程 &lt;br /&gt;
1.新创建的对象的内存都分配自eden。Minor collection的过程就是将eden和在用survivor space中的活对象copy到空闲survivor space中。对象在young generation里经历了一定次数(可以通过参数配置)的minor collection后，就会被移到old generation中，称为tenuring。&lt;br /&gt;
&lt;img src=&quot;/image/jvm-memery.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;类的加载方式 &lt;br /&gt;
1）：本地编译好的class中直接加载&lt;br /&gt;
2）：网络加载：java.net.URLClassLoader可以加载url指定的类&lt;br /&gt;
3）：从jar、zip等等压缩文件加载类，自动解析jar文件找到class文件去加载util类&lt;br /&gt;
4）：从java源代码文件动态编译成为class文件  &lt;/p&gt;

&lt;p&gt;类加载的时机&lt;br /&gt;
1. 类加载的 生命周期 ：加载（Loading）–&amp;gt;验证（Verification）–&amp;gt;准备（Preparation）–&amp;gt;解析（Resolution）–&amp;gt;初始化（Initialization）–&amp;gt;使用（Using）–&amp;gt;卸载（Unloading）&lt;br /&gt;
2. 加载：这有虚拟机自行决定。&lt;br /&gt;
3. 初始化阶段：&lt;br /&gt;
a) 遇到new、getstatic、putstatic、invokestatic这4个字节码指令时，如果类没有进行过初始化，出发初始化操作。 &lt;br /&gt;
b) 使用java.lang.reflect包的方法对类进行反射调用时。&lt;br /&gt;
c) 当初始化一个类的时候，如果发现其父类还没有执行初始化则进行初始化。&lt;br /&gt;
d) 虚拟机启动时用户需要指定一个需要执行的主类，虚拟机首先初始化这个主类。&lt;br /&gt;
注意：接口与类的初始化规则在第三点不同，接口不要气所有的父接口都进行初始化。   &lt;/p&gt;

&lt;p&gt;双亲委派机制 &lt;br /&gt;
JVM在加载类时默认采用的是 双亲委派 机制。通俗的讲，就是某个特定的类加载器在接到加载类的请求时，首先将加载任务委托给父类加载器，依次递归，如果父类加载器可以完成类加载任务，就成功返回；只有父类加载器无法完成此加载任务时，才自己去加载。   &lt;/p&gt;
</description>
        <pubDate>Sun, 20 Sep 2015 00:00:00 +0800</pubDate>
        <link>index.html/blog/2015/09/20/jvm-zaixuexi.html</link>
        <guid isPermaLink="true">index.html/blog/2015/09/20/jvm-zaixuexi.html</guid>
        
        <category>java</category>
        
      </item>
    
      <item>
        <title>Hive配置记录</title>
        <description>&lt;p&gt;hive.fetch.task.conversion: &lt;br /&gt;
hive 0.10.0为了执行效率考虑，简单的查询，就是只是select，不带count,sum,group by这样的，都不走map/reduce，直接读取hdfs文件进行filter过滤。&lt;br /&gt;
这样做的好处就是不新开mr任务，执行效率要提高不少，但是不好的地方就是用户界面不友好，有时候数据量大还是要等很长时间，但是又没有任何返回。&lt;br /&gt;
在hive-site.xml里面有个配置参数叫 &lt;br /&gt;
hive.fetch.task.conversion&lt;br /&gt;
将这个参数设置为more，简单查询就不走map/reduce了，设置为minimal(默认)，就任何简单select都会走map/reduce。   &lt;/p&gt;

&lt;p&gt;付：&lt;br /&gt;
hive的配置：  &lt;/p&gt;

&lt;pre&gt;&lt;code&gt;hive.ddl.output.format：hive的ddl语句的输出格式，默认是text，纯文本，还有json格式，这个是0.90以后才出的新配置；  
hive.exec.script.wrapper：hive调用脚本时的包装器，默认是null，如果设置为python的话，那么在做脚本调用操作时语句会变为python &amp;lt;script command&amp;gt;，null的话就是直接执行&amp;lt;script command&amp;gt;；  
hive.exec.plan：hive执行计划的文件路径，默认是null，会在运行时自动设置，形如hdfs://xxxx/xxx/xx；    
hive.exec.scratchdir：hive用来存储不同阶段的map/reduce的执行计划的目录，同时也存储中间输出结果，默认是/tmp/&amp;lt;user.name&amp;gt;/hive，我们实际一般会按组区分，然后组内自建一个tmp目录存储；  
hive.exec.submitviachild：在非local模式下，决定hive是否要在独立的jvm中执行map/reduce；默认是false，也就是说默认map/reduce的作业是在hive的jvm上去提交的；
hive.exec.script.maxerrsize：当用户调用transform或者map或者reduce执行脚本时，最大的序列化错误数，默认100000，一般也不用修改；
hive.exec.compress.output：一个查询的最后一个map/reduce任务输出是否被压缩的标志，默认为false，但是一般会开启为true，好处的话，节省空间不说，在不考虑cpu压力的时候会提高io；
hive.exec.compress.intermediate：类似上个，在一个查询的中间的map/reduce任务输出是否要被压缩，默认false，
hive.jar.path：当使用独立的jvm提交作业时，hive_cli.jar所在的位置，无默认值；
hive.aux.jars.path：当用户自定义了UDF或者SerDe，这些插件的jar都要放到这个目录下，无默认值；
hive.partition.pruning：在编译器发现一个query语句中使用分区表然而未提供任何分区谓词做查询时，抛出一个错误从而保护分区表，默认是nonstrict；（待读源码后细化，网上资料极少）
hive.map.aggr：map端聚合是否开启，默认开启；
hive.join.emit.interval：在发出join结果之前对join最右操作缓存多少行的设定，默认1000；hive jira里有个对该值设置太小的bugfix；
hive.map.aggr.hash.percentmemory：map端聚合时hash表所占用的内存比例，默认0.5，这个在map端聚合开启后使用，
hive.default.fileformat：CREATE TABLE语句的默认文件格式，默认TextFile，其他可选的有SequenceFile、RCFile还有Orc；
hive.merge.mapfiles：在只有map的作业结束时合并小文件，默认开启true；
hive.merge.mapredfiles：在一个map/reduce作业结束后合并小文件，默认不开启false；
&amp;lt;property&amp;gt;
  &amp;lt;name&amp;gt;hive.merge.mapredfiles&amp;lt;/name&amp;gt;
  &amp;lt;value&amp;gt;true&amp;lt;/value&amp;gt;
  &amp;lt;description&amp;gt;在一个map/reduce作业结束后合并小文件&amp;lt;/description&amp;gt;
&amp;lt;/property&amp;gt;

hive.merge.size.per.task：作业结束时合并文件的大小，默认256MB；
hive.merge.smallfiles.avgsize：在作业输出文件小于该值时，起一个额外的map/reduce作业将小文件合并为大文件，小文件的基本阈值，设置大点可以减少小文件个数，需要mapfiles和mapredfiles为true，默认值是16MB；
mapred.reduce.tasks：每个作业的reduce任务数，默认是hadoop client的配置1个；
hive.exec.reducers.bytes.per.reducer：每个reducer的大小，默认是1G，输入文件如果是10G，那么就会起10个reducer；
hive.exec.reducers.max：reducer的最大个数，如果在mapred.reduce.tasks设置为负值，那么hive将取该值作为reducers的最大可能值。当然还要依赖（输入文件大小/hive.exec.reducers.bytes.per.reducer）所得出的大小，取其小值作为reducer的个数，hive默认是999；
hive.fileformat.check：加载数据文件时是否校验文件格式，默认是true；
hive.groupby.skewindata：group by操作是否允许数据倾斜，默认是false，当设置为true时，执行计划会生成两个map/reduce作业，第一个MR中会将map的结果随机分布到reduce中，达到负载均衡的目的来解决数据倾斜，
hive.groupby.mapaggr.checkinterval：map端做聚合时，group by 的key所允许的数据行数，超过该值则进行分拆，默认是100000；
hive.mapred.local.mem：本地模式时，map/reduce的内存使用量，默认是0，就是无限制；
hive.mapjoin.followby.map.aggr.hash.percentmemory：map端聚合时hash表的内存占比，该设置约束group by在map join后进行，否则使用hive.map.aggr.hash.percentmemory来确认内存占比，默认值0.3；
hive.map.aggr.hash.force.flush.memeory.threshold：map端聚合时hash表的最大可用内存，如果超过该值则进行flush数据，默认是0.9；
hive.map.aggr.hash.min.reduction：如果hash表的容量与输入行数之比超过这个数，那么map端的hash聚合将被关闭，默认是0.5，设置为1可以保证hash聚合永不被关闭；
hive.optimize.groupby：在做分区和表查询时是否做分桶group by，默认开启true；
hive.multigroupby.singlemr：将多个group by产出为一个单一map/reduce任务计划，当然约束前提是group by有相同的key，默认是false；
hive.optimize.cp：列裁剪，默认开启true，在做查询时只读取用到的列，这个是个有用的优化；
hive.optimize.index.filter：自动使用索引，默认不开启false；
hive.optimize.index.groupby：是否使用聚集索引优化group-by查询，默认关闭false；
hive.optimize.ppd：是否支持谓词下推，默认开启；所谓谓词下推，将外层查询块的 WHERE 子句中的谓词移入所包含的较低层查询块（例如视图），从而能够提早进行数据过滤以及有可能更好地利用索引。
hive.optimize.ppd.storage：谓词下推开启时，谓词是否下推到存储handler，默认开启，在谓词下推关闭时不起作用；
hive.ppd.recognizetransivity：在等值join条件下是否产地重复的谓词过滤器，默认开启；
hive.join.cache.size：在做表join时缓存在内存中的行数，默认25000；
hive.mapjoin.bucket.cache.size：mapjoin时内存cache的每个key要存储多少个value，默认100；
hive.optimize.skewjoin：是否开启数据倾斜的join优化，默认不开启false；
hive.skewjoin.key：判断数据倾斜的阈值，如果在join中发现同样的key超过该值则认为是该key是倾斜的join key，默认是100000；
hive.skewjoin.mapjoin.map.tasks：在数据倾斜join时map join的map数控制，默认是10000；
hive.skewjoin.mapjoin.min.split：数据倾斜join时map join的map任务的最小split大小，默认是33554432，该参数要结合上面的参数共同使用来进行细粒度的控制；
hive.mapred.mode：hive操作执行时的模式，默认是nonstrict非严格模式，如果是strict模式，很多有风险的查询会被禁止运行，比如笛卡尔积的join和动态分区；
hive.exec.script.maxerrsize：一个map/reduce任务允许打印到标准错误里的最大字节数，为了防止脚本把分区日志填满，默认是100000；
hive.exec.script.allow.partial.consumption：hive是否允许脚本不从标准输入中读取任何内容就成功退出，默认关闭false；
hive.script.operator.id.env.var：在用户使用transform函数做自定义map/reduce时，存储唯一的脚本标识的环境变量的名字，默认HIVE_SCRIPT_OPERATOR_ID；
hive.exec.compress.output：控制hive的查询结果输出是否进行压缩，压缩方式在hadoop的mapred.output.compress中配置，默认不压缩false；
hive.exec.compress.intermediate：控制hive的查询中间结果是否进行压缩，同上条配置，默认不压缩false；
hive.exec.parallel：hive的执行job是否并行执行，默认不开启false，在很多操作如join时，子查询之间并无关联可独立运行，这种情况下开启并行运算可以大大加速；
hvie.exec.parallel.thread.number：并行运算开启时，允许多少作业同时计算，默认是8；
hive.exec.rowoffset：是否提供行偏移量的虚拟列，默认是false不提供，Hive有两个虚拟列:一个是INPUT__FILE__NAME,表示输入文件的路径，另外一个是BLOCK__OFFSET__INSIDE__FILE，表示记录在文件中的块偏移量，这对排查出现不符合预期或者null结果的查询是很有帮助的；
hive.task.progress：控制hive是否在执行过程中周期性的更新任务进度计数器，开启这个配置可以帮助job tracker更好的监控任务的执行情况，但是会带来一定的性能损耗，当动态分区标志hive.exec.dynamic.partition开启时，本配置自动开启；
hive.exec.pre.hooks：执行前置条件，一个用逗号分隔开的实现了org.apache.hadoop.hive.ql.hooks.ExecuteWithHookContext接口的java class列表，配置了该配置后，每个hive任务执行前都要执行这个执行前钩子，默认是空；
hive.exec.post.hooks：同上，执行后钩子，默认是空；
hive.exec.failure.hooks：同上，异常时钩子，在程序发生异常时执行，默认是空；
hive.mergejob.maponly：试图生成一个只有map的任务去做merge，前提是支持CombineHiveInputFormat，默认开启true；
hive.mapjoin.smalltable.filesize：输入表文件的mapjoin阈值，如果输入文件的大小小于该值，则试图将普通join转化为mapjoin，默认25MB；
hive.mapjoin.localtask.max.memory.usage：mapjoin本地任务执行时hash表容纳key/value的最大量，超过这个值的话本地任务会自动退出，默认是0.9；
hive.mapjoin.followby.gby.localtask.max.memory.usage：类似上面，只不过是如果mapjoin后有一个group by的话，该配置控制类似这样的query的本地内存容量上限，默认是0.55；
hive.mapjoin.check.memory.rows：在运算了多少行后执行内存使用量检查，默认100000；
hive.heartbeat.interval：发送心跳的时间间隔，在mapjoin和filter操作中使用，默认1000；
hive.auto.convert.join：根据输入文件的大小决定是否将普通join转换为mapjoin的一种优化，默认不开启false；
hive.script.auto.progress：hive的transform/map/reduce脚本执行时是否自动的将进度信息发送给TaskTracker来避免任务没有响应被误杀，本来是当脚本输出到标准错误时，发送进度信息，但是开启该项后，输出到标准错误也不会导致信息发送，因此有可能会造成脚本有死循环产生，但是TaskTracker却没有检查到从而一直循环下去；
hive.script.serde：用户脚本转换输入到输出时的SerDe约束，默认是org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe；
hive.script.recordreader：从脚本读数据的时候的默认reader，默认是org.apache.hadoop.hive.ql.exec.TextRecordReader；
hive.script.recordwriter：写数据到脚本时的默认writer，默认org.apache.hadoop.hive.ql.exec.TextRecordWriter；
hive.input.format：输入格式，默认是org.apache.hadoop.hive.ql.io.CombineHiveInputFormat，如果出现问题，可以改用org.apache.hadoop.hive.ql.io.HiveInputFormat；
hive.udtf.auto.progress：UDTF执行时hive是否发送进度信息到TaskTracker，默认是false；
hive.mapred.reduce.tasks.speculative.execution：reduce任务推测执行是否开启，默认是true；
hive.exec.counters.pull.interval：运行中job轮询JobTracker的时间间隔，设置小会影响JobTracker的load，设置大可能看不出运行任务的信息，要去平衡，默认是1000；
hive.enforce.bucketing：数据分桶是否被强制执行，默认false，如果开启，则写入table数据时会启动分桶，
hive.enforce.sorting：开启强制排序时，插数据到表中会进行强制排序，默认false；
hive.optimize.reducededuplication：如果数据已经根据相同的key做好聚合，那么去除掉多余的map/reduce作业，此配置是文档的推荐配置，建议打开，默认是true；
hive.exec.dynamic.partition：在DML/DDL中是否支持动态分区，默认false；


hive.exec.dynamic.partition.mode：默认strict，在strict模式下，动态分区的使用必须在一个静态分区确认的情况下，其他分区可以是动态；


hive.exec.max.dynamic.partitions：动态分区的上限，默认1000；


hive.exec.max.dynamic.partitions.pernode：每个mapper/reducer节点可以创建的最大动态分区数，默认100；


hive.exec.max.created.files：一个mapreduce作业能创建的HDFS文件最大数，默认是100000；


hive.exec.default.partition.name：当动态分区启用时，如果数据列里包含null或者空字符串的话，数据会被插入到这个分区，默认名字是__HIVE_DEFAULT_PARTITION__；


hive.fetch.output.serde：FetchTask序列化fetch输出时需要的SerDe，默认是org.apache.hadoop.hive.serde2.DelimitedJSONSerDe;


hive.exec.mode.local.auto：是否由hive决定自动在local模式下运行，默认是false，

hive.exec.drop.ignorenoneexistent：在drop表或者视图时如果发现表或视图不存在，是否报错，默认是true；


hive.exec.show.job.failure.debug.info：在作业失败时是否提供一个任务debug信息，默认true；


hive.auto.progress.timeout：运行自动progressor的时间间隔，默认是0等价于forever；


hive.table.parameters.default：新建表的属性字段默认值，默认是empty空；


hive.variable.substitute：是否支持变量替换，如果开启的话，支持语法如${var} ${system:var}和${env.var}，默认是true；


hive.error.on.empty.partition：在遇到结果为空的动态分区时是否报错，默认是false；


hive.exim.uri.scheme.whitelist：在导入导出数据时提供的一个白名单列表，列表项之间由逗号分隔，默认hdfs,pfile；


hive.limit.row.max.size：字面意思理解就是在使用limit做数据的子集查询时保证的最小行数据量，默认是100000；


hive.limit.optimize.limit.file：使用简单limit查询数据子集时，可抽样的最大文件数，默认是10；


hive.limit.optimize.enable：使用简单limit抽样数据时是否开启优化选项，默认是false，关于limit的优化问题，在hive programming书中解释的是这个feature有drawback，对于抽样的不确定性给出了风险提示；


hive.limit.optimize.fetch.max：使用简单limit抽样数据允许的最大行数，默认50000，查询query受限，insert不受影响；


hive.rework.mapredwork：是否重做mapreduce，默认是false；


hive.sample.seednumber：用来区分抽样的数字，默认是0；


hive.io.exception.handlers：io异常处理handler类列表，默认是空，当record reader发生io异常时，由这些handler来处理异常；


hive.autogen.columnalias.prefix.label：当在执行中自动产生列别名的前缀，当类似count这样的聚合函数起作用时，如果不明确指出count(a) as xxx的话，那么默认会从列的位置的数字开始算起添加，比如第一个count的结果会冠以列名_c0，接下来依次类推，默认值是_c，数据开发过程中应该很多人都看到过这个别名；


hive.autogen.columnalias.prefix.includefuncname：在自动生成列别名时是否带函数的名字，默认是false；


hive.exec.perf.logger：负责记录客户端性能指标的日志类名，必须是org.apache.hadoop.hive.ql.log.PerfLogger的子类，默认是org.apache.hadoop.hive.ql.log.PerfLogger；


hive.start.cleanup.scratchdir：当启动hive服务时是否清空hive的scratch目录，默认是false；


hive.output.file.extension：输出文件扩展名，默认是空；


hive.insert.into.multilevel.dirs：是否插入到多级目录，默认是false；


hive.files.umask.value：hive创建文件夹时的dfs.umask值，默认是0002；

hive.metastore.local：控制hive是否连接一个远程metastore服务器还是开启一个本地客户端jvm，默认是true，Hive0.10已经取消了该配置项；


javax.jdo.option.ConnectionURL：JDBC连接字符串，默认jdbc:derby:;databaseName=metastore_db;create=true；


javax.jdo.option.ConnectionDriverName：JDBC的driver，默认org.apache.derby.jdbc.EmbeddedDriver；


javax.jdo.PersisteneManagerFactoryClass：实现JDO PersistenceManagerFactory的类名，默认org.datanucleus.jdo.JDOPersistenceManagerFactory；


javax.jdo.option.DetachAllOnCommit：事务提交后detach所有提交的对象，默认是true；


javax.jdo.option.NonTransactionalRead：是否允许非事务的读，默认是true；


javax.jdo.option.ConnectionUserName：username，默认APP；


javax.jdo.option.ConnectionPassword：password，默认mine；


javax.jdo.option.Multithreaded：是否支持并发访问metastore，默认是true；


datanucleus.connectionPoolingType：使用连接池来访问JDBC metastore，默认是DBCP；


datanucleus.validateTables：检查是否存在表的schema，默认是false；


datanucleus.validateColumns：检查是否存在列的schema，默认false；


datanucleus.validateConstraints：检查是否存在constraint的schema，默认false；


datanucleus.stroeManagerType：元数据存储类型，默认rdbms；


datanucleus.autoCreateSchema：在不存在时是否自动创建必要的schema，默认是true；


datanucleus.aotuStartMechanismMode：如果元数据表不正确，抛出异常，默认是checked；


datanucleus.transactionIsolation：默认的事务隔离级别，默认是read-committed；


datanucleus.cache.level2：使用二级缓存，默认是false；


datanucleus.cache.level2.type：二级缓存的类型，有两种，SOFT:软引用，WEAK:弱引用，默认是SOFT；


datanucleus.identifierFactory：id工厂生产表和列名的名字，默认是datanucleus；


datanucleus.plugin.pluginRegistryBundleCheck：当plugin被发现并且重复时的行为，默认是LOG；


hive.metastroe.warehouse.dir：数据仓库的位置，默认是/user/hive/warehouse；


hive.metastore.execute.setugi：非安全模式，设置为true会令metastore以客户端的用户和组权限执行DFS操作，默认是false，这个属性需要服务端和客户端同时设置；


hive.metastore.event.listeners：metastore的事件监听器列表，逗号隔开，默认是空；


hive.metastore.partition.inherit.table.properties：当新建分区时自动继承的key列表，默认是空；


hive.metastore.end.function.listeners：metastore函数执行结束时的监听器列表，默认是空；


hive.metastore.event.expiry.duration：事件表中事件的过期时间，默认是0；


hive.metastore.event.clean.freq：metastore中清理过期事件的定时器的运行周期，默认是0；


hive.metastore.connect.retries：创建metastore连接时的重试次数，默认是5；


hive.metastore.client.connect.retry.delay：客户端在连续的重试连接等待的时间，默认1；


hive.metastore.client.socket.timeout：客户端socket超时时间，默认20秒；


hive.metastore.rawstore.impl：原始metastore的存储实现类，默认是org.apache.hadoop.hive.metastore.ObjectStore；


hive.metastore.batch.retrieve.max：在一个batch获取中，能从metastore里取出的最大记录数，默认是300；


hive.metastore.ds.connection.url.hook：查找JDO连接url时hook的名字，默认是javax.jdo.option.ConnectionURL；


hive.metastore.ds.retry.attempts：当出现连接错误时重试连接的次数，默认是1次；


hive.metastore.ds.retry.interval：metastore重试连接的间隔时间，默认1000毫秒；


hive.metastore.server.min.threads：在thrift服务池中最小的工作线程数，默认是200；


hive.metastore.server.max.threads：最大线程数，默认是100000；


hive.metastore.server.tcp.keepalive：metastore的server是否开启长连接，长连可以预防半连接的积累，默认是true；


hive.metastore.sasl.enabled：metastore thrift接口的安全策略，开启则用SASL加密接口，客户端必须要用Kerberos机制鉴权，默认是不开启false；


hive.metastore.kerberos.keytab.file：在开启sasl后kerberos的keytab文件存放路径，默认是空；


hive.metastore.kerberos.principal：kerberos的principal，_HOST部分会动态替换，默认是hive-metastore/_HOST@EXAMPLE.COM；


hive.metastore.cache.pinobjtypes：在cache中支持的metastore的对象类型，由逗号分隔，默认是Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order；


hive.metastore.authorization.storage.checks：在做类似drop partition操作时，metastore是否要认证权限，默认是false；


hive.metastore.schema.verification：强制metastore的schema一致性，开启的话会校验在metastore中存储的信息的版本和hive的jar包中的版本一致性，并且关闭自动schema迁移，用户必须手动的升级hive并且迁移schema，关闭的话只会在版本不一致时给出警告，默认是false不开启；



hive.index.compact.file.ignore.hdfs：在索引文件中存储的hdfs地址将在运行时被忽略，如果开启的话；如果数据被迁移，那么索引文件依然可用，默认是false；


hive.optimize.index.filter.compact.minsize：压缩索引自动应用的最小输入大小，默认是5368709120；


hive.optimize.index.filter.compact.maxsize：同上，相反含义，如果是负值代表正无穷，默认是-1；


hive.index.compact.query.max.size：一个使用压缩索引做的查询能取到的最大数据量，默认是10737418240 个byte；负值代表无穷大；


hive.index.compact.query.max.entries：使用压缩索引查询时能读到的最大索引项数，默认是10000000；负值代表无穷大；


hive.index.compact.binary.search：在索引表中是否开启二分搜索进行索引项查询，默认是true；


hive.exec.concatenate.check.index：如果设置为true，那么在做ALTER TABLE tbl_name CONCATENATE on a table/partition（有索引） 操作时，抛出错误；可以帮助用户避免index的删除和重建；


hive.stats.dbclass：存储hive临时统计信息的数据库，默认是jdbc:derby；


hive.stats.autogather：在insert overwrite命令时自动收集统计信息，默认开启true；


hive.stats.jdbcdriver：数据库临时存储hive统计信息的jdbc驱动；


hive.stats.dbconnectionstring：临时统计信息数据库连接串，默认jdbc:derby:databaseName=TempStatsStore;create=true；


hive.stats.defaults.publisher：如果dbclass不是jdbc或者hbase，那么使用这个作为默认发布，必须实现StatsPublisher接口，默认是空；


hive.stats.defaults.aggregator：如果dbclass不是jdbc或者hbase，那么使用该类做聚集，要求实现StatsAggregator接口，默认是空；


hive.stats.jdbc.timeout：jdbc连接超时配置，默认30秒；


hive.stats.retries.max：当统计发布合聚集在更新数据库时出现异常时最大的重试次数，默认是0，不重试；


hive.stats.retries.wait：重试次数之间的等待窗口，默认是3000毫秒；


hive.client.stats.publishers：做count的job的统计发布类列表，由逗号隔开，默认是空；必须实现org.apache.hadoop.hive.ql.stats.ClientStatsPublisher接口；


hive.client.stats.counters：没什么用~~~


hive.security.authorization.enabled：hive客户端是否认证，默认是false；


hive.security.authorization.manager：hive客户端认证的管理类，默认是org.apache.hadoop.hive.ql.security.authorization.DefaultHiveAuthorizationProvider；用户定义的要实现org.apache.hadoop.hive.ql.security.authorization.HiveAuthorizationProvider；


hive.security.authenticator.manager：hive客户端授权的管理类，默认是org.apache.hadoop.hive.ql.security.HadoopDefaultAuthenticator；用户定义的需要实现org.apache.hadoop.hive.ql.security.HiveAuthenticatorProvider；


hive.security.authorization.createtable.user.grants：当表创建时自动授权给用户，默认是空；


hive.security.authorization.createtable.group.grants：同上，自动授权给组，默认是空；


hive.security.authorization.createtable.role.grants：同上，自动授权给角色，默认是空；


hive.security.authorization.createtable.owner.grants：同上，自动授权给owner，默认是空；


hive.security.metastore.authorization.manager：metastore的认证管理类，默认是org.apache.hadoop.hive.ql.security.authorization.DefaultHiveMetastoreAuthorizationProvider；用户定义的必须实现org.apache.hadoop.hive.ql.security.authorization.HiveMetastoreAuthorizationProvider接口；接口参数要包含org.apache.hadoop.hive.ql.security.authorization.StorageBasedAuthorizationProvider接口；使用HDFS的权限控制认证而不是hive的基于grant的方式；


hive.security.metastore.authenticator.manager：metastore端的授权管理类，默认是org.apache.hadoop.hive.ql.security.HadoopDefaultMetastoreAuthenticator，自定义的必须实现org.apache.hadoop.hive.ql.security.HiveAuthenticatorProvider接口；


hive.metastore.pre.event.listeners：在metastore做数据库任何操作前执行的事件监听类列表；



fs.har.impl：访问Hadoop Archives的实现类，低于hadoop 0.20版本的都不兼容，默认是org.apache.hadoop.hive.shims.HiveHarFileSystem；


hive.archive.enabled：是否允许归档操作，默认是false；


hive.archive.har.parentdir.settable：在创建HAR文件时必须要有父目录，需要手动设置，在新的hadoop版本会支持，默认是false；


hive.support.concurrency：hive是否支持并发，默认是false，支持读写锁的话，必须要起zookeeper；


hive.lock.mapred.only.operation：控制是否在查询时加锁，默认是false；


hive.lock.numretries：获取锁时尝试的重试次数，默认是100；


hive.lock.sleep.between.retries：在重试间隔的睡眠时间，默认60秒；


hive.zookeeper.quorum：zk地址列表，默认是空；


hive.zookeeper.client.port：zk服务器的连接端口，默认是2181；


hive.zookeeper.session.timeout：zk客户端的session超时时间，默认是600000；


hive.zookeeper.namespace：在所有zk节点创建后的父节点，默认是hive_zookeeper_namespace；


hive.zookeeper.clean.extra.nodes：在session结束时清除所有额外node；


hive.cluster.delegation.token.store.class：代理token的存储实现类，默认是org.apache.hadoop.hive.thrift.MemoryTokenStore，可以设置为org.apache.hadoop.hive.thrift.ZooKeeperTokenStore来做负载均衡集群；


hive.cluster.delegation.token.store.zookeeper.connectString：zk的token存储连接串，默认是localhost:2181；


hive.cluster.delegation.token.store.zookeeper.znode：token存储的节点跟路径，默认是/hive/cluster/delegation；


hive.cluster.delegation.token.store.zookeeper.acl：token存储的ACL，默认是sasl:hive/host1@example.com:cdrwa,sasl:hive/host2@example.com:cdrwa；


hive.use.input.primary.region：从一张input表创建表时，创建这个表到input表的主region，默认是true；


hive.default.region.name：默认region的名字，默认是default；


hive.region.properties：region的默认的文件系统和jobtracker，默认是空；


hive.cli.print.header：查询输出时是否打印名字和列，默认是false；


hive.cli.print.current.db：hive的提示里是否包含当前的db，默认是false；


hive.hbase.wal.enabled：写入hbase时是否强制写wal日志，默认是true；


hive.hwi.war.file：hive在web接口是的war文件的路径，默认是lib/hive-hwi-xxxx(version).war；


hive.hwi.listen.host：hwi监听的host地址，默认是0.0.0.0；


hive.hwi.listen.port：hwi监听的端口，默认是9999；


hive.test.mode：hive是否运行在测试模式，默认是false；


hive.test.mode.prefix：在测试模式运行时，表的前缀字符串，默认是test_；


hive.test.mode.samplefreq：如果hive在测试模式运行，并且表未分桶，抽样频率是多少，默认是32；


hive.test.mode.nosamplelist：在测试模式运行时不进行抽样的表列表，默认是空；
&lt;/code&gt;&lt;/pre&gt;
</description>
        <pubDate>Sun, 20 Sep 2015 00:00:00 +0800</pubDate>
        <link>index.html/blog/2015/09/20/hiveconfjilu.html</link>
        <guid isPermaLink="true">index.html/blog/2015/09/20/hiveconfjilu.html</guid>
        
        <category>nosql</category>
        
      </item>
    
      <item>
        <title>HDFS 分层存储</title>
        <description>&lt;p&gt;Hadoop  &lt;br /&gt;
众所周知，商用硬件可以组装起来创建拥有大数据存储和计算能力的Hadoop集群。将数据拆分成多个部分，分别存储在每个单独的机器上，数据处理逻辑也在同样的机器上执行。   &lt;/p&gt;

&lt;p&gt;例如：一个1000个节点组成的Hadoop集群，单节点容量有20TB，最多可以存储20PB的数据。因此，所有的这些机器拥有足够的计算能力来履行Hadoop的口号：“take compute to data”。 &lt;br /&gt;
数据的温度 &lt;br /&gt;
集群中通常存储着各种不同类型的数据集，不同的团队不同的业务通过该集群可以共享地处理他们不同类型的工作任务。通过数据管道，每个数据集每时每刻都会得到增长。 &lt;br /&gt;
数据集有一个共同特点就是初始的使用量会很大。在此期间，数据集被认为是“热(HOT)”的。我们通过分析发现，随着时间的推移，使用率会有一定程度的下降，存储的数据每周仅仅就被访问几次，逐渐就变为“温(WARM)”数据。在此后90天中，当数据使用率跌至一个月几次时，它就被定义为“冷(COLD)”数据。&lt;br /&gt;
因此数据在最初几天被认为是“热”的，此后第一个月仍然保持为“温”的。在这期间，任务或应用会使用几次该数据。随着数据的使用率下降得更多，它就变“冷”了，在此后90天内或许只被使用寥寥几次。最终，当数据一年只有一两次使用频率、极少用到时，它的“温度”就是“冰冻”的了。   &lt;/p&gt;

&lt;p&gt;一般来讲，温度与每个数据集都紧密相关。在这个例子中，温度是与数据的年龄成反比的。一个特定数据集的温度也受其他因素影响的。你也可以通过算法决定数据集的温度。&lt;br /&gt;
HDFS的分层存储&lt;br /&gt;
HDFS从Hadoop2.3开始支持分层存储&lt;br /&gt;
它是如何工作的呢？ &lt;br /&gt;
正常情况下，一台机器添加到集群后，将会有指定的本地文件系统目录来存储这块副本。用来指定本地存储目录的参数是dfs.datanode.dir。另一层中，比如归档(ARCHIVE)层，可以使用名为StorageType的枚举来添加。为了表明这个本地目录属于归档层，该本地目录配置中会带有[ARCHIVE]的前缀。理论上，hadoop集群管理员可以定义多个层级。 &lt;br /&gt;
比如说：如果在一个已有1000个节点，其总存储容量为20PB的集群上，增加100个节点，其中每个节点有200TB的存储容量。相比已有的1000个节点，这些新增节点的计算能力就相对较差。接下来，我们在所有本地目录的配置中增加ARCHIVE的前缀。那么现在位于归档层的这100个节点将会有20PB的存储量。最后整个集群被划分为两层——磁盘(DISK)层和归档(ARCHIVE)层，每一层有20PB的容量，总容量为40PB。   &lt;/p&gt;

&lt;p&gt;基于温度将数据映射到存储层 &lt;br /&gt;
在这个例子中，我们将在拥有更强计算能力节点的DISK层存储高频率使用的“热(HOT)”数据。 &lt;br /&gt;
至于“温(WARM)”数据，我们将其大部分的副本存储在磁盘层。对于复制因子(replication factor)为3的数据，我们将在磁盘层存储其两个副本，在归档层存储一个副本。 &lt;br /&gt;
如果数据已经变“冷(COLD)”,那么我们至少将在磁盘层存储其每个块的一个副本。余下的副本都放入归档层。   &lt;/p&gt;

&lt;p&gt;当一个数据集为认为是“冰冻(FROZEN)”的,这就意味着它几乎已经不被使用，将其存储在具有大量CPU、能执行众多任务节点或容器的节点上是不明智的。我们会把它存储到一个具有最小计算能力的节点上。因此，所有处于“冰冻(FROZEN)”状态块的全部副本都可以被移动到归档层。 &lt;br /&gt;
跨层的数据流 &lt;br /&gt;
当数据第一次添加到集群中，它将被存储到默认的磁盘层。基于数据的温度，它的一个或多个副本将被移动到归档层。移动器就是用来把数据从一个层移动到另一层的。移动器的工作原理类似平衡器，除了它可以跨层地移动块的副本。移动器可接受一条HDFS路径，一个副本数目和目的地层信息。然后它将基于所述层的信息识别将要被移动的副本，并调度数据在源数据节点到目的数据节点的移动。&lt;br /&gt;
Hadoop 2.6中支持分层存储的变化 &lt;br /&gt;
Hadoop 2.6中有许多的改进使其能够进一步支持分层存储。你可以附加一个存储策略到某个目录来指明它是“热(HOT)”的,“温(WARM)”的,“冷(COLD)”的, 还是“冰冻(FROZEN)”的。存储策略定义了每一层可存储的副本数量。我可以改变目录的存储策略并启动该目录的移动器来使得策略生效。 &lt;br /&gt;
使用数据的应用 &lt;br /&gt;
基于数据的温度，数据的部分或者全部副本可能存储在任一层中。但对于通过HDFS来使用数据的应用而言，其位置是透明的。 &lt;br /&gt;
虽然“冰冻”数据的所有副本都在归档层，应用依然可以像访问HDFS的任何数据一样来访问它。由于归档层中的节点并没有计算能力，运行在磁盘层的映射(map)任务将从归档层的节点上读取数据，但这会导致增加应用的网络流量消耗。如果这种情况频繁地发生，你可以指定该数据为“温/冷”,并让移动器移回一个或多个副本到磁盘层。 &lt;br /&gt;
确定数据温度以及完成指定的副本移动至预先定义的分层存储可以全部自动化。 &lt;br /&gt;
总结 &lt;br /&gt;
无计算能力的存储比有计算能力的存储要便宜。我们可以依据数据的温度来确保具计算能力的存储能得到充分地使用。因为每一个分块的数据都会被复制多次（默认是3次），根据数据的温度，许多副本都会被移动到低成本的存储中。HDFS支持分层存储并提供必要的工具来进行跨层的数据移动。   &lt;/p&gt;
</description>
        <pubDate>Sun, 20 Sep 2015 00:00:00 +0800</pubDate>
        <link>index.html/blog/2015/09/20/hdfs-fenceng.html</link>
        <guid isPermaLink="true">index.html/blog/2015/09/20/hdfs-fenceng.html</guid>
        
        <category>hadoop</category>
        
      </item>
    
      <item>
        <title>大数据处理的关键架构层</title>
        <description>&lt;p&gt;&lt;img src=&quot;/image/大数据处理的关键架构层.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
</description>
        <pubDate>Sun, 20 Sep 2015 00:00:00 +0800</pubDate>
        <link>index.html/blog/2015/09/20/guanjianjiagouceng.html</link>
        <guid isPermaLink="true">index.html/blog/2015/09/20/guanjianjiagouceng.html</guid>
        
        <category>hadoop</category>
        
      </item>
    
      <item>
        <title>多源ETL考虑</title>
        <description>&lt;p&gt;ETL解决问题：&lt;br /&gt;
1.数据分散问题&lt;br /&gt;
2.数据不清洁问题&lt;br /&gt;
3.对数据格式不统一  &lt;/p&gt;

&lt;p&gt;ETL 过程模型&lt;br /&gt;
1. 元数据库&lt;br /&gt;
 元数据（metadata）是定义和描述其它数据的数据    关于数据内容、质量、状况和其他特性的信息，在整个数据抽取转换加载过程中起到基础的作用。元数据使用户可以掌握数据的历史情况，如数据从哪里来，流通时间多长，更新频率是多大，数据元素的含义是什么，对它已经进行了哪些计算、转换和筛选等等。&lt;br /&gt;
    元数据具有下列属性：&lt;br /&gt;
（1）描述性，元数据是描述数据的数据，这是元数据的最本质的特征。&lt;br /&gt;
（2）动态性，元数据不是静止不变的，它随着所描述对象的变化而变化。&lt;br /&gt;
（3）多样性，元数据的类型多样。&lt;br /&gt;
（4）复杂性，一方面元数据既可以是集合概念也可以是个体概念，元数据中还可以包括其它的元数据；另一方面对不同的描述对象，有些元数据项是必须有的，而有些却不一定强求，即强制性的元数据与选择性的元数据共存。&lt;br /&gt;
（5）多层次性，这是由元数据所描述对象的多层次和元数据使用对象的多层次性决定的。&lt;br /&gt;
（6）支撑性，元数据相对描述对象而言处于次要的地位，但又是必不可少的，起支撑的作用。&lt;br /&gt;
    元数据库是以一定的组织方式存储在一起的相关的元数据集合。  &lt;/p&gt;

&lt;p&gt;元数据可以应该包括下列7个组件。 &lt;br /&gt;
    （1）环境状况组件。环境状况组件主要是用于监控网络和源数据的状况，它包括：网络状态，各种源数据状态，最佳抽取时间。网络状态和各种源数据状态的数据可以通过定时对网络状况和源数据状况探测自动获取。通过对这两组数据的历史记录分析可以生成最佳抽取时间。&lt;br /&gt;
    （2）基本组件。基本组件包括数据源数据库表结构、数据源数据库表属性、数据仓库表结构、数据仓库表属性，等等。基本构件和其它元数据最大的区别在于它是具有版本标识的数据，具有版本标识的数据在很长的一段时间内可以跟踪数据的变化情况。基本构件主要是对源数据的特征进行描述，它包括：可以提供源数据的数据库名，数据库编号，这些数据库的表，表的编号，表中的属性，属性的编号，以及可以提供源数据的文件系统的文件类型、分隔符、转换为数据库系统中目标表的表名等。&lt;br /&gt;
    （3）数据状态组件。数据状态组件用于标识数据仓库中的数据是“活性”的还是“惰性”的。由于数据仓库中的数据都是基于共享设计的，因而当将数据仓库中的数据作为源数据进行抽取和转换时，其中的某些数据可能包括一些误导信息，因而对于这些表就需要数据状态字段对它进行控制。 &lt;br /&gt;
    （4）存取模式组件。存取模式组件是用于确定异构的源数据什么时候将什么数据迁移到数据仓库中，它包括：存取数据的类型、总数以及频率等。在并行环境下，它可以确定如何物理地分离数据，这样可以极大地提高数据传送的效率。 &lt;br /&gt;
    （5）数据质量要求组件。数据质量规则定义了源数据中的质量要求，它包括了数据源的编号、错误类型编号、可能的修改规则编号等。 &lt;br /&gt;
    （6）映射规则组件。映射规则定义了数据由数据源到数据仓库映射的规则，它包括：源字段的编号；简单的属性到属性的映射；字段类型的转换；多个源表到一个目标表之间复杂的转换；命名的改变；关键字的改变；等等。 &lt;br /&gt;
    （7）抽取日志组件。抽取日志组件记录了对数据仓库中的数据进行的每次操作的时间、操作方式、操作过程以及结果。这些信息对于数据仓库的维护非常有用，拥有这些信息可以对ETL过程中的每一步进行监控。 &lt;br /&gt;
 &lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;数据预抽取&lt;br /&gt;
    按照元数据定义的内容、频率和规则，将保存在有关数据源中的数据抽取出来，存放到另外的数据库中，并将预抽取操作记录在元数据库中。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;数据质量检验 &lt;br /&gt;
    数据质量是数据使用的适合性。数据质量要求是关于数据明示的、通常隐含的或必须履行的需求或期望。
    数据质量检验是依据元数据中定义的各数据质量要求，通过判断，对数据与质量要求的符合性进行评价，并将数据质量检验操作记录在元数据库中。
    数据质量主要有两个方面的问题：一个是单数据源数据质量问题，另一个是多数据源的数据交互集成时的数据质量问题。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

</description>
        <pubDate>Sun, 20 Sep 2015 00:00:00 +0800</pubDate>
        <link>index.html/blog/2015/09/20/duoyuan-ETL.html</link>
        <guid isPermaLink="true">index.html/blog/2015/09/20/duoyuan-ETL.html</guid>
        
        <category>hadoop</category>
        
      </item>
    
  </channel>
</rss>
