<!DOCTYPE HTML>
<html>
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0">
  <meta name="description" content="Will go, Just do it.">
  <meta name="author" content="Will.Quan">
  <meta name="keywords" content="Will go, Just do it., willgo, 最好的从未错过, Will.Quan">
  <title>Will go, Just do it.</title>
  <link rel="canonical" href="index.html">
  <link rel="icon" href="/res/img/favicon.ico" type="image/x-icon">
  <link rel="shortcut icon" href="/res/img/favicon.ico" type="image/x-icon">
  <link rel="stylesheet" href="/res/css/public.css">
  <link rel="stylesheet" href="/res/css/light.css">
  <script src="/res/js/light.js"></script>
  <script type="text/javascript">
var _bdhmProtocol = (("https:" == document.location.protocol) ? " https://" : " http://");
document.write(unescape("%3Cscript src='" + _bdhmProtocol + "hm.baidu.com/h.js%3Fda48b6233123178f300913a0e707883e' type='text/javascript'%3E%3C/script%3E"));
</script>

</head>
<body>
  <div id="blog">
    <div class="sidebar">
  <div class="profilepic">
    <a href="/"><img src="/res/img/icon.png" alt="logo"></img></a>
  </div>
  <h1 class="title"><a href="/">willgo</a></h1>
  <h2 class="sub-title">最好的从未错过</h2>
  <nav id="nav">
    <ul>
    
      <li><a href="/page/timing.html"><i class="fa fa-clock-o"></i>&nbsp;资料时间</a></li>
    
      <li><a href="/page/category.html"><i class="fa fa-tags"></i>&nbsp;文章分类</a></li>
    
      <li><a href="/page/read.html"><i class="fa fa-book"></i>&nbsp;逗绊读书</a></li>
    
      <li><a href="/page/life.html"><i class="fa fa-eyedropper"></i>&nbsp;生活记录</a></li>
    
      <li><a href="/page/about.html"><i class="fa fa-paper-plane-o"></i>&nbsp;假装关于</a></li>
    
    </ul>
  </nav>  
  <nav id="sub-nav">
    <a class="weibo " href="http://weibo.com/u/2369015654" title="新浪微博" target="_blank"><i class="fa fa-weibo"></i></a>
    <a class="github" href="https://github.com/will0815/" title="GitHub" target="_blank"><i class="fa fa-github fa-2x"></i></a>
    <a class="rss" href="/page/feed.xml" title="RSS订阅" target="_blank"><i class="fa fa-rss"></i></a>
  </nav>
  <div id="license">
    <a href="http://creativecommons.org/licenses/by-nc-sa/3.0/cn/" target="_blank" title="本站所有作品采用：&#10;知识共享《署名 非商业性使用 相同方式共享 3.0》&#10;进行许可" >
    <img alt="License" height="31" width="88" src="/res/img/license.png" /></a>
  </div>
</div>

    <div class="main">
        <div style="font-family: segoepr;font-size: 40px;
              margin-top:-5px; color:#fff;">
            <script type="text/javascript" 
            src="http://open.iciba.com/ds_open.php?id=11519&name=willgo&auth=BE45CDE6481CC46336529A1B407855B1" charset="utf-8">
            </script>
        </div>
    
<div class="posts">
  <section class="post">
    <header class="post-header">
      <h1 class="post-title"><a href="/blog/2015/09/20/data-store-platform.html">数据平台拓展</a></h1>
      <p class="post-meta">
        <i class="fa fa-calendar"></i>
        2015年09月20日
        <i class="space"></i>
        <i class="fa fa-tags"></i>
        <a class="post-category" href="/page/category.html#nosql">nosql</a>
      </p>
    </header>
    <div class="post-main">
      <h3 id="section">一、数据的位置</h3>
<p>数据存储数据存储，管理的就是数据的位置，在CPU的位置，数据相对其他数据的位置，CPU善于处理顺序性操作数据指令，即：数据预取。<br />
随机读取操作即成为瓶颈，预取到缓存、前端总线的数据都是无效的。传统意义上说，磁盘的存取性能要弱于内存，但是要分随机存取及顺序存取不同的场景下讨论。<br />
#####1、数据存储和更新<br />
追加写可以让我们尽量保持顺序存储文件。但是当数据要进行更新的时候，有两种选择，一种是在数据原地进行更新操作，这样我们就有了随机IO操作。另一种是把更新都放到文件末尾，然后需要读取更新数据的时候进行替换。<br />
#####2、读取数据<br />
一下子读取整个文件，也是很耗费时间的事情，例如数据库中的全表扫描。当我们读取文件中某一个字段时候，我们需要索引。索引的方式有多种，我们可以用一种简单的固定数值大小的有序数组来做索引，数组里存的是当前数据在文件中的存储偏移量。其他索引技术，如hash索引，位图索引等。<br />
索引相当于在数据之上又加了一层树状结构，可以迅速的读取数据。但是打破了我们前面讲的数据的追加写，这些数据都是根据索引随机写入的。在数据库上建立索引的时候都会遇到这个问题，在传统的机械式磁盘上，这个问题会造成千倍的性能差异。  <br />
有三种方法可以解决上述问题：<br />
1）把索引放到内存中，可以随机存储和读取，把数据顺序存储到硬盘上。MongoDB，Cassandra都是采取这种方式。这种方式有一个弊端是存储的数据量受限于内存的大小，数据量一大，索引也增大，数据就饱和了。   </p>

<p>2）第二种方式是把大的索引结构，拆成很多小的索引来存储。在内存中批量进来的数据，当积累到一个预定的量，就排序然后顺序写到磁盘上，本身就是一个小的索引，数据存储完，最后加一块小的全局索引数据即可。这样读取数据的时候，要遍历一些小的索引，会有随机读取。本质是用部分小的随机读换取了整体的数据顺序存储。我们通过在内存中保存一个元索引或者Bloom filter来实现处理那些小索引的低延迟。<br />
日志结构的归并树（log structed merge tree）是一种典型的实现，
	有三个特征：<br />
	a)一组小的、不变的索引集。 <br />
	b)只能追加写 ，合并重复的文件。 <br />
	c)少量的内存索引消耗换来读取的性能提升。这是一种写优化索引结构。HBase、Cassandra、Bigtable都是通过这种比较小的内存开销来实现读取和存储的平衡。  </p>

<p>3）列式存储或者面向列的存储。纯列式存储和谷歌bigtable那种列式存储还是有所不同的，，虽然占用了同一个名字。列式存储很好理解，就是把数据按照列顺序存储到文件中，读取的时候只读需要的列。列式存储需要保持每一列数据都有相同的顺序，即行N在每一列都有相同的偏移。这很重要，因为同一查询中可能要返回多个列的数据，同时可能我们要对多列直接进行连接。每一列保持同样的顺序我们可以用非常简单的循环实现上述操作，且都是高效的CPU和缓存操作。<br />
列式存储的缺点是更新数据的时候需要更新每一个列文件中的相应数据，一个常用的方法就是类似LSM那种批量内存写的方式。当查询只是返回某几列数据，列式存储可以大规模减少磁盘IO。除此之外，列式存储的数据往往属于同一类型，可以进行高效的压缩，一些低延迟，高压缩率的扫描宽度、位填充算法都试用。即使对于未压缩的数据流，同时可以进行针对其编码格式的预取。 <br />
列式存储尤其适用于大表扫描，求均值、最大最小值、分组等聚合查询场景。列式存储天然的保持了一列中数据的顺序性，方便两列数据进行关联，而heap-file index结构关联时候，一份数据可以按顺序读取，则另一份数据就会有随机读取了。<br />
典型优势总结：<br />
1）列式压缩，低IO <br />
2）列中每行数据保持顺序，可以按照行id进行关联合并<br />
3）压缩后的数据依然可以进行预取<br />
4）数据延迟序列化<br />
通过heap-file结构把索引存储在内存，是很多NoSQL数据库及一些关系型数据库的首选，例如Riak，CouchBase和MongoDB，模型简单并且运行良好。 <br />
要处理更大量的数据，LSM技术应用更为广泛，提供了同时满足高效存储和读取效率的基于磁盘的存取结构。HBase、Cassandra、RocksDB, LevelDB，甚至MongoDB最新版也支持这种技术。   </p>

<p>列式存储在MPP数据库里面应用广泛，例如RedShift、Vertica及hadoop上的Parquet等。这种结构适合需要大表扫描的数据处理问题，数据聚合类操作（最大最小值）更是他的主战场。  </p>

<h3 id="section-1">二、并行化</h3>
<p>把数据放到分布式集群中运算，有两点最为重要：分区（partition）和副本（replication）。
分区又被称为sharding，在随机访问和暴力扫描任务下都表现不错。通过hash函数把数据分布到多个机器上，很像单机上使用的hashtable，只不过这儿每一个桶都被放到了不同的机器上。这样可以通过hash函数直接去存储数据的机器上把数据取出来，这种模式有很强的扩展性，也是唯一可以根据客户端请求数线性扩展的模式。请求会被独立分发到某一机器上单独处理。  <br />
我们通过分区可以实现批量任务的并行化，例如聚合函数或者更复杂的聚类或者其他机器学习算法，我们通过广播的方式在所有机器上使任务同时执行。我们还可以运行分治策略来使得高计算的任务在一个更短的时间内解决。批处理系统处理大型的计算问题有不错的效果，但是它的并发性不好，因为执行任务的时候会非常消耗集群的资源。所以分区方式在两个极端情况非常简单：1）直接hash访问 2）广播，然后分而治之，在这两种情况之间还有中间地带，那就是在NoSQL数据库中常用的二级索引技术。</p>

<p>二级索引是指不是构建在主键上的索引，意味着数据不会因为索引的值而进行分区。不能直接通过hash函数去路由到数据本身。我们必须把请求广播到所有节点上，这样会限制了并发性，每一个请求都会卷入所有的节点。因此好多基于key-value的数据库拒绝引入二级索引，虽然它很有价值，例如Hbase和Voldemort。也有些数据库系统包含它了，因为它有用，例如Cassandra、MongoDB、Riak等。重要的是我们要理解好他的效益及他对并发性所造成的影响。        <br />
解决上述并发性瓶颈的一个途径是数据副本，例如异步从数据库和Cassandra、MongoDB中的数据副本。实际上副本数据可以是透明的（只是数据恢复时候使用）、只读的（增加读的并发性），可读写的（增加分区容错性）。这些选择都会对系统的一致性造成影响。 </p>

<p>这些对一致性的折中，给我们带来一个值得思考的问题？一致性到底有什么用？实现一致性的代价非常昂贵。在数据库中是用串行化来保证ACID的。他的基本保证是所有操作都是顺序排列的。这样实现起来的代价非常昂贵，所以好多关系型数据库也不把他当成默认选项。所以说要想在包含分布式写操作的系统上实现强一致性，如同坠入深渊。
解决一致性问题的方案也很简单，避免他。假如不能避免它把他隔离到尽可能少的写入和尽可能少的机器上。<br />
当然有些业务场景是必须要保证数据一致性的，例如银行转账时候。有些业务场景感觉上是必须保持一致性的，但实际上不是，例如标记一个交易是否有潜在的欺诈，我们可以先把它更新到一个新的字段里面，另外再用一条单独的记录数据去关联最开始的那个交易。所以对一个数据平台来说有效的方式是去避免或者孤立需要一致性的请求，一种孤立的方法是采取单一写入者的策略，Datamic就是典型的例子。另一种物理隔离的方法就是去区分请求中可变和不可变的字段，分别查询。Bloom/CALM把这种理念走的更远，默认的配置选项就是无序执行的策略，只有在必要的时候才启用顺序执行读写语句。</p>

      <div class="readall"><a href="/blog/2015/09/20/data-store-platform.html" id="post-readall">阅读全文&nbsp;<i class="fa fa-chevron-right"></i></a></div>
    </div>
  </section>
</div>

<div class="posts">
  <section class="post">
    <header class="post-header">
      <h1 class="post-title"><a href="/blog/2015/09/20/dashujupingtaisheji.html">基于Hadoop生态的企业数据中心考虑</a></h1>
      <p class="post-meta">
        <i class="fa fa-calendar"></i>
        2015年09月20日
        <i class="space"></i>
        <i class="fa fa-tags"></i>
        <a class="post-category" href="/page/category.html#hadoop">hadoop</a>
      </p>
    </header>
    <div class="post-main">
      <p>一．面临的问题<br />
如今的大数据已经炒得妇孺皆知但实际上真正在企业实践中能够发挥其优点并从中获益的很少。大数据的挑战与机遇并存，大数据的发展将从预期膨胀、炒作阶段转入将逐渐理性发展、落地应用阶段，不可否认大数据是必然趋势，未来的大数据发展依然存在诸多挑战。 <br />
目前大数据的发展依然存在诸多挑战，如：<br />
1.各业务部门没有清晰的大数据需求导致数据资产流失；<br />
2.内部数据孤岛严重，数据价值不能充分挖掘；<br />
3.数据可用性低，质量差，利用效率低；<br />
4.数据管理技术和架构无法满足大数据要求；<br />
5.没有明确的大数据构想；<br />
大数据现阶段仍在起步阶段，存在诸多挑战，但未来的发展依然非常乐观。大数据的发展呈现的趋势也逐渐清晰：<br />
1.数据资源化，将成为企业主要资产；<br />
2.将全面落地于企业业务各个方面；<br />
3.与传统数据分析融合，定制分析方案；<br />
4.数据安全的重要性更加明显；<br />
二. 为何选择Hadoop生态<br />
1.成本低;<br />
2.生态圈成熟,社区活跃;<br />
3.能解决大多数大数据需求。  </p>

<p>三.实施步骤<br />
1.讨论确定数据分析的应用方向<br />
A.数据支持的业务优化(如：根据用户行为热点调整用户界面)<br />
B.独立数据分析应用(如：投资组合推荐TOP100)<br />
C.大数据精准运营(如：用户流失预警模型)<br />
D.决策支持(如：用户来源细分)<br />
E.。。。<br />
2.统计各产品现有统计需求<br />
明确当前公司的数据分析环境。。。<br />
3.召集各部门产品负责人讨论，协助明确大数据需求<br />
明确大数据能做什么不能做什么<br />
4.确定分析数据源<br />
A.业务数据<br />
B.SDK收集数据<br />
C.日志文件<br />
D.网络爬虫<br />
5.选择确定技术方案<br />
以Hadoop为基础，利用Hive/HBase/Sqoop/Flume/MR/Mahout/Spark/Storm/MQ/Mysql/
技术整体实现<br />
6.设计实施技术架构<br />
A.数据收集（ETL）<br />
以Sqoop/MR 从业务数据库拉取数据到数据中心(定期)<br />
SDK收集数据直接推送至数据消息队列随后持久化到数据中心(统一开发各平台数据收集SDK，保证收集数据一致性)<br />
爬虫数据保存至数据中心()<br />
各种数据融合消除数据孤岛()<br />
B.数据中心设计<br />
数据中心以HDFS为基础<br />
Hive构建历史数据仓库<br />
	Hive数据存储设计：<br />
	数据只添加不更新/删除，数据以大表的形式存储？ Hive数据仓库设计？<br />
HBase存储近期数据满足准实时分析需求<br />
分析数据以MySQL形式形成满足不同业务部需求的数据集市<br />
C.数据分析处理<br />
	以自定义Map/Reduce作为主要数据处理工具，对数据仓库数据进行分析处理，结果推送至MySQL，形成数据集市。<br />
	以Spark/Storm 流式计算处理近实时分析需求<br />
	以Mahout作为主要数据挖掘分析工具，对数据进行进一步的分析利用<br />
D.结果数据存储(数据集市)<br />
	    以MySQL为数据库构建数据集市，面向不同业务方向提供不同主题的数据。各集市数据规模控制在千万级以内<br />
E.数据深度处理 <br />
		基于Mahout/?实现数据的深度挖掘。<br />
F.。。。<br />
7.开发具体分析 <br />
整体架构：<br />
<img src="/image/shujupingtaisheji.png" alt="" /></p>

<p>大数据的能力可以分为四层： <br />
第一层是骨骼，硬件和存储的计算能力.<br />
第二层是血液，数据建模和管理能力，包括数据如何进行采集、如何进行建模，以及如何最快的计算和最高效的存储等等; <br />
第三层是思想，业务理解和算法能力，很多算法工程师、科学家用大数据的时候可以作出很多关于用户的标签画像，其实这一部分正是他们对于业务的理解和算法能力的体现，但是仅仅这样还不够，大数据应用过 程中需要一个系统、体系完整的解决方案。 <br />
第四层是大脑，告诉你今天的数据表现是什么样，问题是什么，未来你可以做一些什么，做完之后的效果会是怎么样，因为这个效果可以 做什么样的改善，这本身就是一个非常复杂完整的体系，产品设计和服务能力，大脑进行产品的设计以及很好的服务能力。　<br />
这四者结合起来，分别表示着大数据的基础建设能力和数据产品能力。　　<br />
大数据能力背后意味着非常巨大的挑战，大数据挑战本身也是一个非常完整的闭环。　　   </p>

<p>基础建设方面，包括如何进行很好的数据采 集，如果你的数据只有几百个T或1PB左右，采集起来不那么难，如果面临几百个PB的数据如何采集，在无线时代已经到来的时候，无线的采集应该如何做?采 集之后非常重要的是计算，如果把数据采集、计算都做得很好，可是上面的应用根本没有很好的方法，快速和有效拿到这些数据来使用，其实前面所有的工作都是白费的。　　   </p>

<p>关于采集、计算、服务是大数据基础建设方面的挑战，所有这些工作最终都要在上面浮现出来，就是我们所理解的数据产品，我们希望数据产品给到大家真的不只是一个报表，而是希望我们真的用数据产品去理解 商业，理解它的目标，并且把数据不是枯燥的以一个表格的方式呈现给用户，而是告诉你这个数据的含义是什么，如何解读它，在这个过程中去追求数据价值的最大化。关于基础建设和数据产品价值两大块挑战的关键词就是质量、效率和价值。　　   </p>

<p>关于大数据的能力看起来非常具有想象空间，挑战也是巨大的，这是非常复杂的事情，复杂的事情一定要用复杂的方法去解决，所以我们需要有简单、清晰、明确的顶层设计。过去两年，阿里巴巴大数据实战过程 中的一个顶层设计，下面我们做好数据的基础建设，包括数据采集计算以及服务，上面我们面临的客户数据方面首先是阿里自己的小二，要有能力用好数据做数据化营运，第二块是我们非常紧密的伙伴，即商家。我们在做这件事情顶层设计时，同步考虑阿里的小二和商家如何同时用好数据。数据基础建设方面做好one data和one service，数据上面一块做好对内和对外能够共享的one Platform。解决的是关于大数据本身背后的挑战，基础建设和价值两块的挑战。    </p>


      <div class="readall"><a href="/blog/2015/09/20/dashujupingtaisheji.html" id="post-readall">阅读全文&nbsp;<i class="fa fa-chevron-right"></i></a></div>
    </div>
  </section>
</div>

<div class="posts">
  <section class="post">
    <header class="post-header">
      <h1 class="post-title"><a href="/blog/2015/09/20/10-suanfa-biji.html">C4.5+聚类+SVM+关联+EM+PageRank+迭代+kNN+NB+分类笔记</a></h1>
      <p class="post-meta">
        <i class="fa fa-calendar"></i>
        2015年09月20日
        <i class="space"></i>
        <i class="fa fa-tags"></i>
        <a class="post-category" href="/page/category.html#hadoop">hadoop</a>
      </p>
    </header>
    <div class="post-main">
      <ol>
  <li>
    <p>C4.5算法<br />
C4.5是做什么  <br />
C4.5 以决策树的形式构建了一个分类器。<br />
什么是分类器  <br />
分类器是进行数据挖掘的一个工具，它处理大量需要进行分类的数据，并尝试预测新数据所属的类别。
ru:假定一个包含很多病人信息的数据集。我们知道每个病人的各种信息，比如年龄、脉搏、血压、最大摄氧量、家族病史等。这些叫做数据属性。<br />
现在：  <br />
给定这些属性，我们想预测下病人是否会患癌症。病人可能会进入下面两个分类：会患癌症或者不会患癌症。算法负责告诉我们每个病人的分类。<br />
做法：<br />
用一个病人的数据属性集和对应病人的反馈类型，C4.5构建了一个基于新病人属性预测他们类型的决策树。<br />
什么是决策树呢？  <br />
决策树学习是创建一种类似与流程图的东西对新数据进行分类。  <br />
基本原则：<br />
流程图的每个环节都是一个关于属性值的问题，并根据这些数值，病人就被分类了。你可以找到很多决策树的例子。<br />
这属于监督学习算法，因为训练数据是已经分好类的。使用分好类的病人数据，C4.5算法不需要自己学习病人是否会患癌症。  <br />
C4.5算法和决策树系统区别  <br />
首先，C4.5算法在生成信息树的时候使用了信息增益。  <br />
其次，尽管其他系统也包含剪枝，C4.5使用了一个单向的剪枝过程来缓解过渡拟合。剪枝给结果带来了很多改进。 再次，C4.5算法既可以处理连续数据也可以处理离散数据。最后，不完全的数据用算法自有的方式进行了处理。  <br />
决策树最好的卖点是他们方便于翻译和解释。他们速度也很快，是种比较流行的算法。输出的结果简单易懂。  </p>
  </li>
  <li>
    <p>k 均值聚类算法<br />
K-聚类算法从一个目标集中创建多个组，每个组的成员都是比较相似的。这是个想要探索一个数据集时比较流行的聚类分析技术。<br />
什么是聚类分析<br />
聚类分析属于设计构建组群的算法，这里的组成员相对于非组成员有更多的相似性。在聚类分析的世界里，类和组是相同的意思。<br />
如：假设我们定义一个病人的数据集。在聚类分析里，这些病人可以叫做观察对象。我们知道每个病人的各类信息，比如年龄、血压、血型、最大含氧量和胆固醇含量等。这是一个表达病人特性的向量。<br />
基本认为一个向量代表了我们所知道的病人情况的一列数据。这列数据也可以理解为多维空间的坐标。脉搏是一维坐标，血型是其他维度的坐标等等。  <br />
Kmeans算法更深层次的这样处理问题： <br />
1.k-means 算法在多维空间中挑选一些点代表每一个 k 类。他们叫做中心点。<br />
2.每个病人会在这 k 个中心点中找到离自己最近的一个。我们希望病人最靠近的点不要是同一个中心点，所以他们在靠近他们最近的中心点周围形成一个类。<br />
3.我们现在有 k 个类，并且现在每个病人都是一个类中的一员。 <br />
4.之后k-means 算法根据它的类成员找到每个 k 聚类的中心<br />
5.这个中心成为类新的中心点。<br />
6.因为现在中心点在不同的位置上了，病人可能现在靠近了其他的中心点。换句话说，他们可能会修改自己的类成员身份。  <br />
7.重复2-6步直到中心点不再改变，这样类成员也就稳定了。这也叫做收敛性。  <br />
k-means 可以认为是非监督学习的类型。并不是指定分类的个数，也没有观察对象该属于那个类的任何信息   <br />
k-means 关键卖点是它的简单。它的简易型意味着它通常要比其他的算法更快更有效，尤其是要大量数据集的情况下更是如此。 <br />
改进：<br />
k-means 可以对已经大量数据集进行预先聚类处理，然后在针对每个子类做成本更高点的聚类分析。k-means 也能用来快速的处理“K”和探索数据集中是否有被忽视的模式或关系。  <br />
k means算法的两个关键弱点分别是它对异常值的敏感性和它对初始中心点选择的敏感性。</p>
  </li>
  <li>
    <p>支持向量机<br />
支持向量机（SVM）获取一个超平面将数据分成两类。除了不会使用决策树以外，SVM与 C4.5算法是执行相似的任务的。  <br />
超平面（hyperplane）是个函数，类似于解析一条线的方程。实际上，对于只有两个属性的简单分类任务来说，超平面可以是一条线的。 <br />
其实事实证明：
SVM 可以使用一个小技巧，把你的数据提升到更高的维度去处理。一旦提升到更高的维度中，SVM算法会计算出把你的数据分离成两类的最好的超平面。  <br />
简单的例子。我发现桌子上开始就有一堆红球和蓝球，如果这这些球没有过分的混合在一起，不用移动这些球，你可以拿一根棍子把它们分离开。
当在桌上加一个新球时，通过已经知道的棍字的哪一边是哪个颜色的球，你就可以预测这个新球的颜色了。 <br />
SVM 算法可以算出这个超平面的方程。
如果事情变得更复杂该怎么办？当然了，事情通常都很复杂。如果球是混合在一起的，一根直棍就不能解决问题了。 <br />
通过使用核函数（kernel），我们在高维空间也有很棒的操作方法。这张大纸依然叫做超平面，但是现在它对应的方程是描述一个平面而不是一条线了。<br />
那么在桌上或者空中的球怎么用现实的数据解释呢？桌上的每个球都有自己的位置，我们可以用坐标来表示。打个比方，一个球可能是距离桌子左边缘20cm 距离底部边缘 50 cm，另一种描述这个球的方式是使用坐标(x,y)或者(20,50)表达。x和 y 是代表球的两个维度。<br />
可以这样理解：如果我们有个病人的数据集，每个病人可以用很多指标来描述，比如脉搏，胆固醇水平，血压等。每个指标都代表一个维度。<br />
基本上，SVM 把数据映射到一个更高维的空间然后找到一个能分类的超平面。 <br />
类间间隔(margin)经常会和 SVM 联系起来，类间间隔是什么呢？它是超平面和各自类中离超平面最近的数据点间的距离。在球和桌面的例子中，棍子和最近的红球和蓝球间的距离就是类间间隔(margin)。 <br />
SVM 的关键在于，它试图最大化这个类间间隔，使分类的超平面远离红球和蓝球。这样就能降低误分类的可能性。 <br />
SVM 属于监督学习。因为开始需要使用一个数据集让 SVM学习这些数据中的类型。只有这样之后 SVM 才有能力对新数据进行分类。  </p>
  </li>
  <li>
    <p>Apriori 关联算法<br />
Apriori算法学习数据的关联规则(association rules)，适用于包含大量事务（transcation）的数据库。<br />
什么是关联规则<br />
关联规则学习是学习数据库中不同变量中的相互关系的一种数据挖掘技术。
举个 Apriori 算法的例子：我们假设有一个充满超市交易数据的数据库，你可以把数据库想象成一个巨大的电子数据表，表里每一行是一个顾客的交易情况，每一列代表不用的货物项。<br />
通过使用 Apriori 算法，我们就知道了同时被购买的货物项，这也叫做关联规则。它的强大之处在于，你能发现相比较其他货物来说，有一些货物更频繁的被同时购买—终极目的是让购物者买更多的东西。这些常被一起购买的货物项被称为项集（itemset）。 <br />
如：“薯条+蘸酱”和“薯条+苏打水”的组合频繁的一起出现。这些组合被称为2-itemsets。在一个足够大的数据集中，就会很难“看到”这些关系了，尤其当还要处理3-itemset 或者更多项集的时候。这正是 Apriori 可以帮忙的地方！<br />
基本的 Apriori 算法有三步：
1.参与，扫描一遍整个数据库，计算1-itemsets 出现的频率。<br />
2.剪枝，满足支持度和可信度的这些1-itemsets移动到下一轮流程，再寻找出现的2-itemsets。<br />
3.重复，对于每种水平的项集 一直重复计算，知道我们之前定义的项集大小为止。 <br />
Apriori 一般被认为是一种非监督的学习方法，因为它经常用来挖掘和发现有趣的模式和关系。  </p>
  </li>
  <li>
    <p>EM 最大期望算法 Expectation Maximization<br />
在数据挖掘领域，最大期望算法（Expectation-Maximization,EM） 一般作为聚类算法（类似 kmeans 算法）用来知识挖掘。<br />
在统计学上，当估运算带有无法观测隐藏变量的统计模型参数时,EM 算法不断迭代和优化可以观测数据的似然估计值。 <br />
统计模型？我把模型看做是描述观测数据是如何生成的。例如，一场考试的分数可能符合一种钟形曲线，因此这种分数分布符合钟形曲线（也称正态分布）的假设就是模型。<br />
什么是分布？分布代表了对所有可测量结果的可能性。例如，一场考试的分数可能符合一个正态分布。这个正态分布代表了分数的所有可能性。换句话说，给定一个分数，你可以用这个分布来预计多少考试参与者可能会得到这个分数。<br />
那么，你描述正态分布需要的所有东西就是这两个参数：<br />
1.平均值<br />
2.方差 <br />
什么是似然性：<br />
回到我们之前的钟形曲线例子，假设我们已经拿到很多的分数数据，并被告知分数符合一个钟形曲线。然而，我们并没有给到所有的分数，只是拿到了一个样本。
可以这样做：<br />
我们不知道所有分数的平均值或者方差，但是我们可以使用样本计算它们。似然性就是用估计的方差和平均值得到的钟形曲线在算出很多分数的概率。<br />
换句话说，给定一系列可测定的结果，让我们来估算参数。再使用这些估算出的参数，得到结果的这个假设概率就被称为似然性。<br />
算法的优势是：对于数据挖掘和聚类，观察到遗失的数据的这类数据点对我们来说很重要。我们不知道具体的类，因此这样处理丢失数据对使用 EM 算法做聚类的任务来说是很关键的。
算法的精髓在于：<br />
通过优化似然性，EM 生成了一个很棒的模型，这个模型可以对数据点指定类型标签—听起来像是聚类算法！ <br />
EM 算法是怎么帮助实现聚类的呢？EM 算法以对模型参数的猜测开始。然后接下来它会进行一个循环的3步：   <br />
1.E 过程：基于模型参数，它会针对每个数据点计算对聚类的分配概率。<br />
2.M 过程：基于 E 过程的聚类分配，更新模型参数。<br />
3.重复知道模型参数和聚类分配工作稳定（也可以称为收敛）。 <br />
EM 是非监督学习算法。
EM 算法的一个关键卖点就是它的实现直接。它不但可以优化模型参数，还可以反复的对丢失数据进行猜测。  <br />
这使算法在聚类和产生带参数的模型上都表现出色。在得知聚类情况和模型参数的情况下，我们有可能解释清楚有相同属性的分类情况和新数据属于哪个类之中。 <br />
EM算法弱点… <br />
第一，EM 算法在早期迭代中都运行速度很快，但是越后面的迭代速度越慢。<br />
第二，EM 算法并不能总是寻到最优参数，很容易陷入局部最优而不是找到全局最优解。   </p>
  </li>
  <li>
    <p>PageRank算法
PageRank是为了决定一些对象和同网络中的其他对象之间的相对重要程度而设计的连接分析算法(link analysis algorithm)。   <br />
什么是连接分析算法？<br />
它是一类针对网络的分析算法，探寻对象间的关系（也可成为连接）。<br />
举个例子：最流行的 PageRank 算法是 Google 的搜索引擎。尽管他们的搜索引擎不止是依靠它，但  PageRank依然是 Google 用来测算网页重要度的手段之一。 <br />
万维网上的网页都是互相链接的。如果 Rayli.net 链接到了 CNN 上的一个网页，CNN 网页就增加一个投票，表示 rayli.net 和 CNN 网页是关联的。 <br />
反过来，来自rayli.net 网页的投票重要性也要根据 rayli.net 网的重要性和关联性来权衡。换句话说，任何给 rayli.net 投票的网页也能提升 rayli.net 网页的关联性。 <br />
概括一下： <br />
投票和关联性就是 PageRank 的概念。rayli.net 给CNN 投票增加了 CNN 的 Pagerank，rayli.net 的 PageRank级别同时也影响着它为 CNN 投票多大程度影响了CNN 的 PageRank。<br />
这排名有点像一个网页流行度的竞争。我们的头脑中都有了一些这些网站的流行度和关联度的信息。
PageRank只是一个特别讲究的方式来定义了这些而已。 <br />
PageRank还有什么其他应用呢？ PageRank是专门为了万维网设计的。 <br />
可以考虑一下，以核心功能的角度看，PageRank算法真的只是一个处理链接分析极度有效率的方法。处理的被链接的对象不止只是针对网页。  <br />
给出PageRank 的三个实现：<br />
1 C++ OpenSource PageRank Implementation<br />
2 Python PageRank Implementation<br />
3 igraph – The network analysis package (R)  </p>
  </li>
  <li>
    <p>AdaBoost 迭代算法
AdaBoost 是个构建分类器的提升算法。 <br />
分类器拿走大量数据，并试图预测或者分类新数据元素的属于的类别。
提升(boost) 指的什么？提升是个处理多个学习算法（比如决策树）并将他们合并联合起来的综合的学习算法。目的是将弱学习算法综合或形成一个组，把他们联合起来创造一个新的强学习器。 <br />
强弱学习器之间有什么区别呢？弱学习分类器的准确性仅仅比猜测高一点。一个比较流行的弱分类器的例子就是只有一层的决策树。 <br />
另一个，强学习分类器有更高的准确率，一个通用的强学习器的例子就是 SVM。 <br />
举个 AdaBoost 算法的例子：我们开始有3个弱学习器，我们将在一个包含病人数据的数据训练集上对他们做10轮训练。数据集里包含了病人的医疗记录各个细节。 <br />
问题来了，那我们怎么预测某个病人是否会得癌症呢？AdaBoost 是这样给出答案的： <br />
第一轮，AdaBoost 拿走一些训练数据，然后测试每个学习器的准确率。最后的结果就是我们找到最好的那个学习器。另外，误分类的样本学习器给予一个比较高的权重，这样他们在下轮就有很高的概率被选中了。  <br />
再补充一下，最好的那个学习器也要给根据它的准确率赋予一个权重，并将它加入到联合学习器中（这样现在就只有一个分类器了） <br />
第二轮， AdaBoost 再次试图寻找最好的学习器。 <br />
关键部分来了，病人数据样本的训练数据现在被有很高误分配率的权重影响着。换句话说，之前误分类的病人在这个样本里有很高的出现概率。 <br />
为什么？
这就像是在电子游戏中已经打到了第二级，但当你的角色死亡后却不必从头开始。而是你从第二级开始然后集中注意，尽力升到第三级。 <br />
同样地，第一个学习者有可能对一些病人的分类是正确的，与其再度试图对他们分类，不如集中注意尽力处理被误分类的病人。   <br />
最好的学习器也被再次赋予权重并加入到联合分类器中，误分类的病人也被赋予权重，这样他们就有比较大的可能性再次被选中，我们会进行过滤和重复。 <br />
在10轮结束的时候，我们剩下了一个带着不同权重的已经训练过的联合学习分类器，之后重复训练之前回合中被误分类的数据。<br />
算法灵活通用，AdaBoost 可以加入任何学习算法，并且它能处理多种数据。<br />
AdaBoost 有很多程序实现和变体。给出一些： <br />
▪ scikit-learn<br />
▪ ICSIBoost<br />
▪ gbm: Generalized Boosted Regression Models   </p>
  </li>
  <li>
    <p>kNN：k最近邻算法<br />
kNN，或 K 最近邻(k-Nearest Neighbors), 诗歌分类算法。然而，它和我们之前描述的分类器不同，因为它是个懒散学习法。<br />
懒散学习法？  和存储训练数据的算法不同，懒散学习法在训练过程中不需要做许多处理。只有当新的未被分类的数据输入时，这类算法才会去做分类。 <br />
但在另一方面，积极学习法则会在训练中建立一个分类模型，当新的未分类数据输入时，这类学习器会把新数据也提供给这个分类模型。 <br />
那么 C4.5，SVM 和 AdaBoost 属于哪类呢？不像 kNN算法，他们都是积极学习算法。 <br />
给出原因：<br />
1 C4.5 在训练中建立了一个决策分类树模型。<br />
2 SVM在训练中建立了一个超平面的分类模型。<br />
3 AdaBoost在训练中建立了一个联合的分类模型。<br />
那么 kNN 做了什么？ kNN 没有建立这样的分类模型，相反，它只是储存了一些分类好的训练数据。那么新的训练数据进入时，kNN 执行两个基本步骤：<br />
1 首先，它观察最近的已经分类的训练数据点—也就是，k最临近点（k-nearest neighbors）<br />
2 第二部，kNN使用新数据最近的邻近点的分类， 就对新数据分类得到了更好的结果了。 <br />
你可能会怀疑…kNN 是怎么计算出最近的是什么？ 对于连续数据来说，kNN 使用一个像欧氏距离的距离测度，距离测度的选择大多取决于数据类型。有的甚至会根据训练数据学习出一种距离测度。关于 kNN 距离测度有更多的细节讨论和论文描述。 <br />
对于离散数据，解决方法是可以把离散数据转化为连续数据。给出两个例子： <br />
1 使用汉明距离（Hamming distance ）作为两个字符串紧密程度的测度。 <br />
2 把离散数据转化为二进制表征。 <br />
当临近的点是不同的类，kNN 怎么给新数据分类呢？当临近点都是同一类的时候，kNN 也就不费力气了。我们用直觉考虑，如果附近点都一致，那么新数据点就很可能落入这同一个类中了。 <br />
当临近点不是同一类时，kNN 怎么决定分类情况的呢？<br />
处理这种情况通常有两种办法：<br />
1 通过这些临近点做个简单的多数投票法。哪个类有更多的票，新数据就属于那个类。<br />
2 还是做个类似的投票，但是不同的是，要给那些离的更近的临近点更多的投票权重。这样做的一个简单方法是使用反距离(reciprocal distance). 比如，如果某个临近点距离5个单位，那么它的投票权重就是1/5.当临近点越来越远是，倒数距离就越来越小…这正是我们想要的。 <br />
 kNN 算法提供了已经被分类好的数据集，所以它是个监督学习算法。 <br />
我们需要注意的5点： <br />
1 当试图在一个大数据集上计算最临近点时，kNN 算法可能会耗费高昂的计算成本。<br />
2 噪声数据(Noisy data)可能会影响到 kNN 的分类。 <br />
3 选择大范围的属性筛选(feature)会比小范围的筛选占有很多优势，所以属性筛选(feature)的规模非常重要。 <br />
4 由于数据处理会出现延迟，kNN 相比积极分类器，一般需要更强大的存储需求。 <br />
5 选择一个合适的距离测度对 kNN 的准确性来说至关重要。 <br />
哪里用过这个方法？有很多现存的 kNN 实现手段：<br />
▪ MATLAB k-nearest neighbor classification<br />
▪ scikit-learn KNeighborsClassifier<br />
▪ k-Nearest Neighbour Classification in R   </p>
  </li>
  <li>Naive Bayes 朴素贝叶斯算法 <br />
朴素贝叶斯（Naive Bayes）并不只是一个算法，而是一系列分类算法，这些算法以一个共同的假设为前提：  <br />
被分类的数据的每个属性与在这个类中它其他的属性是独立的。 <br />
独立是什么意思呢？当一个属性值对另一个属性值不产生任何影响时，就称这两个属性是独立的。
举个例子：
比如说你有一个病人的数据集，包含了病人的脉搏，胆固醇水平，体重，身高和邮编这样的属性。如果这些属性值互相不产生影响，那么所有属性都是独立的。对于这个数据集来说，假定病人的身高和邮编相互独立，这是合理的。因为病人的身高和他们的邮编没有任何关系。但是我们不能停在这，其他的属性间是独立的么？ <br />
很遗憾，答案是否定的。给出三个并不独立的属性关系： <br />
▪ 如果身高增加，体重可能会增加。 <br />
▪ 如果胆固醇水平增加，体重可能增加。<br />
▪ 如果胆固醇水平增加，脉搏也可能会增加。<br />
以我的经验来看，数据集的属性一般都不是独立的。<br />
这样就和下面的问题联系起来了…<br />
为什么要把算法称为朴素的(naive)呢？数据集中所有属性都是独立的这个假设正是我们称为朴素（naive）的原因—— 通常下例子中的所有属性并不是独立的。  <br />
我们在深入研究一下..
这个等式是什么意思？在属性1和属性2的条件下，等式计算出了A 类的概率。换句话说，如果算出属性1 和2，等式算出的数据属于 A 类的概率大小。
等式这样写解释为：在属性1和属性2条件下，分类 A 的概率是一个分数。
▪ 分数的分子是在分类 A条件下属性1的概率，乘以在分类 A 条件下属性2的概率，再乘以分类 A 的概率
▪ 分数的分母是属性1的概率乘以属性2的概率。
Naive Bayes 的实现可以从Orange, scikit-learn, Weka和 R 里面找到。
最后，看一下第十种算法吧。  </li>
  <li>CART 分类算法<br />
CART 代表分类和回归树(classification and regression trees)。它是个决策树学习方法，同时输出分类和回归树。 像 C4.5一样，CART 是个分类器。 <br />
分类树像决策树一样么？分类树是决策树的一种。分类树的输出是一个类。 <br />
举个例子，根据一个病人的数据集、你可能会试图预测下病人是否会得癌症。这个分类或者是“会的癌症”或者是“不会得癌症”。 <br />
那回归树是什么呢？和分类树预测分类不同，回归树预测一个数字或者连续数值，比如一个病人的住院时间或者一部智能手机的价格。<br />
这么记比较简单：<br />
分类树输出类、回归树输出数字。 <br />
为了构造分类和回归树模型，需要给它提供被分类好的训练数据集，因此 CART 是个监督学习算法。
为什么要使用 CART 呢？使用 C4.5的原因大部分也适用于 CART，因为它们都是决策树学习的方法。便于说明和解释这类的原因也适用于 CART。
和 C4.5一样，它们的计算速度都很快，算法也都比较通用流行，并且输出结果也具有可读性。
scikit-learn 在他们的决策树分类器部分实现了 CART 算法；R 语言的 tree package 也有 CART 的实现；Weka 和 MATLAB 也有CART的实现过程。</li>
</ol>


      <div class="readall"><a href="/blog/2015/09/20/10-suanfa-biji.html" id="post-readall">阅读全文&nbsp;<i class="fa fa-chevron-right"></i></a></div>
    </div>
  </section>
</div>

<div class="posts">
  <section class="post">
    <header class="post-header">
      <h1 class="post-title"><a href="/blog/2015/05/16/sqoop-spacil-chart.html">sqoop 特殊字符导入问题</a></h1>
      <p class="post-meta">
        <i class="fa fa-calendar"></i>
        2015年05月16日
        <i class="space"></i>
        <i class="fa fa-tags"></i>
        <a class="post-category" href="/page/category.html#hadoop">hadoop</a>
      </p>
    </header>
    <div class="post-main">
      <p>Sqoop从MySQL导入数据到hive，示例：<br />
sqoop import –connect  jdbc:mysql://10.255.2.89:3306/test?charset=utf-8 –  username selectuser –password select##select##  –table test_sqoop_import \<br />
–columns ‘id,content,updateTime’ \<br />
–split-by id \<br />
–hive-import –hive-table basic.test2;  </p>

<p>如果不加其他参数，导入的数据默认的列分隔符是’\001’，默认的行分隔符是’\n’。<br />
这样问题就来了，如果导入的数据中有’\n’，hive会认为一行已经结束，后面的数据被分割成下一行。这种情况下，导入之后hive中数据的行数就比原先数据库中的多，而且会出现数据不一致的情况。<br />
Sqoop也指定了参数 –fields-terminated-by和 –lines-terminated-by来自定义行分隔符和列分隔符。<br />
可是当你真的这么做时 坑爹呀<br />
INFO hive.HiveImport: FAILED: SemanticException 1:381 LINES TERMINATED BY only supports newline ’\n’ right now.    
也就是说虽然你通过–lines-terminated-by指定了其他的字符作为行分隔符，但是hive只支持’\n’作为行分隔符。<br />
简单的解决办法就是加上参数–hive-drop-import-delims来把导入数据中包含的hive默认的分隔符去掉。  </p>

<p>附 sqoop 命令参考  </p>

<p>导入数据到 hdfs<br />
使用 sqoop-import 命令可以从关系数据库导入数据到 hdfs。<br />
$ sqoop import –connect jdbc:mysql://192.168.56.121:3306/metastore –username hiveuser –password redhat –table TBLS –target-dir /user/hive/result
注意：<br />
mysql jdbc url 请使用 ip 地址<br />
如果重复执行，会提示目录已经存在，可以手动删除<br />
如果不指定 –target-dir，导入到用户家目录下的 TBLS 目录<br />
你还可以指定其他的参数：  </p>

<pre><code>--append	将数据追加到hdfs中已经存在的dataset中。使用该参数，sqoop将把数据先导入到一个临时目录中，然后重新给文件命名到一个正式的目录中，以避免和该目录中已存在的文件重名。  
--as-avrodatafile	将数据导入到一个Avro数据文件中  
--as-sequencefile	将数据导入到一个sequence文件中  
--as-textfile	将数据导入到一个普通文本文件中，生成该文本文件后，可以在hive中通过sql语句查询出结果。  
--boundary-query &lt;statement&gt;	边界查询，也就是在导入前先通过SQL查询得到一个结果集，然后导入的数据就是该结果集内的数据，格式如：--boundary-query 'select id,no from t where id = 3'，表示导入的数据为id=3的记录，或者 select min(&lt;split-by&gt;), max(&lt;split-by&gt;) from &lt;table name&gt;，注意查询的字段中不能有数据类型为字符串的字段，否则会报错
--columns&lt;col,col&gt;	指定要导入的字段值，格式如：--columns id,username
--direct	直接导入模式，使用的是关系数据库自带的导入导出工具。官网上是说这样导入会更快
--direct-split-size	在使用上面direct直接导入的基础上，对导入的流按字节数分块，特别是使用直连模式从PostgreSQL导入数据的时候，可以将一个到达设定大小的文件分为几个独立的文件。
--inline-lob-limit	设定大对象数据类型的最大值
-m,--num-mappers	启动N个map来并行导入数据，默认是4个，最好不要将数字设置为高于集群的节点数
--query，-e &lt;sql&gt;	从查询结果中导入数据，该参数使用时必须指定–target-dir、–hive-table，在查询语句中一定要有where条件且在where条件中需要包含 \$CONDITIONS，示例：--query 'select * from t where \$CONDITIONS ' --target-dir /tmp/t –hive-table t
--split-by &lt;column&gt;	表的列名，用来切分工作单元，一般后面跟主键ID
--table &lt;table-name&gt;	关系数据库表名，数据从该表中获取
--delete-target-dir	删除目标目录
--target-dir &lt;dir&gt;	指定hdfs路径
--warehouse-dir &lt;dir&gt;	与 --target-dir 不能同时使用，指定数据导入的存放目录，适用于hdfs导入，不适合导入hive目录
--where	从关系数据库导入数据时的查询条件，示例：--where "id = 2"
-z,--compress	压缩参数，默认情况下数据是没被压缩的，通过该参数可以使用gzip压缩算法对数据进行压缩，适用于SequenceFile, text文本文件, 和Avro文件
--compression-codec	Hadoop压缩编码，默认是gzip
--null-string &lt;null-string&gt;	可选参数，如果没有指定，则字符串null将被使用
--null-non-string &lt;null-string&gt;	可选参数，如果没有指定，则字符串null将被使用
</code></pre>

<p>使用 sql 语句<br />
参照上表，使用 sql 语句查询时，需要指定 $CONDITIONS  </p>

<p>$ sqoop import –connect jdbc:mysql://192.168.56.121:3306/metastore –username hiveuser –password redhat –query ‘SELECT * from TBLS where $CONDITIONS ‘ –split-by tbl_id -m 4 –target-dir /user/hive/result<br />
	上面命令通过 -m 1 控制并发的 map 数。</p>

<p>使用 direct 模式：<br />
$ sqoop import –connect jdbc:mysql://192.168.56.121:3306/metastore –username hiveuser –password redhat –table TBLS –delete-target-dir –direct –default-character-set UTF-8 –target-dir /user/hive/result</p>

<p>指定文件输出格式：<br />
$ sqoop import –connect jdbc:mysql://192.168.56.121:3306/metastore –username hiveuser –password redhat –table TBLS –fields-terminated-by “\t” –lines-terminated-by “\n” –delete-target-dir  –target-dir /user/hive/result</p>

<p>指定空字符串：<br />
$ sqoop import –connect jdbc:mysql://192.168.56.121:3306/metastore –username hiveuser –password redhat –table TBLS –fields-terminated-by “\t” –lines-terminated-by “\n” –delete-target-dir –null-string ‘\N’ –null-non-string ‘\N’ –target-dir /user/hive/result</p>

<p>如果需要指定压缩：  <br />
$ sqoop import –connect jdbc:mysql://192.168.56.121:3306/metastore –username hiveuser –password redhat –table TBLS –fields-terminated-by “\t” –lines-terminated-by “\n” –delete-target-dir –null-string ‘\N’ –null-non-string ‘\N’ –compression-codec “com.hadoop.compression.lzo.LzopCodec” –target-dir /user/hive/result  </p>

<p>附：可选的文件参数如下表。   </p>

<pre><code>--enclosed-by &lt;char&gt;	给字段值前后加上指定的字符，比如双引号，示例：--enclosed-by '\"'，显示例子："3","jimsss","dd@dd.com"
--escaped-by &lt;char&gt;	给双引号作转义处理，如字段值为"测试"，经过 --escaped-by "\\" 处理后，在hdfs中的显示值为：\"测试\"，对单引号无效
--fields-terminated-by &lt;char&gt;	设定每个字段是以什么符号作为结束的，默认是逗号，也可以改为其它符号，如句号.，示例如：--fields-terminated-by
--lines-terminated-by &lt;char&gt;	设定每条记录行之间的分隔符，默认是换行串，但也可以设定自己所需要的字符串，示例如：--lines-terminated-by "#" 以#号分隔
--mysql-delimiters	Mysql默认的分隔符设置，字段之间以,隔开，行之间以换行\n隔开，默认转义符号是\，字段值以单引号'包含起来。
--optionally-enclosed-by &lt;char&gt;	enclosed-by是强制给每个字段值前后都加上指定的符号，而--optionally-enclosed-by只是给带有双引号或单引号的字段值加上指定的符号，故叫可选的
</code></pre>

<p>创建 hive 表<br />
生成与关系数据库表的表结构对应的HIVE表：  </p>

<pre><code>$ sqoop create-hive-table --connect jdbc:mysql://192.168.56.121:3306/metastore --username hiveuser --password redhat --table TBLS 
--hive-home &lt;dir&gt;	Hive的安装目录，可以通过该参数覆盖掉默认的hive目录
--hive-overwrite	覆盖掉在hive表中已经存在的数据
--create-hive-table	默认是false，如果目标表已经存在了，那么创建任务会失败
--hive-table	后面接要创建的hive表
--table	指定关系数据库表名  导入数据到 hive   执行下面的命令会将 mysql 中的数据导入到 hdfs 中，然后创建一个hive 表，最后再将 hdfs 上的文件移动到 hive 表的目录下面。   $ sqoop import --connect jdbc:mysql://192.168.56.121:3306/metastore --username hiveuser --password redhat --table TBLS --fields-terminated-by "\t" --lines-terminated-by "\n" --hive-import --hive-overwrite --create-hive-table --hive-table dw_srclog.TBLS --delete-target-dir   说明：   可以在 hive 的表名前面指定数据库名称   可以通过 --create-hive-table 创建表，如果表已经存在则会执行失败   从上面可见，数据导入到 hive 中之后分隔符为默认分隔符，参考上文你可以通过设置参数指定其他的分隔符。   另外，Sqoop 默认地导入空值（NULL）为 null 字符串，而 hive 使用 \N 去标识空值（NULL），故你在 import 或者 export 时候，需要做相应的处理。在 import 时，使用如下命令：   $ sqoop import  ... --null-string '\\N' --null-non-string '\\N'   在导出时，使用下面命令：   $ sqoop import  ... --input-null-string '' --input-null-non-string ''
</code></pre>

<p>增量导入<br />
–check-column (col)	用来作为判断的列名，如id<br />
–incremental (mode)	append：追加，比如对大于last-value指定的值之后的记录进行追加导入。lastmodified：最后的修改时间，追加last-value指定的日期之后的记录<br />
–last-value (value)	指定自从上次导入后列的最大值（大于该指定的值），也可以自己设定某一值</p>

<p>合并 hdfs 文件<br />
将HDFS中不同目录下面的数据合在一起，并存放在指定的目录中，示例如：<br />
sqoop merge –new-data /test/p1/person –onto /test/p2/person –target-dir /test/merged –jar-file /opt/data/sqoop/person/Person.jar –class-name Person –merge-key id<br />
其中，–class-name 所指定的 class 名是对应于 Person.jar 中的 Person 类，而 Person.jar 是通过 Codegen 生成的<br />
–new-data <path>	Hdfs中存放数据的一个目录，该目录中的数据是希望在合并后能优先保留的，原则上一般是存放越新数据的目录就对应这个参数。  
--onto <path>	Hdfs中存放数据的一个目录，该目录中的数据是希望在合并后能被更新数据替换掉的，原则上一般是存放越旧数据的目录就对应这个参数。  
--merge-key &lt;col&gt;	合并键，一般是主键ID  
--jar-file <file>	合并时引入的jar包，该jar包是通过Codegen工具生成的jar包  
--class-name <class>	对应的表名或对象名，该class类是包含在jar包中的。  
--target-dir <path>	合并后的数据在HDFS里的存放目录  </path></class></file></path></path></p>

      <div class="readall"><a href="/blog/2015/05/16/sqoop-spacil-chart.html" id="post-readall">阅读全文&nbsp;<i class="fa fa-chevron-right"></i></a></div>
    </div>
  </section>
</div>

<div class="posts">
  <section class="post">
    <header class="post-header">
      <h1 class="post-title"><a href="/blog/2015/05/16/hive-datahouse.html">hive 数据仓库优劣学习</a></h1>
      <p class="post-meta">
        <i class="fa fa-calendar"></i>
        2015年05月16日
        <i class="space"></i>
        <i class="fa fa-tags"></i>
        <a class="post-category" href="/page/category.html#hadoop">hadoop</a>
      </p>
    </header>
    <div class="post-main">
      <h5 id="hive">Hive数据仓库优劣</h5>
<p>Hive是一个基于Hadoop的数据仓库平台，通过Hive，可以方便地进行数据提取转化加载（ETL）的工作。Hive定义了一个类似于SQL的查询语言HQL，能够将用户编写的SQL转化为相应的MapReduce程序。当然，用户也可以自定义Mapper和Reducer来完成复杂的分析工作。<br />
基于MapReduce的Hive具有良好的扩展性和容错性。不过由于MapReduce缺乏结构化数据分析中有价值的特性，以及Hive缺乏对执行计划的充分优化，导致Hive在很场景中执行效率不高。  </p>

<p>强大的数据仓库和数据分析平台至少需要具备以下几点特性。<br />
灵活的存储引擎<br />
高效的执行引擎<br />
良好的可扩展性<br />
强大的容错机制<br />
多样化的可视化  </p>

<h6 id="section">#存储引擎</h6>
<p>Hive数据存储没有特有的数据结构，（只是hdfs文件的映射）也没有为数据建立索引，可以自由地组织Hive中的表，只要在创建表时告诉Hive数据中的列分隔符和行分隔符，Hive就可以解析数据。Hive的元数据存储在RDBMS中，所有数据都基于HDFS存储。Hive包含Table、External Table、Partition和Bucket等数据模型。  </p>

<h6 id="section-1">#执行引擎</h6>
<p>Hive的编译器负责编译源代码并生成最终的执行计划，包括语法分析、语义分析、目标代码生成，所做的优化并不多。Hive基于MapReduce，Hive的Sort和GroupBy都依赖MapReduce。而MapReduce相当于固化了执行算子，Map的MergeSort必须执行，GroupBy算子也只有一种模式，Reduce的Merge-Sort也必须可选。另外Hive对Join算子的支持也较少。内存拷贝和数据预处理也会影响Hive的执行效率。<br />
#######扩展性
Hive关注水平扩展性。简单来讲，水平扩展性指系统可以通过简单的增加资源来支持更大的数据量和负载。Hive处理的数据量是PB级的，而且每小时每天都在增长，这就使得水平扩展性成为一个非常重要的指标。Hadoop系统的水平扩展性是非常好的，而Hive基MapReduce框架，因此能够很自然地利用这点。
#######容错性
Hive有较好的容错性。Hive的执行计划在MapReduce框架上以作业的方式执行，每个作业的中间结果文件写到本地磁盘，最终输出文件写到HDFS文件系统，利用HDFS的多副本机制来保证数据的可靠性，从而达到作业的容错性。如果在作业执行过程中某节点出现故障，那么Hive执行计划基本不会受到影响。因此，基于Hive实现的数据仓库可以部署在由普通机器构建的分布式集群之上。
#######可视化
Hive的可视化界面基本属于字符终端，用户的技术水平一般比较高。面向不同的应用和用户，提供个性化的可视化展现，是Hive改进的一个重要方向。</p>

      <div class="readall"><a href="/blog/2015/05/16/hive-datahouse.html" id="post-readall">阅读全文&nbsp;<i class="fa fa-chevron-right"></i></a></div>
    </div>
  </section>
</div>

<div class="posts">
  <section class="post">
    <header class="post-header">
      <h1 class="post-title"><a href="/blog/2015/05/16/MR-join.html">MR中Join方案</a></h1>
      <p class="post-meta">
        <i class="fa fa-calendar"></i>
        2015年05月16日
        <i class="space"></i>
        <i class="fa fa-tags"></i>
        <a class="post-category" href="/page/category.html#hadoop">hadoop</a>
      </p>
    </header>
    <div class="post-main">
      <h4 id="mrjoin">MR中Join方案</h4>
<p>与传统数据一样在MR中的join操作也是一项耗时的操作，但是我们可以根据MR 的设计思想对其进行优化。<br />
#######Reduce side join<br />
这是最简单的一种形式，主要思想：<br />
在map阶段，map函数同时读取两个文件File1和File2，为了区分两种来源的key/value数据对，对每条数据打一个标签（tag）,比如：tag=0表示来自文件File1，tag=2表示来自文件File2。即：map阶段的主要任务是对不同文件中的数据打标签。<br />
在reduce阶段，reduce函数获取key相同的来自File1和File2文件的value list， 然后对于同一个key，对File1和File2中的数据进行join（笛卡尔乘积）。即：reduce阶段进行实际的连接操作。这种形式主要是利用hadoop mr 的框架机制。  </p>

<h6 id="map-side-join">#Map side join</h6>
<p>reduce side join，是因为在map阶段不能获取所有需要的join字段，即：同一个key对应的字段可能位于不同map中。Reduce side join是非常低效的，因为shuffle阶段要进行大量的数据传输。<br />
Map side join是针对以下场景进行的优化：两个待连接表中，有一个表非常大，而另一个表非常小，以至于小表可以直接存放到内存中。这样，我们可以将小表复制多份，让每个map task内存中存在一份（比如存放到hash table中），然后只扫描大表：对于大表中的每一条记录key/value，在hash table中查找是否有相同的key的记录，如果有，则连接后输出即可。<br />
为了支持文件的复制，<br />
Hadoop提供了一个类DistributedCache，使用该类的方法如下：<br />
（1）用户使用静态方法DistributedCache.addCacheFile(URI)指定要复制的文件。JobTracker在作业启动之前会获取这个URI列表，并将相应的文件拷贝到各个TaskTracker的本地磁盘上。<br />
（2）用户使用DistributedCache.getLocalCacheFiles()方法获取文件目录，并使用标准的文件读写API读取相应的文件。<br />
如果足够小 也可以 放在context里面（不推荐）<br />
#######Semi Join<br />
Semi Join，也叫半连接，是从分布式数据库中借鉴过来的方法。它的产生动机是：对于reduce side join，跨机器的数据传输量非常大，这成了join操作的一个瓶颈，如果能够在map端过滤掉不会参加join操作的数据，则可以大大节省网络IO。<br />
实现方法很简单：选取一个小表，假设是File1，将其参与join的key抽取出来，保存到文件File3中，File3文件一般很小，可以放到内存中。在map阶段，使用DistributedCache将File3复制到各个TaskTracker上，然后将File2中不在File3中的key对应的记录过滤掉，剩下的reduce阶段的工作与reduce side join相同。<br />
#######reduce side join + BloomFilter<br />
在某些情况下，SemiJoin抽取出来的小表的key集合在内存中仍然存放不下，这时候可以使用BloomFiler以节省空间。<br />
BloomFilter最常见的作用是：判断某个元素是否在一个集合里面。它最重要的两个方法是：add() 和contains()。最大的特点是不会存在 false negative，即：如果contains()返回false，则该元素一定不在集合中，但会存在一定的 false positive，即：如果contains()返回true，则该元素一定可能在集合中。因而可将小表中的key保存到BloomFilter中，在map阶段过滤大表，可能有一些不在小表中的记录没有过滤掉（但是在小表中的记录一定不会过滤掉），这没关系，只不过增加了少量的网络IO而已。<br />
#######二次排序
在Hadoop中，默认情况下是按照key进行排序，如果要按照value进行排序怎么办？即：对于同一个key，reduce函数接收到的value list是按照value排序的。这种应用需求在join操作中很常见，比如，希望相同的key中，小表对应的value排在前面。<br />
有两种方法进行二次排序，分别为：buffer and in memory sort和 value-to-key conversion。
对于buffer and in memory sort，主要思想是：在reduce()函数中，将某个key对应的所有value保存下来，然后进行排序。 这种方法最大的缺点是：可能会造成out of memory。<br />
对于value-to-key conversion，主要思想是：将key和部分value拼接成一个组合key（实现WritableComparable接口或者调用setSortComparatorClass函数），这样reduce获取的结果便是先按key排序，后按value排序的结果，需要注意的是，用户需要自己实现Paritioner，以便只按照key进行数据划分。<br />
Hadoop显式的支持二次排序，在Configuration类中有个setGroupingComparatorClass()方法，可用于设置排序group的key值
实际工作多数情况下，会采用第一种，reduce side join 简单 安全  。。。。  </p>


      <div class="readall"><a href="/blog/2015/05/16/MR-join.html" id="post-readall">阅读全文&nbsp;<i class="fa fa-chevron-right"></i></a></div>
    </div>
  </section>
</div>

<div class="pagination">
  
  
  <a class="pagination-item newer" href="/"><i class="fa fa-arrow-left"></i>&nbsp;&nbsp;上一页</a>
  
    
  
  <a class="pagination-item older" href="/page/3">下一页&nbsp;&nbsp;<i class="fa fa-arrow-right"></i></a>
  
</div>
    <footer>Copyright&nbsp;&copy;&nbsp;2015 <a href="index.html">willgo</a><br/><i class="fa fa-cogs" style="color:blueviolet;">&nbsp;</i>Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a>
</br> <a href="http://m.kuaidi100.com" target="_blank">快递查询</a> 
</footer>

    </div>
  </div>    
  <div id="top"><a id="rocket" href="javascript:;" title="返回顶部"><i></i></a></div>
  
</body>
</html>
