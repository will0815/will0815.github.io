<!DOCTYPE HTML>
<html>
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0">
  <meta name="description" content="Will go, Just do it.">
  <meta name="author" content="Will.Quan">
  <meta name="keywords" content="Will go, Just do it., willgo, 最好的从未错过, Will.Quan">
  <title>Will go, Just do it.</title>
  <link rel="canonical" href="index.html">
  <link rel="icon" href="/res/img/favicon.ico" type="image/x-icon">
  <link rel="shortcut icon" href="/res/img/favicon.ico" type="image/x-icon">
  <link rel="stylesheet" href="/res/css/public.css">
  <link rel="stylesheet" href="/res/css/light.css">
  <script src="/res/js/light.js"></script>
  <script type="text/javascript">
var _bdhmProtocol = (("https:" == document.location.protocol) ? " https://" : " http://");
document.write(unescape("%3Cscript src='" + _bdhmProtocol + "hm.baidu.com/h.js%3Fda48b6233123178f300913a0e707883e' type='text/javascript'%3E%3C/script%3E"));
</script>

</head>
<body>
  <div id="blog">
    <div class="sidebar">
  <div class="profilepic">
    <a href="/"><img src="/res/img/icon.png" alt="logo"></img></a>
  </div>
  <h1 class="title"><a href="/">willgo</a></h1>
  <h2 class="sub-title">最好的从未错过</h2>
  <nav id="nav">
    <ul>
    
      <li><a href="/page/timing.html"><i class="fa fa-clock-o"></i>&nbsp;资料时间</a></li>
    
      <li><a href="/page/category.html"><i class="fa fa-tags"></i>&nbsp;文章分类</a></li>
    
      <li><a href="/page/read.html"><i class="fa fa-book"></i>&nbsp;逗绊读书</a></li>
    
      <li><a href="/page/life.html"><i class="fa fa-eyedropper"></i>&nbsp;生活记录</a></li>
    
      <li><a href="/page/about.html"><i class="fa fa-paper-plane-o"></i>&nbsp;假装关于</a></li>
    
    </ul>
  </nav>  
  <nav id="sub-nav">
    <a class="weibo " href="http://weibo.com/u/2369015654" title="新浪微博" target="_blank"><i class="fa fa-weibo"></i></a>
    <a class="github" href="https://github.com/will0815/" title="GitHub" target="_blank"><i class="fa fa-github fa-2x"></i></a>
    <a class="rss" href="/page/feed.xml" title="RSS订阅" target="_blank"><i class="fa fa-rss"></i></a>
  </nav>
  <div id="license">
    <a href="http://creativecommons.org/licenses/by-nc-sa/3.0/cn/" target="_blank" title="本站所有作品采用：&#10;知识共享《署名 非商业性使用 相同方式共享 3.0》&#10;进行许可" >
    <img alt="License" height="31" width="88" src="/res/img/license.png" /></a>
  </div>
</div>

    <div class="main">
        <div style="font-family: segoepr;font-size: 40px;
              margin-top:-5px; color:#fff;">
            <script type="text/javascript" 
            src="http://open.iciba.com/ds_open.php?id=11519&name=willgo&auth=BE45CDE6481CC46336529A1B407855B1" charset="utf-8">
            </script>
        </div>
    
<div class="posts">
  <section class="post">
    <header class="post-header">
      <h1 class="post-title"><a href="/blog/2015/01/23/Linux-point1.html">Linux学习笔记1-文件安全与权限</a></h1>
      <p class="post-meta">
        <i class="fa fa-calendar"></i>
        2015年01月23日
        <i class="space"></i>
        <i class="fa fa-tags"></i>
        <a class="post-category" href="/page/category.html#linux">linux</a>
      </p>
    </header>
    <div class="post-main">
      <h1 id="section">文件安全与权限</h1>
<p>包含内容：<br />
• 文件和目录的权限。<br />
• setuid。<br />
• chown和chgrp。<br />
• umask。<br />
• 符号链接。  </p>

<p>创建文件时系统保存文件的有关信息包括：<br />
1、长度<br />
2、类型<br />
3、位置<br />
4、所属用户以及哪些用户可以访问<br />
5、i节点<br />
6、修改时间 （-rwxr-xr-x：分别表示文件所有者、同组、以及其他用户对该文件的权限）  </p>

<p>文件类型: 如上权限位-rwxr-xr-x 其中最前面的横杠即表示该文件的类型（这里为普通文件）<br />
文件类型分为：<br />
d 目录<br />
l 符号链接。<br />
s 套接字文件<br />
b 块设备文件<br />
c 字符设备文件<br />
p 命名管道文件  </p>

<p><strong>更改文件权限位命令：chmod</strong> 
chmod [who] operator [permission] filename<br />
who:<br />
u 文件所有者<br />
g 同组用户<br />
o 其他用户<br />
a 所有用户<br />
operator：<br />
+ 增加权限<br />
- 取消权限<br />
= 设定权限<br />
permission：<br />
r 读<br />
w 写<br />
x 执行<br />
s 文件所有者和组set-ID<br />
t 粘性位*<br />
l 给文件加锁 是其他用户无法访问<br />
u  </p>

<p>chmod命令的绝对模式<br />
chmod [mode] file<br />
如： chmod -R 755 myfile<br />
755 分别对应文件所有者、同组、以及其他用户对该文件的权限<br />
7：rwx(4+2+1)<br />
5: rx(4+1)<br />
-R 表示如果是目录及将对应操作应用于每一个子目录及文件  </p>

<p>目录与文件<br />
目录权限中<br />
r：列出目录内容<br />
w: 创建文件<br />
x: 搜索和访问<br />
如果一个目录只有执行（x）权限，但目录中有个可执行程序，只要用户知道它的路径仍然可以执行，此时用户不能进入目录但可以执行其中的程序
目录的权限将会覆盖目录中文件的权限   </p>

<p>chown和chgrp<br />
当你创建一个文件时，你就是该文件的属主。一旦你拥有某个文件，就可以改变它的所<br />
有权，把它的所有权交给另外一个/etc/passwd文件中存在的合法用户。可以使用用户名或用<br />
户ID号来完成这一操作。在改变一个文件的所有权时，相应的suid也将被清除，这是出于安<br />
全性的考虑。只有文件的属主和系统管理员可以改变文件的所有权。一旦将文件的所有权交<br />
给另外一个用户，就无法再重新收回它的所有权。如果真的需要这样做，那么就只有求助于<br />
系统管理员了。<br />
chown命令的一般形式为：<br />
chmod -R -h owner file<br />
-R选项意味着对所有子目录下的文件也都进行同样的操作。-h选项意味着在改变符号链<br />
接文件的属主时不影响该链接所指向的目标文件。<br />
chgrp与chown类似  </p>

<p>umask<br />
umask命令确定创建文件的缺省模式。系统管理员必须要为你设置一个合理的umask值，以确保你创建的<br />
文件具有所希望的缺省权限，防止其他非同组用户对你的文件具有写权限。
在已经登录之后，可以按照个人的偏好使用umask命令来改变文件创建的缺省权限。相应<br />
的改变直到退出该shell或使用另外的umask命令之前一直有效。
一般来说,umask命令是在/etc/profile文件中设置的，每个用户在登录时都会引用这个文件，所以如果希望改变所有用户的umask，可以在该文件中加入相应的条目。如果希望永久性地设置自己的umask值，那么就把它放在自己$HOME目录下的.profile文件中。  </p>

<p>符号链接<br />
两种不同类型的链接：软链接（指向文件的指针)/硬链接<br />
软链接可以理解为windows中的快捷方式 <br />
该命令的一般形式为： <br />
ln [-s] source_path target_path <br />
其中的路径可以是目录也可以是文件。    </p>


      <div class="readall"><a href="/blog/2015/01/23/Linux-point1.html" id="post-readall">阅读全文&nbsp;<i class="fa fa-chevron-right"></i></a></div>
    </div>
  </section>
</div>

<div class="posts">
  <section class="post">
    <header class="post-header">
      <h1 class="post-title"><a href="/blog/2014/11/30/collaborative-filtering-algorithm.html">协同过滤简解</a></h1>
      <p class="post-meta">
        <i class="fa fa-calendar"></i>
        2014年11月30日
        <i class="space"></i>
        <i class="fa fa-tags"></i>
        <a class="post-category" href="/page/category.html#hadoop">hadoop</a>
      </p>
    </header>
    <div class="post-main">
      <h2 id="section">协同过滤算法</h2>
<p><strong>Collaborative Filtering</strong><br />
基于一组兴趣相同的用户进行推荐。协同过滤基于这样的假设：为用户找到他真正感兴趣的内容的好方法是，首先找他与他兴趣相似的用户，然后将这些用户感兴趣的内容推荐给此用户</p>

<p>协同过滤技术可以分为三类：<br />
基于用户（User-based）的协同过滤；<br />
基于项目（Item-based）的协同过滤；<br />
基于模型（Model-based）的协同过滤</p>

<p>步骤一，收集可以代表用户兴趣的信息。<br />
如传统的打分的方式，豆瓣上的“我来评价”之类也是这种方法。这种方式被称为“显式评分”方法。其缺点是，收集数据比较困难，用户通常并不愿意费力气为你贡献这种数据。这导致这种系统通常更多出现在实验室或者论文里面。在实际的商业系统中，即使使用了这种方法，也多会被包装为一种更加有好如游戏方式。<br />
另外一种被认为更有效的方法是“隐式评分”方法。这种方法不需要用户直接输入评价数据，而是根据用户的行为特征由系统代替用户完成评价。一种研究得比较多的方法是 Web Mining 。电子商务网站在隐式评分的数据获取上有先天的优势，用户购买的商品记录是非常有用的数据。</p>

<p>步骤二，最近邻搜索。<br />
协同过滤的出发点是与你兴趣相同的一组用户，术语叫做“最近邻”。最近邻搜索的核心是计算两个用户的相似度。例如用户A和用户B，首先需要获取用户A和用户B所有的评分项，然后选择一个合适的相似度计算方法，基于评分项数据，计算得到用户A和用户B的相似度数值。<br />
目前使用比较多的相似度算法包括，皮尔森相关系数（Person Correlation Coefficient）、余弦相似性（Cosine-based Similarity）以及调整余弦相似性（Adjusted Consine Similarity）</p>

<p>步骤三，生成推荐结果。<br />
有了最近邻集合，就可以对目标用户的兴趣进行预测，生成推荐结果。通常根据推荐目的的不同，可以进行多种形式的推荐。最常见的推荐结果有两种，Top-N 推荐和关联推荐。<br />
Top-N 推荐，这里的 Top-N 和一般网站（比如 digg）上见到的“最热门”列表是不同的。热门列表是基于全部数据集产生的，它对每个人都是一样的；Top-N 推荐是针对单个用户产生的，它对每个人是不一样的：通过对你的最近邻用户进行统计，选择出现频率最高且在你的评分项目中不存在的项目作为推荐结果。豆瓣上的“排行”栏目，应该是传统的“热门”列表，不是 Top-N 推荐。<br />
关联推荐，也称为基于关联规则的推荐。与传统关联规则针对全部数据进行挖掘不同的是，此方法仅对最近邻用户的购买记录进行关联规则挖掘。如果你曾经购买过关联规则左边的商品，而没有购买过关联规则右边的商品，那么就把右边的这个商品推荐给你。它最突出的优点就是，可以帮助你发现你感兴趣的而以前却从来没有注意过的商品。在 Amazon 介绍书的详细信息的页面上，可以看到这种推荐的一个实际应用。例如在《The Search》的页面上，Amazon 给我的推荐是，</p>

      <div class="readall"><a href="/blog/2014/11/30/collaborative-filtering-algorithm.html" id="post-readall">阅读全文&nbsp;<i class="fa fa-chevron-right"></i></a></div>
    </div>
  </section>
</div>

<div class="posts">
  <section class="post">
    <header class="post-header">
      <h1 class="post-title"><a href="/blog/2014/11/30/clustering-algorithm.html">聚类以及增量聚类的简单理解</a></h1>
      <p class="post-meta">
        <i class="fa fa-calendar"></i>
        2014年11月30日
        <i class="space"></i>
        <i class="fa fa-tags"></i>
        <a class="post-category" href="/page/category.html#hadoop">hadoop</a>
      </p>
    </header>
    <div class="post-main">
      <h2 id="section">聚类以及增量聚类的简单理解</h2>
<p>K-Means聚类基本步骤如下：   </p>

<pre><code>Step1: 确定K值以及初始化聚类中心，选取K个初始凝聚点，做为欲形成的中心（最简单的方法就是K的值自己确定，但必须小于数据集个数，然后从数据集中选取K个数据做为初始聚类中心）；  
Step2: 计算每个数据到K个初始凝聚点的距离，将每个数据和最近的凝聚点分到一组，形成K个初始分类；  
Step3: 计算初始分类的重心（或均值），做为新的凝聚点，重新计算每个数据到分类重心（或均值）的距离，将每个数据和最近的凝聚点分为一组；
Step4: 重复进行Step2和Step3，直到分类的重心或均值没有明显变化为止。
</code></pre>

<p>Canopy算法流程  </p>

<pre><code>      （1）、将数据集向量化得到一个list后放入内存，选择两个距离阈值：T1和T2，其中T1 &gt; T2，对应上图，实线圈为T1，虚线圈为T2，T1和T2的值可以用交叉校验来确定；  
      （2）、从list中任取一点P，用低计算成本方法快速计算点P与所有Canopy之间的距离（如果当前不存在Canopy，则把点P作为一个Canopy），如果点P与某个Canopy距离在T1以内，则将点P加入到这个Canopy；  
      （3）、如果点P曾经与某个Canopy的距离在T2以内，则需要把点P从list中删除，这一步是认为点P此时与这个Canopy已经够近了，因此它不可以再做其它Canopy的中心了；  
      （4）、重复步骤2、3，直到list为空结束。
</code></pre>

<p>单机版Canopy算法：  </p>

<pre><code>      1、从PointList中取一个Point ，寻找已经建立好的Canopy 计算这个点于所有的Canopy的距离。如果和某一个Canopy的距离小于T1，则把这个点加到Canopy中，如果没有Canopy则选择这个点为一个Canopy的中心。  
      2、如果这个店Point和某个Canopy的距离小于T2,则把这个点从PointList中删除（这个点以后做不了其他的Canopy的中心了）。  
      3、循环直到所有的Point都被加入进来，然后计算各个Canopy的Center和Radius。
</code></pre>

<p>MapReduce：</p>

<pre><code>  1、把数据整理成SequcnceFile格式（Mahout-InputMapper）作为初始化文件PointFile  
  2、CanopyMapper阶段本机聚成小的Canopy 中间文件写成SequenceFile 这一步的T1、T2 和Reduce的T1、T2可以是不同的（ index、Canpy）  
  3、所有的Mapper阶段的输出到1个Reducer中 然后Reduce把Map阶段中的Center点再次做聚类算法。聚出全局的Canopy。同时计算每个Canopy的CenterPoint点。写到临时文件CenterPoint中。  
  4、针对全集合PointFile在CenterPoint上的findClosestCanopy操作（通过传入的距离算法）。然后输出一个SequenceFile。     
</code></pre>

<p>基于距离-增量聚类的实现方式</p>

<pre><code>1.
每增加一个数据，立即(或延迟)计算它与其他对象之间
的相异度度量值，并将其插入到R2中；检查计算出相异度
度量值，当d(id，j)&lt;δ时，如果id在R3中存在，则elements= 
elements+1；否则在R3中插入一个新记录(id，1)。  

2.
数据的增加部分可以简单的加到原有聚类结果中,具体情况可以分两种: 
(1)增加记录在原有聚类结果的某一类范围内(n维空间),这样这条记录的加入增加了这类的数据密度,因此可以直接把这条记录加入这一类当中. 
(2)增加记录不在原来聚类结果的任何一类范围内,也就是相当于n维空间上的一个孤立的点,这种情况可暂把这条数据记录单独看成一类. 
</code></pre>

      <div class="readall"><a href="/blog/2014/11/30/clustering-algorithm.html" id="post-readall">阅读全文&nbsp;<i class="fa fa-chevron-right"></i></a></div>
    </div>
  </section>
</div>

<div class="posts">
  <section class="post">
    <header class="post-header">
      <h1 class="post-title"><a href="/blog/2014/11/04/mahout-xiangjie.html">mahout--初学</a></h1>
      <p class="post-meta">
        <i class="fa fa-calendar"></i>
        2014年11月04日
        <i class="space"></i>
        <i class="fa fa-tags"></i>
        <a class="post-category" href="/page/category.html#hadoop">hadoop</a>
      </p>
    </header>
    <div class="post-main">
      <h2 id="mahout">Mahout</h2>
<p><strong>What is Mahout</strong></p>

<p>官方解释：<br />
The Apache Mahout project’s goal is to build a scalable machine learning library.<br />
—-即一个可扩展的机器学习程序库<br />
Mahout是Hadoop家族中与众不同的一个成员，他是基于一个Hadoop的机器学习和数据挖掘的分布式计算框架。是一个跨学科产品，是现在Hadoop生态环境中，对于普通开发者来说最具竞争力，最难掌握，也是最值得学习的一个项目之一。它为数据分析人员，降低了大数据分析的门槛；为算法工程师，提供基础的算法库；为Hadoop开发人员，提供了数据建模的标准。 <br />
**Mahout 0.9 Applicable Models:  **</p>

<pre><code>	• User and Item based recommenders （基于用户/物品推荐）
	• Matrix factorization based recommenders （基于矩阵因数分解推荐）
	• K-Means, Fuzzy K-Means clustering（K-Means 聚类）
	• Latent Dirichlet Allocation （三层贝叶斯概率模型）
	• Singular Value Decomposition （奇异值分解）
	• Logistic regression classifier （逻辑回归 LR分类器）
	• (Complementary) Naive Bayes classifier（朴素贝叶斯分类器）
	• Random forest classifier （随机森林基于决策树的分类器）
	• High performance java collections （高性能java集合(以前的colt集合)）
	• A vibrant community （社区）
</code></pre>

<p><strong>Mahout运行/开发环境</strong><br />
运行：<br />
Mahout运行可以指定运行在hadoop上或是直接本地运行。 默认是在hadoop环境上运行，Mahout运行时会读取profile文件中我们配置的HADOOP_HOME和HADOOP_CONF_DIR项读取hadoop环境，从而在得以运行于hadoop上。 
但是如果少量的数据分析可能不需要hadoop的介入，单机就能分析完成而且效率会更好，这时只需要在运行Mahout时加上参数 MAHOUT_LOCAL：设置是否本地运行，如果设置这个参数就不会运行hadoop了，一旦设置这个参数，那HADOOP_CONF_DIR 和HADOOP_HOME 这两个参数的设置就自动失效了。
开发：<br />
Mahout以maven构建，在开发Mahout相关程序时只需要在maven工程中引入Mahout相关依赖即可。如：  </p>

<pre><code>&lt;dependencies&gt;
		&lt;dependency&gt;
			&lt;groupId&gt;org.apache.mahout&lt;/groupId&gt;
			&lt;artifactId&gt;mahout-core&lt;/artifactId&gt;
			&lt;version&gt;${mahout.version}&lt;/version&gt;
		&lt;/dependency&gt;
		&lt;dependency&gt;
			&lt;groupId&gt;org.apache.mahout&lt;/groupId&gt;
			&lt;artifactId&gt;mahout-integration&lt;/artifactId&gt;
			&lt;version&gt;${mahout.version}&lt;/version&gt;
			&lt;exclusions&gt;
				&lt;exclusion&gt;
					&lt;groupId&gt;org.mortbay.jetty&lt;/groupId&gt;
					&lt;artifactId&gt;jetty&lt;/artifactId&gt;
				&lt;/exclusion&gt;
				&lt;exclusion&gt;
					&lt;groupId&gt;org.apache.cassandra&lt;/groupId&gt;
					&lt;artifactId&gt;cassandra-all&lt;/artifactId&gt;
				&lt;/exclusion&gt;
				&lt;exclusion&gt;
					&lt;groupId&gt;me.prettyprint&lt;/groupId&gt;
					&lt;artifactId&gt;hector-core&lt;/artifactId&gt;
				&lt;/exclusion&gt;
			&lt;/exclusions&gt;
		&lt;/dependency&gt;
	&lt;/dependencies&gt;
</code></pre>

<p>坑：在实际开发中如果只是像上面引用Mahout依赖，然后maven打包放到hadoop环境中去运行会出现各种问题，如jar包不兼容丢失等等。
解决办法：参考Mahout-examples中的pom.xml配置加入 job.xml文件</p>

<p><strong>Mahout学习路线图：</strong> 
<img src="/image/mahout1.png" alt="" />  </p>

<p><strong>聚类算法(clustering algorithm)</strong> 
<strong><em>What is clustering</em></strong> <br />
   Clustering–聚类,即将数据对象分组成为多个类或者簇 (Cluster)，它的目标：在同一个簇中的对象之间具有较高的相似度，而不同簇中的对象差别较大。即一个簇中的数据对象应当可以被作为一个整体来对待，从而减少计算量或者提高计算质量。 <br />
聚类分析广泛的应用在许多应用中，包括模式识别，数据分析，图像处理以及市场研究。通过聚类，人们能意识到密集和稀疏的区域，发现全局的分布模式，以及数据属性之间的相互关系。<br />
最被广泛使用的既是对 Web 上的文档进行分类，组织信息的发布，给用户一个有效分类的内容浏览系统（门户网站），同时可以加入时间因素，进而发现各个类内容的信息发展，最近被大家关注的主题和话题，或者分析一段时间内人们对什么样的内容比较感兴趣，这些有趣的应用都得建立在聚类的基础之上。作为一个数据挖掘的功能，聚类分析能作为独立的工具来获得数据分布的情况，观察每个簇的特点，集中对特定的某些簇做进一步的分析，此外，聚类分析还可以作为其他算法的预处理步骤，简化计算量，提高分析效率。它是数据挖掘及机器学习领域内的重点问题之一<br />
   聚类是在给定的数据集合中寻找同类的数据子集合，每一个子集合形成一个类簇，同类簇中的数据具有更大的相似性。聚类算法大体上可分为基于划分的方法、基于层次的方法、基于密度的方法、基于网格的方法以及基于模型的方法。选择聚类算法是需要考虑如下几个方面：   </p>

<pre><code>1.聚类结果是排他的还是可重叠的
  一个元素，他是否可以属于聚类结果中的多个簇中，如果是，则是一个可重叠的聚类问题，如果否，那么是一个排他的聚类问题。
2. 基于层次还是基于划分.
  基于划分：一组对象，按照一定的原则将它们分成不同的组，这是典型的划分聚类问题
  基于层次：顶层将对象进行大致的分组，随后每一组再被进一步的细分，也许所有路径最终都要到达一个单独实例（或者自底向上）
3. 簇数目固定的还是无限制的聚类
   聚类问题是在执行聚类算法前已经确定聚类的结果应该得到多少簇，还是根据数据本身的特征，由聚类算法选择合适的簇的数目。
4. 基于距离还是基于概率分布模型
   基于距离的聚类问题很好理解，即距离近的相似的对象聚在一起
   基于概率分布模型的聚类问题，就是在一组对象中，找到能符合特定分布模型的点的集合，他们不一定是距离最近的或者最相似的，而是能完美的呈现出概率分布模型所描述的模型。
</code></pre>

<p><strong>Mahout clustering</strong> <br />
数据模型<br />
Mahout 的聚类算法将对象表示成一种简单的数据模型：向量 (Vector)。在向量数据描述的基础上，可以轻松的计算两个对象的相似性.Mahout 中的向量 Vector 是一个每个域是浮点数 (double) 的复合对象，最容易想到的实现就是一个浮点数的数组。但在具体应用由于向量本身数据内容的不同，比如有些向量的值很密集，每个域都有值；有些则是很稀疏，可能只有少量域有值，所以 Mahout 提供了多个实现：  </p>

<pre><code>1. DenseVector，它的实现就是一个浮点数数组，对向量里所有域都进行存储，适合用于存储密集向量。
2. RandomAccessSparseVector 基于浮点数的 HashMap 实现的，key 是整形 (int) 类型，value 是浮点数 (double) 类型，它只存储向量中不为空的值，并提供随机访问。
3. SequentialAccessVector 实现为整形 (int) 类型和浮点数 (double) 类型的并行数组，它也只存储向量中不为空的值，但只提供顺序访问。
</code></pre>

<p><strong>K-Maens:</strong><br />
K-Means算法是一种得到最广泛使用的基于划分的聚类算法，把n个对象分为k个簇，以使簇内具有较高的相似度。相似度的计算根据一个簇中对象的平均值来进行。算法首先随机地选择k个对象，每个对象初始地代表了一个簇的平均值或中心。对剩余的每个对象根据其与各个簇中心的距离，将它赋给最近的簇，然后重新计算每个簇的平均值。这个过程不断重复，直到准则函数收敛。
K-Means是典型的基于距离的排他的划分方法：给定一个 n 个对象的数据集，它可以构建数据的 k 个划分，每个划分就是一个聚类，并且 k&lt;=n，同时还需要满足两个要求：  </p>

<ol>
  <li>每个组至少包含一个对象</li>
  <li>每个对象必须属于且仅属于一个组。</li>
</ol>

<p>基本原理:  </p>

<ol>
  <li>首先创建一个初始划分，随机地选择 k 个对象，每个对象初始地代表了一个簇中心。对于其他的对象，根据其与各个簇中心的距离，将它们赋给最近的簇。</li>
  <li>然后采用一种迭代的重定位技术，尝试通过对象在划分间移动来改进划分。所谓重定位技术，就是当有新的对象加入簇或者已有对象离开簇的时候，重新计算簇的平均值，然后对对象进行重新分配。这个过程不断重复，直到没有簇中对象的变化。</li>
</ol>

<p>优缺点：<br />
首先原理简单，实现起来也相对简单，同时执行效率和对于大数据量的可伸缩性还是较强的。当结果簇是密集的，而且簇和簇之间的区别比较明显时，K-Means的效果比较好。<br />
K-Means的最大问题是要求用户必须事先给出 k 的个数，k 的选择一般都基于一些经验值和多次实验结果，对于不同的数据集，k 的取值没有可借鉴性。另外，K 均值对“噪音”和孤立点数据是敏感的，少量这类的数据就能对平均值造成极大的影响。</p>

<p>基于 Hadoop 的 Mahout K-Means聚类算法实现（主要步骤代码）</p>

<pre><code>  //将原始数据randomData.csv，指定向量类型，转成Mahout sequence files of VectorWritable。
    InputDriver.runJob(new Path(inPath), new Path(seqFile), "org.apache.mahout.math.RandomAccessSparseVector");
    //指定聚类的簇数
    //通过随机的方法，选中kmeans的3个中心，做为初始集群
    int k = 3;
    Path seqFilePath = new Path(seqFile);
    Path clustersSeeds = new Path(seeds);
    DistanceMeasure measure = new EuclideanDistanceMeasure();
    clustersSeeds = RandomSeedGenerator.buildRandom(conf, seqFilePath, clustersSeeds, k, measure);
    //利用K-means算法实现聚类分析
    //根据迭代次数的设置，执行MapReduce，进行 10次迭代计算
    KMeansDriver.run(conf, seqFilePath, clustersSeeds, new Path(outPath), measure, 0.01, 10, true, 0.01, false);
    Path outGlobPath = new Path(outPath, "clusters-*-final");
    Path clusteredPointsPath = new Path(clusteredPoints);
    System.out.printf("Dumping out clusters from clusters: %s and clusteredPoints: %s\n", outGlobPath, clusteredPointsPath);
    //结果输出
    ClusterDumper clusterDumper = new ClusterDumper(outGlobPath, clusteredPointsPath);
    clusterDumper.printClusters(null);
</code></pre>

<p>对于K-Means不足的补救办法：<br />
对于K-Means 需要事先指定簇数的不足，我们可以结合另一个聚类算法:canopy,从而解决这个不足。<br />
<strong>Canopy 聚类算法：</strong><br />
Canopy 聚类算法的基本原则是：首先应用成本低的近似的距离计算方法高效的将数据分为多个组，这里称为一个 Canopy，Canopy 之间可以有重叠的部分；然后采用严格的距离计算方式准确的计算在同一 Canopy 中的点，将他们分配与最合适的簇中。Canopy 聚类算法经常用于 K 均值聚类算法的预处理，用来找合适的 k 值和簇中心。  </p>

<p>在Mahout的examples中的kmeans算法实现就是这种解决思维：</p>

<pre><code>public static void run(Configuration conf, Path input, Path output, DistanceMeasure measure, double t1, 	double t2, double convergenceDelta, int maxIterations)throws Exception{
	Path directoryContainingConvertedInput = new Path(output,DIRECTORY_CONTAINING_CONVERTED_INPUT);
	log.info("Preparing Input");
	InputDriver.runJob(input, directoryContainingConvertedInput,"org.apache.mahout.math.RandomAccessSparseVector");
	log.info("Running Canopy to get initial clusters");
	CanopyDriver.run(conf, directoryContainingConvertedInput, output, measure,t1, t2, false, false);
	log.info("Running KMeans");
	KMeansDriver.run(conf, directoryContainingConvertedInput, new Path(output,Cluster.INITIAL_CLUSTERS_DIR), output, measure,convergenceDelta,maxIterations, true, false);
	// run ClusterDumper
	ClusterDumper clusterDumper = new ClusterDumper(finalClusterPath(conf,
	output, maxIterations), new Path(output, "clusteredPoints"));
	clusterDumper.printClusters(null);
} CanopyDriver.run( ) ： 即用Canopy算法确定初始簇的个数和簇的中心。 KMeansDriver.run( ) ： K-means算法进行聚类。
</code></pre>

      <div class="readall"><a href="/blog/2014/11/04/mahout-xiangjie.html" id="post-readall">阅读全文&nbsp;<i class="fa fa-chevron-right"></i></a></div>
    </div>
  </section>
</div>

<div class="posts">
  <section class="post">
    <header class="post-header">
      <h1 class="post-title"><a href="/blog/2014/10/12/nosqlhbase.html">nosql&hbase</a></h1>
      <p class="post-meta">
        <i class="fa fa-calendar"></i>
        2014年10月12日
        <i class="space"></i>
        <i class="fa fa-tags"></i>
        <a class="post-category" href="/page/category.html#hadoop">hadoop</a>
      </p>
    </header>
    <div class="post-main">
      <h2 id="nosqlhbase">NoSQL/Hbase</h2>

<p><strong>NoSQL (Not Only SQL )</strong></p>

<p>一项全新的数据库革命性运动，早期就有人提出，发展至2009年趋势越发高涨。NoSQL的拥护者们提倡运用非关系型的数据存储，相对于铺天盖地的关系型数据库运用，这一概念无疑是一种全新的思维的注入。任何技术的流行都有着现实的强烈需求，NoSQL也是如此，随着互联网的移动化、大数据化，超大规模和高并发的现实需求 传统的关系型数据暴露出越来越多难以满足的缺陷。如：</p>

<pre><code>1、High performance - 对数据库高并发读写的需求
2、Huge Storage - 对海量数据的高效率存储和访问的需求
3、High Scalability &amp;&amp; High Availability- 对数据库的高可扩展性和高可用性的需求
</code></pre>

<p>什么是NoSQL：</p>

<p>wiki上的定义是“NoSQL is a movement promoting a loosely defined class of non-relational data stores that break with a long history of relational databases”。其实并不存在一个叫NoSQL的产品，它是一类non-relational data stores的集合。NoSQL的重点是non-relational，而传统的数据库是relational。传统关系型数据库的最大缺陷是扩展性，虽然各个数据库厂家都有cluster的解决方案，但是不管是share storage还是share nothing的解决方案，扩展性都十分有限。目前解决数据库扩展性的思路主要有两个：第一是数据分片(sharding)或者功能分区，虽然说可以很好的解决数据库扩展性的问题，但是在实际使用过程中，一旦采用数据分片或者功能分区，必然会导致牺牲“关系型”数据库的最大优势-join，对业务局限性非常大，而数据库也退化成为一个简单的存储系统。另外一个思路是通过maser-slave复制的方式，通过读写分离技术在某种程度上解决扩展性的问题，但这种方案中，由于每个数据库节点必须保存所有的数据，这样每个存储的IO subsystem必然成为扩展的瓶颈，而且masert节点也是一个瓶颈。总的来说，传统关系型数据库的扩展能力十分有限。</p>

<p>NoSQL与关系型数据库在概念上的不同：</p>

<p>CAP：</p>

<pre><code>Consistency(一致性)，数据一致更新，所有数据变动都是同步的
Availability(可用性)，好的响应性能
Partition tolerance(分区容错性) 可靠性 CAP原理告诉我们，这三个因素最多只能满足两个，不可能三者兼顾。对于分布式系统来说，分区容错是基本要求，所以必然要放弃一致性。对于大型网站来说，分区容错和可用性的要求更高，所以一般都会选择适当放弃一致性。对应CAP理论，NoSQL追求的是AP，而传统数据库追求的是CA，这也可以解释为什么传统数据库的扩展能力有限的原因。
</code></pre>

<p>BASE：</p>

<pre><code>Basically Availble：基本可用
Soft-state： 软状态/柔性事务
Eventual Consistency：最终一致性 BASE模型是传统ACID模型的反面，不同与ACID，BASE强调牺牲高一致性，从而获得可用性，数据允许在一段时间内的不一致，只要保证最终一致就可以了。最终一致性是整个NoSQL中的一个核心理念。
</code></pre>

<p>现如今NoSQL产品分类：</p>

<pre><code>Key-Value模型：Key-Value模型是最简单，也是使用最方便的数据模型，它支持简单的key对value的键值存储和提取；Key-Value模型的一个大问题是它通常是由HashTable实现的，所以无法进行范围查询，所以有序Key-Value模型就出现了，有序Key-Value支持范围查询；

Ordered Key-Value：虽然有序Key-Value模型能够解决范围查询和问题，但是其Value值依然是无结构的二进制码或纯字符串，通常我们只能在应用层去解析相应的结构。

BigTable的数据模型，能够支持结构化的数据，包括列、列簇、时间戳以及版本控制等元数据的存储；
文档型存储相对到类BigTable存储又有两个大的提升，一是其Value值支持复杂的结构定义，二是支持数据库索引的定义；全文索引模型与文档型存储的主要区别在于文档型存储的索引主要是按照字段名来组织的，而全文索引模型是按字段的具体值来组织的；

图数据库模型也可以看作是从Key-Value模型发展出来的一个分支，不同的是它的数据之间有着广泛的关联，并且这种模型支持一些图结构的算法。
他们之间的关系幽默的表示如下： ![](/image/nosql.jpg)
</code></pre>

<p>目前NoSQL几个分类的代表产品： </p>

<pre><code>Key-Value 存储：Oracle Coherence、Redis、Kyoto Cabinet
类BigTable存储：Apache HBase、Apache Cassandra
文档数据库：MongoDB、CouchDB
全文索引：Apache Lucene、Apache Solr
图数据库：neo4j、FlockDB
</code></pre>

<p><strong>NoSQL 之Hbase</strong></p>

<p>Hbase是从hadoop中分离出来的apache顶级开源项目。用java实现了Google的Bigtable系统大部分特性，是一个高可靠性、高性能、面向列、可伸缩的分布式存储系统，利用HBase技术可在廉价PC Server上搭建起大规模结构化存储集群。类似Google Bigtable利用GFS作为其文件存储系统，HBase利用Hadoop HDFS作为其文件存储系统；Google Bigtable利用 Chubby作为协同服务，HBase利用Zookeeper作为对应。</p>

<p>Hbase在Hadoop 生态环境中的位置：
<img src="/image/hbase-local-in-hadooop.jpg" alt="" /></p>

<p>如上图HBase位于Hadoop ecosystem结构化存储层，Hadoop HDFS为HBase提供了高可靠性的底层存储支持，Hadoop MapReduce为HBase提供了高性能的计算能力，Zookeeper为HBase提供了稳定服务和failover机制。此外，Pig和Hive还为HBase提供了高层语言支持，使得在HBase上进行数据统计处理变的非常简单。 Sqoop则为HBase提供了方便的RDBMS数据导入功能，使得传统数据库数据向HBase中迁移变的非常方便。与hadoop一样，Hbase目标主要依靠横向扩展，通过不断增加廉价的商用服务器，来增加计算和存储能力。</p>

<p>HBase中的表一般有这样的特点：</p>

<pre><code>1 大：一个表可以有上亿行，上百万列
2 面向列:面向列(族)的存储和权限控制，列(族)独立检索。
3 稀疏:对于为空(null)的列，并不占用存储空间，因此，表可以设计的非常稀疏。
</code></pre>

<p><strong>HBase数据模型</strong></p>

<p>Table:</p>

<pre><code>table
  |-column family
  	|-column
  		|-cell
			|-rowkey
			|-timestamp
  			|-value HBase没有database的概念。rowkey是数据的一部分，其必须人工指定。 column family 在定义表时需指明。数据存储逻辑视图如下：

Row Key	Timestamp	Column Family
		URI	Parser
r1	T3	url=http://www.xxxx.com	title=xxxx
	T2	host=xxxx.com	 
	T1	 	 
r2	T2	url=http://www.xxxx.com	content=xxxx…
	T1	host=xxxx.com	 

Ø Row Key: 行键，Table的主键，Table中的记录按照Row Key排序
Ø Timestamp: 时间戳，每次数据操作对应的时间戳，可以看作是数据的version number
Ø Column Family：列簇，Table在水平方向有一个或者多个Column Family组成，
一个Column Family中可以由任意多个Column组成，即Column Family支持动态扩展，
无需预先定义Column的数量以及类型，所有Column均以二进制格式存储，用户需要自行进行类型转换。
</code></pre>

<p>Region:</p>

<pre><code>当Table随着记录数不断增加而变大后，会逐渐分裂成多份splits，成为regions，一个region由
[startkey,endkey)表示，不同的region会被Master分配给相应的RegionServer进行管理：
</code></pre>

<p>-ROOT- &amp;&amp; .META. Table</p>

<pre><code>HBase中有两张特殊的Table，-ROOT-和.META.
Ø .META.：记录了用户表的Region信息，.META.可以有多个regoin
Ø -ROOT-：记录了.META.表的Region信息，-ROOT-只有一个region
Ø Zookeeper中记录了-ROOT-表的location
</code></pre>

<p>Hbase整体寻址结构如下：</p>

<pre><code> ![](/image/hbase-addre.jpg)
</code></pre>

<p>Client访问用户数据之前需要首先访问zookeeper，然后访问-ROOT-表，接着访问.META.表，最后才能找到用户数据的位置去访问，中间需要多次网络操作，不过client端会做cache缓存。
HBase系统架构</p>

<p><img src="/image/hbase-structure.jpg" alt="" /></p>

<p>Client</p>

<pre><code>HBase Client使用HBase的RPC机制与HMaster和HRegionServer进行通信，对于管理类操作，
Client与HMaster进行RPC；对于数据读写类操作，Client与HRegionServer进行RPC Zookeeper

Zookeeper Quorum中除了存储了-ROOT-表的地址和HMaster的地址，HRegionServer也会把自己以
Ephemeral方式注册到 Zookeeper中，使得HMaster可以随时感知到各个HRegionServer的健康状态
。此外，Zookeeper也避免了HMaster的 单点问题。 HMaster

HMaster没有单点问题，HBase中可以启动多个HMaster，通过Zookeeper的Master Election机
制保证总有一个Master运行，HMaster在功能上主要负责Table和Region的管理工作：
1.       管理用户对Table的增、删、改、查操作
2.       管理HRegionServer的负载均衡，调整Region分布
3.       在Region Split后，负责新Region的分配
4.       在HRegionServer停机后，负责失效HRegionServer 上的Regions迁移 HRegionServer
</code></pre>

<p><img src="/image/region-server.jpg" alt="" /></p>

<p>HRegionServer主要负责响应用户I/O请求，向HDFS文件系统中读写数据，是HBase中最核心的模块。
HRegionServer内部管理了一系列HRegion对象，每个HRegion对应了Table中的一个Region，HRegion中由多 个HStore组成。每个HStore对应了Table中的一个Column Family的存储，可以看出每个Column Family其实就是一个集中的存储单元，因此最好将具备共同IO特性的column放在一个Column Family中，这样最高效。</p>

<p>HStore存储是HBase存储的核心了，其中由两部分组成，一部分是MemStore，一部分是StoreFiles。MemStore是 Sorted Memory Buffer，用户写入的数据首先会放入MemStore，当MemStore满了以后会Flush成一个StoreFile（底层实现是HFile）， 当StoreFile文件数量增长到一定阈值，会触发Compact合并操作，将多个StoreFiles合并成一个StoreFile，合并过程中会进 行版本合并和数据删除，因此可以看出HBase其实只有增加数据，所有的更新和删除操作都是在后续的compact过程中进行的，这使得用户的写操作只要 进入内存中就可以立即返回，保证了HBase I/O的高性能。当StoreFiles Compact后，会逐步形成越来越大的StoreFile，当单个StoreFile大小超过一定阈值后，会触发Split操作，同时把当前 Region Split成2个Region，父Region会下线，新Split出的2个孩子Region会被HMaster分配到相应的HRegionServer 上，使得原先1个Region的压力得以分流到2个Region上。下图描述了Compaction和Split的过程：</p>

<p><img src="/image/region-compacte.png" alt="" /></p>

<p>在理解了上述HStore的基本原理后，还必须了解一下HLog的功能，因为上述的HStore在系统正常工作的前提下是没有问题的，但是在分布式 系统环境中，无法避免系统出错或者宕机，因此一旦HRegionServer意外退出，MemStore中的内存数据将会丢失，这就需要引入HLog了。 每个HRegionServer中都有一个HLog对象，HLog是一个实现Write Ahead Log的类，在每次用户操作写入MemStore的同时，也会写一份数据到HLog文件中，HLog文件定期会滚动出新的，合并删除旧的文件（已持久化到StoreFile中的数据）。当HRegionServer意外终止后，HMaster会通过Zookeeper感知到，HMaster首先会处理遗留的 HLog文件，将其中不同Region的Log数据进行拆分，分别放到相应region的目录下，然后再将失效的region重新分配，领取 到这些region的HRegionServer在Load Region的过程中，会发现有历史HLog需要处理，因此会Replay HLog中的数据到MemStore中，然后flush到StoreFiles，完成数据恢复。</p>

<pre><code>HBase实践总结：
column family 不应超过两个，因为一个column family 对应一个store，flush和compaction的触发的基本单位都是Region级别，所以当一个column family有大量的数据的时候会触发整个region里面的其他column family的memstore（其实这些memstore可能仅有少量的数据，还不需要flush的）也发生flush动作；另外compaction触发的条件是当store file的个数（不是总的store file的大小）达到一定数量的时候会发生，而flush产生的大量store file通常会导致compaction，flush/compaction会发生很多IO相关的负载，这对Hbase的整体性能有很大影响。

一个table对应一个或多个region，在map/reduce 中对应一个map 一个column family 对应一个store。 store 中含一个menstore 和多个fileStore 当menstore 存储到达一定阀值就生成一个filestore， fileStore存放在hdfs上， 当fileStore 数量到达一定阀值就将其合并成一个大的file。

在插入rowkey时，不考虑region的大小及状态，只是根据它的access 编码排序，找到对应的startkey，endkey,确定插入的位置。
HBase两张管理表：+ .META.:管理用户region，它可以有多个region。 ROOT: 管理.META.的region，它只有一个region。 所以读写数据的流程为： client–&gt;ROOT–&gt;META–&gt;regionserver 在数据的读写过程中client不会与master产生关系 master只是负责管理表结构。

利用backup-master机制，解决master单点的问题。
region在存储的数据到达一定阀值后将split，split的同时更形.META.和ROOT表的相关数据。 split region的过程不需要master的参与，master只负责将split后新的region分配到对应的regionserver。

HBase的数据安全机制之一：在插入数据时先写Hlog然后在写到内存。 一个regionserver中只有一个Hlog， Hlog中只记录内存中存放的数据。 当机器挂掉后，master会根据Hlog中region将其分割，然后分配到不同的机器加载，做数据恢复。
</code></pre>

      <div class="readall"><a href="/blog/2014/10/12/nosqlhbase.html" id="post-readall">阅读全文&nbsp;<i class="fa fa-chevron-right"></i></a></div>
    </div>
  </section>
</div>

<div class="posts">
  <section class="post">
    <header class="post-header">
      <h1 class="post-title"><a href="/blog/2014/10/09/zookeeper-install.html">zookeeper install</a></h1>
      <p class="post-meta">
        <i class="fa fa-calendar"></i>
        2014年10月09日
        <i class="space"></i>
        <i class="fa fa-tags"></i>
        <a class="post-category" href="/page/category.html#hadoop">hadoop</a>
      </p>
    </header>
    <div class="post-main">
      <p><strong>Zookeeper集群安装配置</strong></p>

<ol>
  <li>
    <p>修改zookeeper配置文件zoo.cfg</p>

    <p>在Linux系统下解压zookeeper安装包zookeeper-3.4.3.tar.gz ，进入到conf目录，将zoo_sample.cfg拷贝一份命名为zoo.cfg（Zookeeper 在启动时会找这个文件作为默认配置文件），打开该文件进行修改为以下格式</p>

    <pre><code> dataDir=/home/hadoop/temp/zookeeper/data
 server.23=192.168.6.23:2888:3888
 server.24=192.168.6.24:2888:3888
 server.25=192.168.6.25:2888:3888
</code></pre>
  </li>
  <li>
    <p>新建目录、新建并编辑myid文件
（本次配置myid文件放在/home/hadoop/temp/zookeeper/data目录下）</p>

    <pre><code> mkdir /home/hadoop/will/zookeeper/data	//dataDir目录
 vi /home/hadoop/temp/zookeeper/data/myid  注意myid文件中的内容为：Master中为0，Slave1中为1，Slave2中为2，分别与zoo.cfg中对应起来。
</code></pre>
  </li>
  <li>
    <p>同步安装包</p>

    <p>将解压修改后的zookeeper-3.4.3文件夹分别拷贝到Master、Slave1、Slave2的相同zookeeper安装路径下。注意：myid文件的内容不是一样的，各服务器中分别是对应zoo.cfg中的设置。</p>
  </li>
  <li>
    <p>启动zookeeper
 Zookeeper的启动与hadoop不一样，需要每个节点都执行，分别进入3个节点的zookeeper-3.4.3目录，启动zookeeper：</p>

    <pre><code> bin/zkServer.sh start  注意：此时如果报错先不理会，需要继续在另两台服务器中执行相同操作。 
</code></pre>
  </li>
  <li>
    <p>检查zookeeper是否配置成功
 待3台服务器均启动后，如果过程正确的话zookeeper应该已经自动选好leader，进入每台服务器的zookeeper-3.4.3目录，执行以下操作查看zookeeper启动状态：</p>

    <pre><code> bin/zkServer.sh status  如果出现以下代码表示安装成功了。

 [java] view plaincopy
 JMX enabled by default  
 Using config: /home/hadoop/zookeeper-3.4.3/bin/../conf/zoo.cfg  
 Mode: follower	//或者有且只有一个leader
</code></pre>
  </li>
</ol>

<h2 id="zookeeper">2 - 解读zookeeper的配置项</h2>
<p>zookeeper的默认配置文件为zookeeper/conf/zoo_sample.cfg，需要将其修改为zoo.cfg。其中各配置项的含义，解释如下：</p>

<ol>
  <li>
    <p>tickTime：CS通信心跳数</p>

    <p>Zookeeper 服务器之间或客户端与服务器之间维持心跳的时间间隔，也就是每个 tickTime 时间就会发送一个心跳。tickTime以毫秒为单位。
 1.tickTime=2000  </p>
  </li>
  <li>
    <p>initLimit：LF初始通信时限</p>

    <p>集群中的follower服务器(F)与leader服务器(L)之间初始连接时能容忍的最多心跳数（tickTime的数量）。
 1.initLimit=5  </p>
  </li>
  <li>
    <p>syncLimit：LF同步通信时限</p>

    <p>集群中的follower服务器与leader服务器之间请求和应答之间能容忍的最多心跳数（tickTime的数量）。
 1.syncLimit=2  
 </p>
  </li>
  <li>
    <p>dataDir：数据文件目录</p>

    <p>Zookeeper保存数据的目录，默认情况下，Zookeeper将写数据的日志文件也保存在这个目录里。
 1.dataDir=/home/michael/opt/zookeeper/data  </p>
  </li>
  <li>
    <p>dataLogDir：日志文件目录</p>

    <p>Zookeeper保存日志文件的目录。
 1.dataLogDir=/home/michael/opt/zookeeper/log  </p>
  </li>
  <li>
    <p>clientPort：客户端连接端口</p>

    <p>客户端连接 Zookeeper 服务器的端口，Zookeeper 会监听这个端口，接受客户端的访问请求。
 1.clientPort=2333  </p>
  </li>
  <li>
    <p>服务器名称与地址：集群信息（服务器编号，服务器地址，LF通信端口，选举端口）</p>

    <p>这个配置项的书写格式比较特殊，规则如下：
 1.server.N=YYY:A:B  </p>

    <p>其中N表示服务器编号，YYY表示服务器的IP地址，A为LF通信端口，表示该服务器与集群中的leader交换的信息的端口。B为选举端口，表示选举新leader时服务器间相互通信的端口（当leader挂掉时，其余服务器会相互通信，选择出新的leader）。一般来说，集群中每个服务器的A端口都是一样，每个服务器的B端口也是一样。但是当所采用的为伪集群时，IP地址都一样，只能时A端口和B端口不一样。
 如：</p>

    <pre><code> 1.server.0=192.168.6.23:2008:6008  
 2.server.1=192.168.6.24:2008:6008  
 3.server.2=192.168.6.25:2008:6008  
 4.server.3=192.168.6.26:2008:6008  
</code></pre>
  </li>
</ol>

      <div class="readall"><a href="/blog/2014/10/09/zookeeper-install.html" id="post-readall">阅读全文&nbsp;<i class="fa fa-chevron-right"></i></a></div>
    </div>
  </section>
</div>

<div class="pagination">
  
  
  <a class="pagination-item newer" href="/page/3"><i class="fa fa-arrow-left"></i>&nbsp;&nbsp;上一页</a>
  
    
  
  <a class="pagination-item older" href="/page/5">下一页&nbsp;&nbsp;<i class="fa fa-arrow-right"></i></a>
  
</div>
    <footer>Copyright&nbsp;&copy;&nbsp;2015 <a href="index.html">willgo</a><br/><i class="fa fa-cogs" style="color:blueviolet;">&nbsp;</i>Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a>
</br> <a href="http://m.kuaidi100.com" target="_blank">快递查询</a> 
</footer>

    </div>
  </div>    
  <div id="top"><a id="rocket" href="javascript:;" title="返回顶部"><i></i></a></div>
  
</body>
</html>
